[{"content":"对于大部分公司而言，能够写底层代码或者中间件代码的人总是有限的，写业务代码会面临更高的复杂度。这里分三个层次来看其中的机会：\n 第一个层次，让代码写的不一样。可从代码规范、可读性、可扩展性等角度着手，这也是程序员的基本功。 第二个层次，考虑业务问题和技术问题的匹配。可从写业务代码中理解需求，并做好分析设计。被动接收需求和实现接口，确实成长空间不大。 第三个层次，总结相关方法体系，成为业务及技术双料专家。  ","permalink":"https://zhenfeng-zhu.github.io/posts/%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81%E7%9A%84%E6%88%90%E9%95%BF%E6%9C%BA%E4%BC%9A/","summary":"对于大部分公司而言，能够写底层代码或者中间件代码的人总是有限的，写业务代码会面临更高的复杂度。这里分三个层次来看其中的机会：\n 第一个层次，让代码写的不一样。可从代码规范、可读性、可扩展性等角度着手，这也是程序员的基本功。 第二个层次，考虑业务问题和技术问题的匹配。可从写业务代码中理解需求，并做好分析设计。被动接收需求和实现接口，确实成长空间不大。 第三个层次，总结相关方法体系，成为业务及技术双料专家。  ","title":"业务代码的成长机会"},{"content":" 让每个程序都做好一件事。要做一件新的工作，写一个新程序，而不是通过添加“功能”让老程序复杂化。 期待每个程序的输出成为另一个程序的输入。不要将无关信息混入输出。避免使用严格的列数据或二进制输入格式。不要坚持交互式输入。 设计和构建软件，甚至是操作系统，要尽早尝试，最好在几周内完成。不要犹豫，扔掉笨拙的部分，重建它们。 优先使用工具来减轻编程任务，即使必须曲线救国编写工具，且在用完后很可能要扔掉大部分。  ","permalink":"https://zhenfeng-zhu.github.io/posts/unix-philosophy/","summary":" 让每个程序都做好一件事。要做一件新的工作，写一个新程序，而不是通过添加“功能”让老程序复杂化。 期待每个程序的输出成为另一个程序的输入。不要将无关信息混入输出。避免使用严格的列数据或二进制输入格式。不要坚持交互式输入。 设计和构建软件，甚至是操作系统，要尽早尝试，最好在几周内完成。不要犹豫，扔掉笨拙的部分，重建它们。 优先使用工具来减轻编程任务，即使必须曲线救国编写工具，且在用完后很可能要扔掉大部分。  ","title":"unix 哲学"},{"content":"一般来说web前端是指网站业务逻辑之前的部分，比如：浏览器加载、网站视图模型、图片服务、CDN服务等等。web前端优化主要从如下三个方面入手：\n浏览器访问优化   减少http请求\nhttp协议是一个无状态的，每次请求都需要建立通信链路进行传输，在服务器端，一般每个请求都会分配一个线程去处理。\n减少http请求的主要手段是合并CSS、合并js、合并图片。\n  使用浏览器缓存\ncss、js、Logo、图标等静态资源文件更新频率较低，可以将这些文件缓存在浏览器中。\n在更新js等文件的时候，一般不是将文件内容更新，而是生成一个新的文件，然后更新html的引用。\n更新静态资源的时候，也是要逐量更新，以避免用户浏览器的大量缓存失效，造成服务器负载增加、网络堵塞。\n  启用压缩\n在服务器对文件压缩，然后在浏览器端解压缩，可以减少通信传输的数据量。\n  CSS放在页面最上面，js放在页面最下面\n浏览器会在下载完全部CSS之后才对整个页面进行渲染，而浏览器是在加载js之后就立即执行，有可能会阻塞整个页面。因此最好的做法就是把CSS放在最上面，js放在最下面。但是如果是页面解析的时候就用到js，也是要相应的js放在上面。\n  减少cookie传输\ncookie会包含在每次请求和响应中，太大的cookie会影响数据传输，需要慎重考虑哪些数据写入cookie中。\n对于某些静态资源的访问，如css和js等，发送cookie没意义，可以考虑静态资源使用独立域名访问，避免请求静态资源时发送cookie。\n  CDN加速 CDN（content distribute network，内容分发网络）的本质仍然是一个缓存。将缓存放在离用户最近的地方，使得用户可以以最快的速度获取数据。\nCDN缓存的一般是静态资源，如图片、文件、CSS、js、静态网页等。\n反向代理 反向代理服务器位于网站中心机房的一侧，代理网站web服务器接收http请求。\n反向代理可以在一定程度上保护网站安全，来自互联网的访问请求必须经过代理服务器，相当于在web服务器和攻击之间加了一个屏障。\n反向代理也可以通过配置缓存，静态资源被缓存在反向代理服务器，当用户访问时，可以从反向代理服务器上返回。有些网站也会将部分动态内容缓存在代理服务器上，通过内部通知机制，更新缓存。\n反向代理也可以实现负载均衡的功能。\n写在最后 可以发现，在web前端性能优化的时候，提到最多的就是缓存。\n 网站性能优化第一定律：优先考虑使用缓存！\n ","permalink":"https://zhenfeng-zhu.github.io/posts/web%E5%89%8D%E7%AB%AF%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","summary":"一般来说web前端是指网站业务逻辑之前的部分，比如：浏览器加载、网站视图模型、图片服务、CDN服务等等。web前端优化主要从如下三个方面入手：\n浏览器访问优化   减少http请求\nhttp协议是一个无状态的，每次请求都需要建立通信链路进行传输，在服务器端，一般每个请求都会分配一个线程去处理。\n减少http请求的主要手段是合并CSS、合并js、合并图片。\n  使用浏览器缓存\ncss、js、Logo、图标等静态资源文件更新频率较低，可以将这些文件缓存在浏览器中。\n在更新js等文件的时候，一般不是将文件内容更新，而是生成一个新的文件，然后更新html的引用。\n更新静态资源的时候，也是要逐量更新，以避免用户浏览器的大量缓存失效，造成服务器负载增加、网络堵塞。\n  启用压缩\n在服务器对文件压缩，然后在浏览器端解压缩，可以减少通信传输的数据量。\n  CSS放在页面最上面，js放在页面最下面\n浏览器会在下载完全部CSS之后才对整个页面进行渲染，而浏览器是在加载js之后就立即执行，有可能会阻塞整个页面。因此最好的做法就是把CSS放在最上面，js放在最下面。但是如果是页面解析的时候就用到js，也是要相应的js放在上面。\n  减少cookie传输\ncookie会包含在每次请求和响应中，太大的cookie会影响数据传输，需要慎重考虑哪些数据写入cookie中。\n对于某些静态资源的访问，如css和js等，发送cookie没意义，可以考虑静态资源使用独立域名访问，避免请求静态资源时发送cookie。\n  CDN加速 CDN（content distribute network，内容分发网络）的本质仍然是一个缓存。将缓存放在离用户最近的地方，使得用户可以以最快的速度获取数据。\nCDN缓存的一般是静态资源，如图片、文件、CSS、js、静态网页等。\n反向代理 反向代理服务器位于网站中心机房的一侧，代理网站web服务器接收http请求。\n反向代理可以在一定程度上保护网站安全，来自互联网的访问请求必须经过代理服务器，相当于在web服务器和攻击之间加了一个屏障。\n反向代理也可以通过配置缓存，静态资源被缓存在反向代理服务器，当用户访问时，可以从反向代理服务器上返回。有些网站也会将部分动态内容缓存在代理服务器上，通过内部通知机制，更新缓存。\n反向代理也可以实现负载均衡的功能。\n写在最后 可以发现，在web前端性能优化的时候，提到最多的就是缓存。\n 网站性能优化第一定律：优先考虑使用缓存！\n ","title":"Web前端性能优化"},{"content":"前几天跟一个朋友聊了一些关于网站缓存分布式的一些东西，发现自己的知识还是太过贫瘠。理论+协议，这是现在我亟待加强的。这个周末买了两本关于分布式网站的书，本着好记性不如烂笔头，便有了这样一系列的文章。希望一同分享，也请多指教。\n code less, play more!\n 前言 这个世界上没有哪个网站从诞生起就是大型网站；也没有哪个网站第一次发布的时候就拥有庞大的用户，高并发的访问，海量的数据；大型网站都是从小型网站发展而来。网站的价值在于它能给用户提供什么家宅，在于网站能做什么，而不在于它是怎么做的，所以网站在小的时候就去追求网站的架构是舍本逐末，得不偿失的。小型网站最需要做的就是为用户提供更好的服务来创造价值，得到用户认可，活下去，野蛮生长。\n大型网站软件系统的特点  高并发，大流量 高可用 海量数据 用户分布广泛，网络情况复杂 安全环境恶劣 需求快速变更，发布平频繁 渐进式发展  大型网站的发展历程   初始阶段的网站架构\n最开始没有多少人访问，所以应用程序，数据库，文件都在同一台机器上。\n  应用服务器和数据服务分离\n应用和数据分离之后，一般需要三台服务器。应用服务器，文件服务器和数据库服务器，这三种服务器对于硬件要求各不相同。\n 应用服务器：更强大的CPU 数据库服务器：更快速的磁盘和更大的内存 文件服务器：容量更大的硬盘    使用缓存改善性能\n网站的访问也遵循二八定律：80%的业务集中在20%的数据上。因此可以把这一小部分数据缓存在内存中，减少数据库的访问压力。\n网站的缓存可以分为两种：\n 本地缓存：缓存在应用服务器上。本地缓存访问速度快，但是受制于内存限制，缓存数量有限，而且也会出现和应用程序争抢内存的情况。 远程分布式缓存：以集群的方式，缓存在大内存的专用缓存服务器。可以在理论上做到不受内存容量限制。    使用应用服务器集群提高并发能力\n当一台服务器的处理能力和存储空间不足的时候，不要企图更换更强大的服务器。对于大型网站来说，不管多么强大的服务器，都满足不了网站持续增长的业务需求。此时就可以考虑集群的方式，通过负载均衡调度服务器，可以将来自用户的请求分发到应用服务器集群中的任何一台服务器上。\n  数据库读写分离\n使用缓存后，大部分的数据读操作访问都可以不通过数据库完成，但是仍有部分读操作（如缓存过期，缓存不命中）和全部的写操作需要访问数据库。\n目前大部分数据库都提供主从热备的功能，在写数据的时候，访问主库，主库通过主从复制机制将数据更新同步至从数据库，在读的时候就可以通过从数据库获取数据。\n  使用反向代理和CDN加速网站响应\n在《web性能权威指南》中有讲到，网站性能的瓶颈，大部分时间都浪费在TCP的握手和传输上。因此可以通过CDN和反向代理的方式来加快响应。\nCDN和反向代理的本质都是通过缓存，不同的主要是：\n CDN部署在服务器器上的机房，用户在请求时，从距离自己最近的机房获取数据。 反向代理是部署在中心机房，用户请求到达中心机房之后，首先访问的服务器是反向代理的拂去其，如果反向代理服务器中缓存着用户请求的额资源，就将其返回给用户。    使用分布式文件系统和分布式数据库系统\n随着业务的发展，依旧不能满足的时候，就采用分布式的文件和分布式的数据库系统。\n分布式数据库是数据库拆分的最后手段，只用在单表数据规模特别庞大的时候才使用。更常用的拆分手段是业务分库，将不同的业务数据存储在不同的数据库中。\n  使用NoSQL和搜索引擎\n对数据检索和存储越来越复杂的时候，就可以采用一些非关系型数据库如HBase和非数据库查询技术如ElasticSearch等等\n  业务拆分\n业务场景复杂的时候，一般讲整个网站业务分为不同的产品线，如首页，订单，买家，卖家等等。\n技术上也会根据产品线划分，将一个网站分为许多不同的应用，每个应用独立部署维护，应用之间可以通过一个超链接建立联系，也可以通过消息队列进行数据分发，当然最多的还是通过访问同一个数据存储系统来构成一个关联的完整系统。\n  分布式服务\n随着业务越拆越小，存储越来越大，维护越来越困难。此时就可以将相同业务操作的提取出来，独立部署。应用系统只需要管理用户界面，通过分布式服务调用共同的业务服务完成具体的业务操作。也就是最近概念越来越火的——微服务。\n  云计算\n大型网站架构解决了海量数据库管理和高并发事务处理，可以将这些解决方案应用到网站自身以外的业务上。现在像阿里云，亚马逊等云计算平台，将计算作为一种基础资源出售，中小网站不需要关系技术架构等问题，只需要按需付费，就可以使网站随着业务的增长获得更大的存储和计算资源。\n  未来\n未来还能变成什么样子，我也不清楚，也许以后都不是开发人员来维护了，所有的这些都是AI来完成，程序员要做的就是如何完善AI。也许AI发展到最后，人类都不需要存在了吧。\n  结语 网站的技术是为业务而存在的，除此以外毫无意义。在技术选型和架构设计中，脱离业务发展实际，一味的追求新技术，可能会把技术发展引入一个歪路。\n技术是用来解决业务的问题，而技术不可能将所有问题都解决掉，涉及业务自身的问题，还是要通过业务手段去解决。\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/","summary":"前几天跟一个朋友聊了一些关于网站缓存分布式的一些东西，发现自己的知识还是太过贫瘠。理论+协议，这是现在我亟待加强的。这个周末买了两本关于分布式网站的书，本着好记性不如烂笔头，便有了这样一系列的文章。希望一同分享，也请多指教。\n code less, play more!\n 前言 这个世界上没有哪个网站从诞生起就是大型网站；也没有哪个网站第一次发布的时候就拥有庞大的用户，高并发的访问，海量的数据；大型网站都是从小型网站发展而来。网站的价值在于它能给用户提供什么家宅，在于网站能做什么，而不在于它是怎么做的，所以网站在小的时候就去追求网站的架构是舍本逐末，得不偿失的。小型网站最需要做的就是为用户提供更好的服务来创造价值，得到用户认可，活下去，野蛮生长。\n大型网站软件系统的特点  高并发，大流量 高可用 海量数据 用户分布广泛，网络情况复杂 安全环境恶劣 需求快速变更，发布平频繁 渐进式发展  大型网站的发展历程   初始阶段的网站架构\n最开始没有多少人访问，所以应用程序，数据库，文件都在同一台机器上。\n  应用服务器和数据服务分离\n应用和数据分离之后，一般需要三台服务器。应用服务器，文件服务器和数据库服务器，这三种服务器对于硬件要求各不相同。\n 应用服务器：更强大的CPU 数据库服务器：更快速的磁盘和更大的内存 文件服务器：容量更大的硬盘    使用缓存改善性能\n网站的访问也遵循二八定律：80%的业务集中在20%的数据上。因此可以把这一小部分数据缓存在内存中，减少数据库的访问压力。\n网站的缓存可以分为两种：\n 本地缓存：缓存在应用服务器上。本地缓存访问速度快，但是受制于内存限制，缓存数量有限，而且也会出现和应用程序争抢内存的情况。 远程分布式缓存：以集群的方式，缓存在大内存的专用缓存服务器。可以在理论上做到不受内存容量限制。    使用应用服务器集群提高并发能力\n当一台服务器的处理能力和存储空间不足的时候，不要企图更换更强大的服务器。对于大型网站来说，不管多么强大的服务器，都满足不了网站持续增长的业务需求。此时就可以考虑集群的方式，通过负载均衡调度服务器，可以将来自用户的请求分发到应用服务器集群中的任何一台服务器上。\n  数据库读写分离\n使用缓存后，大部分的数据读操作访问都可以不通过数据库完成，但是仍有部分读操作（如缓存过期，缓存不命中）和全部的写操作需要访问数据库。\n目前大部分数据库都提供主从热备的功能，在写数据的时候，访问主库，主库通过主从复制机制将数据更新同步至从数据库，在读的时候就可以通过从数据库获取数据。\n  使用反向代理和CDN加速网站响应\n在《web性能权威指南》中有讲到，网站性能的瓶颈，大部分时间都浪费在TCP的握手和传输上。因此可以通过CDN和反向代理的方式来加快响应。\nCDN和反向代理的本质都是通过缓存，不同的主要是：\n CDN部署在服务器器上的机房，用户在请求时，从距离自己最近的机房获取数据。 反向代理是部署在中心机房，用户请求到达中心机房之后，首先访问的服务器是反向代理的拂去其，如果反向代理服务器中缓存着用户请求的额资源，就将其返回给用户。    使用分布式文件系统和分布式数据库系统\n随着业务的发展，依旧不能满足的时候，就采用分布式的文件和分布式的数据库系统。\n分布式数据库是数据库拆分的最后手段，只用在单表数据规模特别庞大的时候才使用。更常用的拆分手段是业务分库，将不同的业务数据存储在不同的数据库中。\n  使用NoSQL和搜索引擎\n对数据检索和存储越来越复杂的时候，就可以采用一些非关系型数据库如HBase和非数据库查询技术如ElasticSearch等等\n  业务拆分","title":"大型网站发展历程"},{"content":"在上一篇文章《Geth入门》中，主要讲了开发环境下以太坊geth客户端的使用。今天简单说下私链的配置。\ngenesis.json { \u0026#34;config\u0026#34;: { \u0026#34;chainId\u0026#34;: 10, \u0026#34;homesteadBlock\u0026#34;: 0, \u0026#34;eip155Block\u0026#34;: 0, \u0026#34;eip158Block\u0026#34;: 0 }, \u0026#34;coinbase\u0026#34; : \u0026#34;0x0000000000000000000000000000000000000000\u0026#34;, \u0026#34;difficulty\u0026#34; : \u0026#34;0x40000\u0026#34;, \u0026#34;extraData\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;gasLimit\u0026#34; : \u0026#34;0xffffffff\u0026#34;, \u0026#34;nonce\u0026#34; : \u0026#34;0x0000000000000042\u0026#34;, \u0026#34;mixhash\u0026#34; : \u0026#34;0x0000000000000000000000000000000000000000000000000000000000000000\u0026#34;, \u0026#34;parentHash\u0026#34; : \u0026#34;0x0000000000000000000000000000000000000000000000000000000000000000\u0026#34;, \u0026#34;timestamp\u0026#34; : \u0026#34;0x00\u0026#34;, \u0026#34;alloc\u0026#34;: { } }    参数 描述     nonce nonce就是一个64位随机数，用于挖矿   mixhash 与nonce配合用于挖矿，由上一个区块的一部分生成的hash   difficulty 设置当前区块的难度，如果难度过大，cpu挖矿就很难，这里设置较小难度   alloc 用来预置账号以及账号的以太币数量，因为私有链挖矿比较容易，所以我们不需要预置有币的账号，需要的时候自己创建即可以   coinbase 矿工的账号，随便填   timestamp 设置创世块的时间戳   parentHash 上一个区块的hash值，因为是创世块，所以这个值是0   extraData 附加信息，随便填，可以填你的个性信息   gasLimit 该值设置对GAS的消耗总量限制，用来限制区块能包含的交易信息总和，因为我们是私有链，所以填最大。   config Fatal: failed to write genesis block: genesis has no chain configuration ：这个错误信息，就是说，你的json文件中，缺少config部分。看到这个信息，我们不需要把geth退回到v1.5版本，而是需要加上config部分。    创建创世区块 打开终端，输入以下命令，在当前目录下创建创世区块。\ngeth --datadir \u0026quot;./\u0026quot; init genesis.json 可以发现在当前目录新增了两个文件夹：\n geth中保存的是区块链的相关数据 keystore中保存的是该链条中的用户信息  启动私链 geth --datadir \u0026quot;./\u0026quot; --nodiscover console 2\u0026gt;\u0026gt;geth.log  --datadir：代表以太坊私链的创世区块的地址 --nodiscover：私链不要让公链上的节点发现  也可将此命令写入一个shell文件中，每次启动的时候执行脚本就可以了。\n输入此命令后，就可以进入到geth的控制台中了，在这里可以进行挖矿，智能合约的编写。\n","permalink":"https://zhenfeng-zhu.github.io/posts/geth-%E7%A7%81%E9%93%BE/","summary":"在上一篇文章《Geth入门》中，主要讲了开发环境下以太坊geth客户端的使用。今天简单说下私链的配置。\ngenesis.json { \u0026#34;config\u0026#34;: { \u0026#34;chainId\u0026#34;: 10, \u0026#34;homesteadBlock\u0026#34;: 0, \u0026#34;eip155Block\u0026#34;: 0, \u0026#34;eip158Block\u0026#34;: 0 }, \u0026#34;coinbase\u0026#34; : \u0026#34;0x0000000000000000000000000000000000000000\u0026#34;, \u0026#34;difficulty\u0026#34; : \u0026#34;0x40000\u0026#34;, \u0026#34;extraData\u0026#34; : \u0026#34;\u0026#34;, \u0026#34;gasLimit\u0026#34; : \u0026#34;0xffffffff\u0026#34;, \u0026#34;nonce\u0026#34; : \u0026#34;0x0000000000000042\u0026#34;, \u0026#34;mixhash\u0026#34; : \u0026#34;0x0000000000000000000000000000000000000000000000000000000000000000\u0026#34;, \u0026#34;parentHash\u0026#34; : \u0026#34;0x0000000000000000000000000000000000000000000000000000000000000000\u0026#34;, \u0026#34;timestamp\u0026#34; : \u0026#34;0x00\u0026#34;, \u0026#34;alloc\u0026#34;: { } }    参数 描述     nonce nonce就是一个64位随机数，用于挖矿   mixhash 与nonce配合用于挖矿，由上一个区块的一部分生成的hash   difficulty 设置当前区块的难度，如果难度过大，cpu挖矿就很难，这里设置较小难度   alloc 用来预置账号以及账号的以太币数量，因为私有链挖矿比较容易，所以我们不需要预置有币的账号，需要的时候自己创建即可以   coinbase 矿工的账号，随便填   timestamp 设置创世块的时间戳   parentHash 上一个区块的hash值，因为是创世块，所以这个值是0   extraData 附加信息，随便填，可以填你的个性信息   gasLimit 该值设置对GAS的消耗总量限制，用来限制区块能包含的交易信息总和，因为我们是私有链，所以填最大。   config Fatal: failed to write genesis block: genesis has no chain configuration ：这个错误信息，就是说，你的json文件中，缺少config部分。看到这个信息，我们不需要把geth退回到v1.","title":"Geth 私链"},{"content":"Geth简介 go-ethereum\ngo-ethereum客户端通常被称为geth，它是个命令行界面，执行在Go上实现的完整以太坊节点。通过安装和运行geth，可以参与到以太坊前台实时网络并进行以下操作：\n 挖掘真的以太币 在不同地址间转移资金 创建合约，发送交易 探索区块历史 及很多其他   网站: http://ethereum.github.io/go-ethereum/\n  Github: https://github.com/ethereum/go-ethereum\n  维基百科: https://github.com/ethereum/go-ethereum/wiki/geth\n  Gitter: https://gitter.im/ethereum/go-ethereum\n mac下安装geth  首先安装homebrew， 使用brew安装即可。在安装geth的时候，会将go也安装上。  brew tap ethereum/ethereum brew install ethereum  在命令行输入geth —help，如果出现\nzhuzhenengdeMBP:blog zhuzhenfeng$ geth --help NAME: geth - the go-ethereum command line interface Copyright 2013-2017 The go-ethereum Authors USAGE: geth [options] command [command options] [arguments...] VERSION: 1.7.3-unstable-eea996e4 证明安装成功。\n  使用Geth   打开终端，输入以下命令，以开发的方式启动geth\ngeth --datadir “~/Documents/github/ethfans/ethdev” --dev \u0026ndash;datadir 是指定geth的开发目录，引号的路径可以随便设置\n  新开一个终端，执行以下命令，进入geth的控制台\ngeth --dev console 2\u0026gt;\u0026gt;file_to_log_output 该命令会将在console中执行的命令，生成一个文本保存在file_to_log_output文件中。\n  再新开一个终端，查看打印出来的日志\ntail -f file_to_log_output   切换到geth控制台终端，geth有如下常用的命令\n  eth.accounts\n查看有什么账户\n  personal.newAccount('密码')\n创建一个账户\n  user1=eth.accounts[0]\n可以把账户赋值给某一个变量\n  eth.getBalance(user1)\n获取某一账户的余额\n  miner.start()\n启动挖矿程序\n  miner.stop()\n停止挖矿程序\n  eth.sendTransaction({from: user1,to: user2,value: web3.toWei(3,\u0026quot;ether\u0026quot;)})\n从user1向user2转以太币\n  personal.unlockAccount(user1, '密码')\n解锁账户\n  以太坊启动挖矿程序的时候，头结点会产生以太币，在进行转账操作之后，必须进行挖矿才会使交易成功。\n","permalink":"https://zhenfeng-zhu.github.io/posts/geth/","summary":"Geth简介 go-ethereum\ngo-ethereum客户端通常被称为geth，它是个命令行界面，执行在Go上实现的完整以太坊节点。通过安装和运行geth，可以参与到以太坊前台实时网络并进行以下操作：\n 挖掘真的以太币 在不同地址间转移资金 创建合约，发送交易 探索区块历史 及很多其他   网站: http://ethereum.github.io/go-ethereum/\n  Github: https://github.com/ethereum/go-ethereum\n  维基百科: https://github.com/ethereum/go-ethereum/wiki/geth\n  Gitter: https://gitter.im/ethereum/go-ethereum\n mac下安装geth  首先安装homebrew， 使用brew安装即可。在安装geth的时候，会将go也安装上。  brew tap ethereum/ethereum brew install ethereum  在命令行输入geth —help，如果出现\nzhuzhenengdeMBP:blog zhuzhenfeng$ geth --help NAME: geth - the go-ethereum command line interface Copyright 2013-2017 The go-ethereum Authors USAGE: geth [options] command [command options] [arguments...] VERSION: 1.7.3-unstable-eea996e4 证明安装成功。\n  使用Geth   打开终端，输入以下命令，以开发的方式启动geth","title":"Geth"},{"content":"json 字符串处理  get_json_object lateral_view explode substr json_tuple  get_json_object get_json_object(string json_string, string path)\n解析 json 字符串 json_string，返回 path 指定的内容。如果输入的 json 字符串是无效的，那么返回 null。\npath 就是 \u0026lsquo;$.字段名\u0026rsquo;。\n如果该字段的 value 也是 json，就可以一直点下去。\n如果该字段的 value 是数组，就可以用 \u0026lsquo;$.字段名[0]'，类似这样下标的形式去访问。\nexplode explode(array)\n经常和 lateral view 一起使用，将数组中的元素拆分成多行显示。\nsubstr substr(string A, int start, int len)\n返回字符串 A 从 start 位置开始，长度为 len 的字符串\njson_tuple json_tuple(string json_string, col1, col2, \u0026hellip;)\n经常和 lateral view 一起使用，同时解析多个 json 字符串中的多个字段。\nparse_url, regexp_replace, regexp_extract parse_url parse_url(string urlString, string partToExtract, string keyToExtract)\n返回 url 中的指定部分，如 host，path，query 等等。\npartToExtract 是个枚举值：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO。\nregex_replace regex_extract(string a, string b, string c)\n将字符串 a 中符合正在表达式 b 的部分替换为 c\njson_to_struct json_to_struct(json, \u0026lsquo;array 或者 map 等\u0026rsquo;)\nunion_map union_map(map(k, v))\nlateral view from_unix_time from_unix_time(unix 时间戳, \u0026lsquo;yyyyMMddHH\u0026rsquo;)\nrow_number row_number() over (partition by 字段 a order by 计算项 b desc ) rank\nhive 中的分组和组内排序\n rank 是排序的别名 partition by： 类似于 hive 的建表，分区的意思 order by： 排序，默认是升序，加 desc 降序  这个意思就是按字段 a 分区，对计算项 b 进行降序排列\n这个是经常用到计算分区中的排序问题。\ncoalesce 非空查找函数\ncoalesce(v1, v2, v3, \u0026hellip;)\n返回参数中的第一个非空值，如果所有值都是 NULL，返回 NULL\n","permalink":"https://zhenfeng-zhu.github.io/posts/hive%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/","summary":"json 字符串处理  get_json_object lateral_view explode substr json_tuple  get_json_object get_json_object(string json_string, string path)\n解析 json 字符串 json_string，返回 path 指定的内容。如果输入的 json 字符串是无效的，那么返回 null。\npath 就是 \u0026lsquo;$.字段名\u0026rsquo;。\n如果该字段的 value 也是 json，就可以一直点下去。\n如果该字段的 value 是数组，就可以用 \u0026lsquo;$.字段名[0]'，类似这样下标的形式去访问。\nexplode explode(array)\n经常和 lateral view 一起使用，将数组中的元素拆分成多行显示。\nsubstr substr(string A, int start, int len)\n返回字符串 A 从 start 位置开始，长度为 len 的字符串\njson_tuple json_tuple(string json_string, col1, col2, \u0026hellip;)\n经常和 lateral view 一起使用，同时解析多个 json 字符串中的多个字段。\nparse_url, regexp_replace, regexp_extract parse_url parse_url(string urlString, string partToExtract, string keyToExtract)","title":"hive常用函数"},{"content":"SQL条件语句 IF if(exp1, exp2, exp3)\nexp1是条件，条件为true的话，是exp2，否则是exp3\ncase when case 列名 when 条件 then 结果 else 其他结果 end 别名 IFNULL IFNULL(exp1, exp2)\n在exp1的值不为null的情况下，返回exp1，如果exp1位null，返回exp2的值。\n","permalink":"https://zhenfeng-zhu.github.io/posts/mysql%E7%9A%84%E5%AD%A6%E4%B9%A0/","summary":"SQL条件语句 IF if(exp1, exp2, exp3)\nexp1是条件，条件为true的话，是exp2，否则是exp3\ncase when case 列名 when 条件 then 结果 else 其他结果 end 别名 IFNULL IFNULL(exp1, exp2)\n在exp1的值不为null的情况下，返回exp1，如果exp1位null，返回exp2的值。","title":"mysql的学习"},{"content":"ClickHouse ClickHouse 是一个用于联机分析（Online Analytical Processing：OLAP）的列式数据库管理系统(DBMS)。通过使用 OLAP 工具，用户能够从多个角度交互地分析多维数据。\nOLAP 由三个基本的分析操作组成：上卷（roll-up）、钻取（drill-down）、切片（slicing）和切块（dicing）。\n  上卷（roll-up）：涉及可以在一个或多个维度中累积和计算的数据的聚合。例如，所有的销售办事处汇总到销售部门，以预测销售趋势。\n  钻取（drill-down）：是一种允许用户浏览详细信息的技术。例如，用户可以查看组成一个地区销售额的单个产品的销售额。\n  切片（slicing）和切块（dicing）：用户可以从 OLAP 多维数据集中取出（切片）一组特定的数据，并从不同的角度查看（切块）切片。这些角度有时被称为维度（例如按销售人员、按日期、按客户、按产品或按地区查看相同的销售情况等）。\n  传统行式数据库中，处于同一行中的数据总是被物理的存在一起。列式数据库总是将同一列的数据存储在一起，不同列的数据分开存储。\n行式数据库：mysql，pg\n列式数据库：vertica，druid\nOLAP的关键特征  大多是读请求 数据总是以相当大的批（\u0026gt;1000w）进行写入 不修改已经添加的数据 每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列 宽表，即每个表包含大量的列 较少的查询（通常每台服务器每秒数百个查询或更少） 对于简单的查询，允许延迟大约50ms 列中的数据相对较小：数字和短字符串 处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行） 事务不是必须的 对数据一致性要求低 每一个查询除了一个大表外都很小 查询结果明显小于数据源，换句话说，数据被过滤或者聚合之后能够被放在单台服务器的内存中  列式数据库更适合OLAP场景 Input/Output  分析类的查询，通常只需要读取表的一小部分列。 数据总是打包成批量读取，列压缩更容易 IO降低了  CPU 由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。\n 向量引擎 代码生成  为了提高CPU效率，查询语言必须是声明型的(SQL或MDX)， 或者至少一个向量(J，K)。 查询应该只包含隐式循环，允许进行优化。\nclickhouse的独特功能 真正的列式数据库管理系统    ","permalink":"https://zhenfeng-zhu.github.io/posts/clickhouse/","summary":"ClickHouse ClickHouse 是一个用于联机分析（Online Analytical Processing：OLAP）的列式数据库管理系统(DBMS)。通过使用 OLAP 工具，用户能够从多个角度交互地分析多维数据。\nOLAP 由三个基本的分析操作组成：上卷（roll-up）、钻取（drill-down）、切片（slicing）和切块（dicing）。\n  上卷（roll-up）：涉及可以在一个或多个维度中累积和计算的数据的聚合。例如，所有的销售办事处汇总到销售部门，以预测销售趋势。\n  钻取（drill-down）：是一种允许用户浏览详细信息的技术。例如，用户可以查看组成一个地区销售额的单个产品的销售额。\n  切片（slicing）和切块（dicing）：用户可以从 OLAP 多维数据集中取出（切片）一组特定的数据，并从不同的角度查看（切块）切片。这些角度有时被称为维度（例如按销售人员、按日期、按客户、按产品或按地区查看相同的销售情况等）。\n  传统行式数据库中，处于同一行中的数据总是被物理的存在一起。列式数据库总是将同一列的数据存储在一起，不同列的数据分开存储。\n行式数据库：mysql，pg\n列式数据库：vertica，druid\nOLAP的关键特征  大多是读请求 数据总是以相当大的批（\u0026gt;1000w）进行写入 不修改已经添加的数据 每次查询都从数据库中读取大量的行，但是同时又仅需要少量的列 宽表，即每个表包含大量的列 较少的查询（通常每台服务器每秒数百个查询或更少） 对于简单的查询，允许延迟大约50ms 列中的数据相对较小：数字和短字符串 处理单个查询时需要高吞吐量（每个服务器每秒高达数十亿行） 事务不是必须的 对数据一致性要求低 每一个查询除了一个大表外都很小 查询结果明显小于数据源，换句话说，数据被过滤或者聚合之后能够被放在单台服务器的内存中  列式数据库更适合OLAP场景 Input/Output  分析类的查询，通常只需要读取表的一小部分列。 数据总是打包成批量读取，列压缩更容易 IO降低了  CPU 由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。\n 向量引擎 代码生成  为了提高CPU效率，查询语言必须是声明型的(SQL或MDX)， 或者至少一个向量(J，K)。 查询应该只包含隐式循环，允许进行优化。\nclickhouse的独特功能 真正的列式数据库管理系统    ","title":"clickhouse"},{"content":"https://mubu.com/doc/oHlgG0FSu0\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E6%AF%8F%E6%97%A5%E5%AD%A6%E4%B9%A0-2019-09-29/","summary":"https://mubu.com/doc/oHlgG0FSu0","title":"每日学习2"},{"content":"开言英语 极客时间 编译原理之美 语义分析（下）：如何做上下文相关情况的处理？  语义分析的本质，就是针对上下文相关的情况做处理。  引用消解：不同作用域里可能有相同名称的变量，必须找到正确的那个，这个过程就是引用消解。  函数引用消解 命名空间引用消解   左值和右值  左值取的是变量的地址或者说是变量的引用，获得地址之后，我们就可以把新值写进去。 右值就是我们常说的值。 不是所有的表达式都能生成一个合格的左值。   属性计算  上下文分析或者说语义分析的一种算法。 属性文法的主要思路是计算机科学的重要开拓者，是在上下文无关文法的基础上做了一些增强，使之可以计算属性值。   过程  类型和作用域解析 类型的消解 引用的消解和S属性的类型推导 做类型检查 做一些语义合法性检查      趣谈Linux操作系统 Namespace技术：内部创业公司应该独立运营 为了隔离不同类型的资源，Linux内核里面有如下几种不同类型的namespace：\n UTS，表示不同的namespace可以配置不同的hostname User，可以配置不同的用户和组 Mount，文件系统挂载点是隔离的 PID，有完全独立的pid Network，有独立的网络协议栈  ","permalink":"https://zhenfeng-zhu.github.io/posts/%E6%AF%8F%E6%97%A5%E5%AD%A6%E4%B9%A0-2019-09-24/","summary":"开言英语 极客时间 编译原理之美 语义分析（下）：如何做上下文相关情况的处理？  语义分析的本质，就是针对上下文相关的情况做处理。  引用消解：不同作用域里可能有相同名称的变量，必须找到正确的那个，这个过程就是引用消解。  函数引用消解 命名空间引用消解   左值和右值  左值取的是变量的地址或者说是变量的引用，获得地址之后，我们就可以把新值写进去。 右值就是我们常说的值。 不是所有的表达式都能生成一个合格的左值。   属性计算  上下文分析或者说语义分析的一种算法。 属性文法的主要思路是计算机科学的重要开拓者，是在上下文无关文法的基础上做了一些增强，使之可以计算属性值。   过程  类型和作用域解析 类型的消解 引用的消解和S属性的类型推导 做类型检查 做一些语义合法性检查      趣谈Linux操作系统 Namespace技术：内部创业公司应该独立运营 为了隔离不同类型的资源，Linux内核里面有如下几种不同类型的namespace：\n UTS，表示不同的namespace可以配置不同的hostname User，可以配置不同的用户和组 Mount，文件系统挂载点是隔离的 PID，有完全独立的pid Network，有独立的网络协议栈  ","title":"每日学习-2019-09-24"},{"content":"折腾一下 tmux\n安装 brew install tmux 概念  session：理解为一个会话，持久保存工作状态。 window：可以理解为我们常说的 tab 页。 pane：一个 window 被分成若干个 pane，理解为 iterm 的分屏。  session 新建\ntmux new -s your-session-name 断开\ntmux detach 恢复\ntmux attach-session -t your-session-name 或者 tmux a -t your-session-name 关闭\n kill-server kill-session kill-window kill-pane  tmux kill-session -t your-session-name tmux kill-server 查看\ntmux list-session tmux ls tmux 的基础配置 prefix 是 tmux 的前缀键，默认是 ctrl+b 。只有按下前缀键，才会激活 tmux，然后再按其他的键进行 tmux 操作。这样可以避免与其他应用的快捷键进行冲突。\n配置前缀 需要去tmux.conf中去配置\n分屏 水平分屏：prefix+\u0026quot;，前缀键加引号 垂直分屏：prefix+%，前缀键加百分号\n","permalink":"https://zhenfeng-zhu.github.io/posts/tmux/","summary":"折腾一下 tmux\n安装 brew install tmux 概念  session：理解为一个会话，持久保存工作状态。 window：可以理解为我们常说的 tab 页。 pane：一个 window 被分成若干个 pane，理解为 iterm 的分屏。  session 新建\ntmux new -s your-session-name 断开\ntmux detach 恢复\ntmux attach-session -t your-session-name 或者 tmux a -t your-session-name 关闭\n kill-server kill-session kill-window kill-pane  tmux kill-session -t your-session-name tmux kill-server 查看\ntmux list-session tmux ls tmux 的基础配置 prefix 是 tmux 的前缀键，默认是 ctrl+b 。只有按下前缀键，才会激活 tmux，然后再按其他的键进行 tmux 操作。这样可以避免与其他应用的快捷键进行冲突。\n配置前缀 需要去tmux.conf中去配置\n分屏 水平分屏：prefix+\u0026quot;，前缀键加引号 垂直分屏：prefix+%，前缀键加百分号","title":"tmux"},{"content":"突然搞明白了 crystal 的 vscode 插件的正确使用姿势，记录一下。\n安装 crystal brew install crystal 安装 vscode 插件 https://marketplace.visualstudio.com/items?itemName=faustinoaq.crystal-lang\n安装 scry scry 是 crystal 的 language server 的 client 工具，在本地安装 scry 就可以做到代码跳转了。\n$ git clone https://github.com/crystal-lang-tools/scry.git $ cd scry $ shards build -v Dependencies are satisfied Building: scry crystal build -o /Users/lucas/Documents/demos/crystal/scry/bin/scry src/scry.cr /Users/lucas/Documents/demos/crystal/scry/bin/scry 就是编译出来的二进制的路径\n配置插件 \u0026#34;crystal-lang.compiler\u0026#34;: \u0026#34;crystal\u0026#34;, \u0026#34;crystal-lang.server\u0026#34;: \u0026#34;/Users/lucas/Documents/demos/crystal/scry/bin/scry\u0026#34;, \u0026#34;crystal-lang.maxNumberOfProblems\u0026#34;: 20, \u0026#34;crystal-lang.mainFile\u0026#34;: \u0026#34;${workspaceRoot}/src/main.cr\u0026#34;, \u0026#34;crystal-lang.processesLimit\u0026#34;: 5, \u0026#34;crystal-lang.hover\u0026#34;: true, \u0026#34;crystal-lang.problems\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;crystal-lang.implementations\u0026#34;: true, \u0026#34;crystal-lang.completion\u0026#34;: true, \u0026#34;crystal-lang.logLevel\u0026#34;: \u0026#34;info\u0026#34;, 把上面的配置加到 vscode 的 settings 文件中，就可以愉快的开发啦。\n","permalink":"https://zhenfeng-zhu.github.io/posts/crystal%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","summary":"突然搞明白了 crystal 的 vscode 插件的正确使用姿势，记录一下。\n安装 crystal brew install crystal 安装 vscode 插件 https://marketplace.visualstudio.com/items?itemName=faustinoaq.crystal-lang\n安装 scry scry 是 crystal 的 language server 的 client 工具，在本地安装 scry 就可以做到代码跳转了。\n$ git clone https://github.com/crystal-lang-tools/scry.git $ cd scry $ shards build -v Dependencies are satisfied Building: scry crystal build -o /Users/lucas/Documents/demos/crystal/scry/bin/scry src/scry.cr /Users/lucas/Documents/demos/crystal/scry/bin/scry 就是编译出来的二进制的路径\n配置插件 \u0026#34;crystal-lang.compiler\u0026#34;: \u0026#34;crystal\u0026#34;, \u0026#34;crystal-lang.server\u0026#34;: \u0026#34;/Users/lucas/Documents/demos/crystal/scry/bin/scry\u0026#34;, \u0026#34;crystal-lang.maxNumberOfProblems\u0026#34;: 20, \u0026#34;crystal-lang.mainFile\u0026#34;: \u0026#34;${workspaceRoot}/src/main.cr\u0026#34;, \u0026#34;crystal-lang.processesLimit\u0026#34;: 5, \u0026#34;crystal-lang.hover\u0026#34;: true, \u0026#34;crystal-lang.problems\u0026#34;: \u0026#34;build\u0026#34;, \u0026#34;crystal-lang.implementations\u0026#34;: true, \u0026#34;crystal-lang.completion\u0026#34;: true, \u0026#34;crystal-lang.logLevel\u0026#34;: \u0026#34;info\u0026#34;, 把上面的配置加到 vscode 的 settings 文件中，就可以愉快的开发啦。","title":"crystal开发环境"},{"content":"关注 crystal 也有一段时间了，看到多线程的 pr 已经提了，今天简单写一下。\n Fast as C, Slick as Ruby\n 语法 crystal 的语法和 Ruby 比较类似。\n# A very basic HTTP server require \u0026#34;http/server\u0026#34; server = HTTP::Server.new do |context| context.response.content_type = \u0026#34;text/plain\u0026#34; context.response.print \u0026#34;Hello world, got #{context.request.path}!\u0026#34; end puts \u0026#34;Listening on http://127.0.0.1:8080\u0026#34; server.listen(8080) 类型系统 crystal 的一大卖点就是静态类型系统，但是写起来又和脚本语言类似。\ndef shout(x) # Notice that both Int32 and String respond_to `to_s` x.to_s.upcase end foo = ENV[\u0026#34;FOO\u0026#34;]? || 10 typeof(foo) # =\u0026gt; (Int32 | String) typeof(shout(foo)) # =\u0026gt; String 空引用检查 crystal 可以在编译的时候检查空引用，避免出现空指针异常。\nif rand(2) \u0026gt; 0 my_string = \u0026#34;hello world\u0026#34; end puts my_string.upcase 如果运行上述的代码，执行结果如下：\n$ crystal hello_world.cr Error in hello_world.cr:5: undefined method \u0026#39;upcase\u0026#39; for Nil (compile-time type is (String | Nil)) puts my_string.upcase 宏 另一个重要的特性是宏。通过宏，可以实现向 ruby 那么强大的元编程。\nclass Object def has_instance_var?(name) : Bool {{ @type.instance_vars.map \u0026amp;.name.stringify }}.includes? name end end person = Person.new \u0026#34;John\u0026#34;, 30 person.has_instance_var?(\u0026#34;name\u0026#34;) #=\u0026gt; true person.has_instance_var?(\u0026#34;birthday\u0026#34;) #=\u0026gt; false 并发 crystal 的并发是通过绿色线程实现的，即 fibers。和 Go 的并发模式很像，也是基于 channel 的 CSP 模型。\nchannel = Channel(Int32).new total_lines = 0 files = Dir.glob(\u0026#34;*.txt\u0026#34;) files.each do |f| spawn do lines = File.read(f).lines.size channel.send lines end end files.size.times do total_lines += channel.receive end puts total_lines C 绑定 C 语言一般用来实现比较底层的系统，而且 C 的生态丰富，一般现代语言都会提供 C 绑定，来复用 C 的生态。\n# Fragment of the BigInt implementation that uses GMP @[Link(\u0026#34;gmp\u0026#34;)] lib LibGMP alias Int = LibC::Int alias ULong = LibC::ULong struct MPZ _mp_alloc : Int32 _mp_size : Int32 _mp_d : ULong* end fun init_set_str = __gmpz_init_set_str(rop : MPZ*, str : UInt8*, base : Int) : Int fun cmp = __gmpz_cmp(op1 : MPZ*, op2 : MPZ*) : Int end struct BigInt \u0026lt; Int def initialize(str : String, base = 10) err = LibGMP.init_set_str(out @mpz, str, base) raise ArgumentError.new(\u0026#34;invalid BigInt: #{str}\u0026#34;) if err == -1 end def \u0026lt;=\u0026gt;(other : BigInt) LibGMP.cmp(mpz, other) end end 依赖管理 任何一个偏工程性的语言，都会提供一个包管理系统。crystal 的包管理是 shards，其实和 go module 类似。这种项目级别的包管理其实更为实用一些。\n但是 go 的任何一个项目，其实都可以是一个包，crystal 还是会有一些限制的。\nname: my-project version: 0.1 license: MIT crystal: 0.21.0 dependencies: mysql: github: crystal-lang/crystal-mysql version: ~\u0026gt; 0.3.1 ","permalink":"https://zhenfeng-zhu.github.io/posts/crystal%E7%AE%80%E4%BB%8B/","summary":"关注 crystal 也有一段时间了，看到多线程的 pr 已经提了，今天简单写一下。\n Fast as C, Slick as Ruby\n 语法 crystal 的语法和 Ruby 比较类似。\n# A very basic HTTP server require \u0026#34;http/server\u0026#34; server = HTTP::Server.new do |context| context.response.content_type = \u0026#34;text/plain\u0026#34; context.response.print \u0026#34;Hello world, got #{context.request.path}!\u0026#34; end puts \u0026#34;Listening on http://127.0.0.1:8080\u0026#34; server.listen(8080) 类型系统 crystal 的一大卖点就是静态类型系统，但是写起来又和脚本语言类似。\ndef shout(x) # Notice that both Int32 and String respond_to `to_s` x.to_s.upcase end foo = ENV[\u0026#34;FOO\u0026#34;]? || 10 typeof(foo) # =\u0026gt; (Int32 | String) typeof(shout(foo)) # =\u0026gt; String 空引用检查 crystal 可以在编译的时候检查空引用，避免出现空指针异常。","title":"crystal简介"},{"content":"Socket 网络模型 osi七层模型  应用层 表示层 会话层 传输层 网络层 数据链路层 物理层  对应的tcpip就是  应用层  dns http   传输层  icmp tcp udp   ip层  ipv4 ipv6   mac层  arp vlan   物理层  Ethernet    为什么要分层 因为网络环境过于复杂，不是一个能够集中控制的体系。全球的服务器和设备各有各的体系，但是可以通过同一套网络协议栈切分成多个层次和组合，来满足不同设备之间的通信需求。\n二层到四层，即mac、ip和传输等层都是Linux内核中处理。应用层的如浏览器、Nginx和Tomcat等都是用户态的。\n传输层的tcp和udp里都有端口的概念，不同应用监听不同的段即可。\n应用层和内核的互通机制，就是通过socket系统调用。其实socket哪一层都不属于，它是属于操作系统的概念，而不是网络分层的概念。因为操作系统把二层到四层的处理代码在内核里，应用层的处理代码让应用自己做，两者需要跨内核态和用户态进行通信，这个就是socket。\nTCP和UDP的区别  tcp是面向连接的，udp是面向无连接的 tcp提供可靠交付，无差错、不丢失、不重复、并且按序到达。udp不提供可靠交付，可能丢失，不按顺序。 tcp是面向字节流的，发送的是一个流，无头无尾。udp是数据报文的，一个一个发送。 tcp可以提供流量控制和拥塞控制，可以防止对端被压垮，也防止网络被压垮。  所谓的连接，指两端的数据结构状态的协同，两边状态对的上，符合tcp协议的规则，就认为连接是存在的，否则就是断掉的。\n所谓的建立连接，其实是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态。并用这样的数据结构来保证面向连接的特性。tcp无法左右中间的任何通路，也没有什么虚拟的连接。\n所谓的可靠，也是两端的数据结构做的事情。不丢失其实是数据结构在“点名”，顺序到达是数据结构在“排序”，面向数据流其实是数据结构将零散的包，按照顺序捏成一个流发给应用层。\n所谓的流量控制和拥塞控制，其实就是根据收到的对端的网络包，调整两端的数据结构状态。\nsocket函数 int socket(int domain, int type, int protocol) socket函数用于创建一个socket文件描述符。\n domain  使用什么ip层的协议。AF_INET标识ipv4，AF_INET6标识ipv6。   type  socket的类型。 SOCK_STREAM，tcp流的 SOCK_DGRAM，udp报文的 SOCK_RAW，可以直接操作ip层，或非tcp和udp类型的   protocol  协议 IPPROTO_TCP, IPPROTO_UDP    ","permalink":"https://zhenfeng-zhu.github.io/posts/socket/","summary":"Socket 网络模型 osi七层模型  应用层 表示层 会话层 传输层 网络层 数据链路层 物理层  对应的tcpip就是  应用层  dns http   传输层  icmp tcp udp   ip层  ipv4 ipv6   mac层  arp vlan   物理层  Ethernet    为什么要分层 因为网络环境过于复杂，不是一个能够集中控制的体系。全球的服务器和设备各有各的体系，但是可以通过同一套网络协议栈切分成多个层次和组合，来满足不同设备之间的通信需求。\n二层到四层，即mac、ip和传输等层都是Linux内核中处理。应用层的如浏览器、Nginx和Tomcat等都是用户态的。\n传输层的tcp和udp里都有端口的概念，不同应用监听不同的段即可。\n应用层和内核的互通机制，就是通过socket系统调用。其实socket哪一层都不属于，它是属于操作系统的概念，而不是网络分层的概念。因为操作系统把二层到四层的处理代码在内核里，应用层的处理代码让应用自己做，两者需要跨内核态和用户态进行通信，这个就是socket。\nTCP和UDP的区别  tcp是面向连接的，udp是面向无连接的 tcp提供可靠交付，无差错、不丢失、不重复、并且按序到达。udp不提供可靠交付，可能丢失，不按顺序。 tcp是面向字节流的，发送的是一个流，无头无尾。udp是数据报文的，一个一个发送。 tcp可以提供流量控制和拥塞控制，可以防止对端被压垮，也防止网络被压垮。  所谓的连接，指两端的数据结构状态的协同，两边状态对的上，符合tcp协议的规则，就认为连接是存在的，否则就是断掉的。\n所谓的建立连接，其实是为了在客户端和服务端维护连接，而建立一定的数据结构来维护双方交互的状态。并用这样的数据结构来保证面向连接的特性。tcp无法左右中间的任何通路，也没有什么虚拟的连接。\n所谓的可靠，也是两端的数据结构做的事情。不丢失其实是数据结构在“点名”，顺序到达是数据结构在“排序”，面向数据流其实是数据结构将零散的包，按照顺序捏成一个流发给应用层。\n所谓的流量控制和拥塞控制，其实就是根据收到的对端的网络包，调整两端的数据结构状态。\nsocket函数 int socket(int domain, int type, int protocol) socket函数用于创建一个socket文件描述符。","title":"socket"},{"content":"Diagnostics go提供了一系列诊断逻辑和性能问题的工具。\n profiling分析 tracing跟踪 debuging调试 运行时统计信息和事件  Profiling profiling信息可以在go test或者net/http/pprof包的时候使用。\nruntime/pprof包有：\n cpu  主动消费cpu周期所花费的时间，不包括睡眠或者io等待   heap  报告内存分配采样； 当前或历史内存使用状况 检测内存泄露   threadcreate  报告创建新的系统线程   goroutine  当前所有协程的堆栈跟踪   block  显示goroutine阻塞等待同步原语的位置。 默认不开启，使用runtime.SetBlockProfileRate启用   mutex  报告锁竞争。 如果认为自己的程序因为互斥锁导致cpu不能充分利用的时候，使用这个。 默认也是不开启，使用 runtime.SetMutexProfileFraction 启用。    其他可用的的性能分析工具\nLinux使用https://perf.wiki.kernel.org/index.php/Tutorial，perf可以分析cgo/SWIG代码和系统内核。\nmac上使用 https://developer.apple.com/library/content/documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/ 就足够了。\n分析线上处于生产状态服务\n在生产上分析程序也是没问题的，但是开启某些指标会增加成本。\n可视化分析数据\ngo 提供了很多可视化的工具，参考https://blog.golang.org/profiling-go-programs\n也可以创建自定义的profil文件：参考https://golang.org/pkg/runtime/pprof/#Profile\n也可以自定义修改pprof程序监听的端口和路径，参考：\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/pprof\u0026#34; ) func main() { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/custom_debug_path/profile\u0026#34;, pprof.Profile) log.Fatal(http.ListenAndServe(\u0026#34;:7777\u0026#34;, mux)) } ","permalink":"https://zhenfeng-zhu.github.io/posts/go%E8%BF%9B%E9%98%B6/","summary":"Diagnostics go提供了一系列诊断逻辑和性能问题的工具。\n profiling分析 tracing跟踪 debuging调试 运行时统计信息和事件  Profiling profiling信息可以在go test或者net/http/pprof包的时候使用。\nruntime/pprof包有：\n cpu  主动消费cpu周期所花费的时间，不包括睡眠或者io等待   heap  报告内存分配采样； 当前或历史内存使用状况 检测内存泄露   threadcreate  报告创建新的系统线程   goroutine  当前所有协程的堆栈跟踪   block  显示goroutine阻塞等待同步原语的位置。 默认不开启，使用runtime.SetBlockProfileRate启用   mutex  报告锁竞争。 如果认为自己的程序因为互斥锁导致cpu不能充分利用的时候，使用这个。 默认也是不开启，使用 runtime.SetMutexProfileFraction 启用。    其他可用的的性能分析工具\nLinux使用https://perf.wiki.kernel.org/index.php/Tutorial，perf可以分析cgo/SWIG代码和系统内核。\nmac上使用 https://developer.apple.com/library/content/documentation/DeveloperTools/Conceptual/InstrumentsUserGuide/ 就足够了。\n分析线上处于生产状态服务\n在生产上分析程序也是没问题的，但是开启某些指标会增加成本。\n可视化分析数据\ngo 提供了很多可视化的工具，参考https://blog.golang.org/profiling-go-programs\n也可以创建自定义的profil文件：参考https://golang.org/pkg/runtime/pprof/#Profile\n也可以自定义修改pprof程序监听的端口和路径，参考：\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/pprof\u0026#34; ) func main() { mux := http.","title":"go进阶"},{"content":"MySQL基本架构 客户端\nserver层\n  连接器：管理连接，权限验证\n  查询缓存：命中规则，直接返回结果 8.0之后全部删除了这个模块\n  分析器：词法分析，语法分析\n  优化器：执行计划生成，索引选择\n  执行器：操作引擎，返回结果\n  存储引擎：存储数据，提供读写接口\n数据库中的长连接指连接成功之后，如果客户端持续有请求，则一直使用同一个连接。短连接是指每次执行完很少的几次查询之后就断开连接，下次再重新建立。\n如果全部使用长连接，会导致mysql内存涨的很快，可能出现OOM，因此要定期断开长连接，或者在执行一个比较大的操作之后，执行mysql_reset_connection重置一下。\n日志系统 redo log重做日志 redo log是innodb引擎特有的。物理日志，记录的是某个数据页上做了什么修改。循环写入。\nWAL技术：Write-Ahead Logging：关键点就是先写日志，再写磁盘。当一条记录更新时，先把记录写到redolog中，更新到内存，这时这个更新操作就成功了。然后innodb引擎就会在适当的时候，将这个操作记录更新到磁盘中。因此在数据库异常重启的时候，之前的提交的记录不会丢失。\nbinlog归档日志 binlog是server层实现的，所有的引擎都可以使用。binlog是逻辑日志，记录的是这个语句的原始逻辑。binlog是写到一定大小后，切换下一个，不会覆盖以前的日志。\n因此一个update操作就是：\n找到该行\n判断数据页是否在内存中，如果是返回行数据，否则从磁盘读入到内存中。\n将值进行更新，写入新行\n新行更新到内存\n写入redolog，处于prepare阶段\n写入binlog\n提交事务，处于commit阶段。\n这个就是两阶段提交。\n事务隔离 Isolation：隔离性\n脏读，幻读，不可重复读\n隔离的越严实，效率越低。\nSQL的标准隔离级别：\n读未提交：一个事务没提交的时候，它做的变更就能被别的事务看到\n读提交：一个事务提交之后，做的变更才能被其他事务看到\n可重复读：一个事务执行时看到的数据，总是跟这个事务启动时看到的数据时一致的。\n串行化：顾名思义，对于同一行记录，写会加写锁，读也会加读锁。当读写锁冲突时，后访问的事务，必须等前一个事务完成。\n在实现的时候，数据库会创建一个视图，访问的时候以视图的逻辑结果为准。\n 可重复读，这个视图是在事务启动时创建，整个事务存在期间都用这个视图。 读提交，这个视图在每个sql语句开始执行的时候创建 读未提交直接返回记录的最新值，没有视图的概念。 串行化是用加锁的方式。  mysql在每条记录更新的时候，都会记录一条回滚操作，记录上的最新值都可以通过回滚操作，得到前一个状态的值。当没有事务需要用到回滚日志时，就会被删除。所以不建议使用长事务，这样会占用存储空间和锁。\nmysql启动事务的方式\n 显式启动：begin或者start transaction。配套的提交语句是commit，回滚语句是rollback。 set autocommit=0这个命令会将这个线程的自动提交关闭。意味着如果只执行一个select语句，事务就启动了，而且不会自动关闭，除非主动执行commit或者rollback，或者断开连接。  因此一般set autocommit=1，打开显示启动的模式。\n","permalink":"https://zhenfeng-zhu.github.io/posts/mysql/","summary":"MySQL基本架构 客户端\nserver层\n  连接器：管理连接，权限验证\n  查询缓存：命中规则，直接返回结果 8.0之后全部删除了这个模块\n  分析器：词法分析，语法分析\n  优化器：执行计划生成，索引选择\n  执行器：操作引擎，返回结果\n  存储引擎：存储数据，提供读写接口\n数据库中的长连接指连接成功之后，如果客户端持续有请求，则一直使用同一个连接。短连接是指每次执行完很少的几次查询之后就断开连接，下次再重新建立。\n如果全部使用长连接，会导致mysql内存涨的很快，可能出现OOM，因此要定期断开长连接，或者在执行一个比较大的操作之后，执行mysql_reset_connection重置一下。\n日志系统 redo log重做日志 redo log是innodb引擎特有的。物理日志，记录的是某个数据页上做了什么修改。循环写入。\nWAL技术：Write-Ahead Logging：关键点就是先写日志，再写磁盘。当一条记录更新时，先把记录写到redolog中，更新到内存，这时这个更新操作就成功了。然后innodb引擎就会在适当的时候，将这个操作记录更新到磁盘中。因此在数据库异常重启的时候，之前的提交的记录不会丢失。\nbinlog归档日志 binlog是server层实现的，所有的引擎都可以使用。binlog是逻辑日志，记录的是这个语句的原始逻辑。binlog是写到一定大小后，切换下一个，不会覆盖以前的日志。\n因此一个update操作就是：\n找到该行\n判断数据页是否在内存中，如果是返回行数据，否则从磁盘读入到内存中。\n将值进行更新，写入新行\n新行更新到内存\n写入redolog，处于prepare阶段\n写入binlog\n提交事务，处于commit阶段。\n这个就是两阶段提交。\n事务隔离 Isolation：隔离性\n脏读，幻读，不可重复读\n隔离的越严实，效率越低。\nSQL的标准隔离级别：\n读未提交：一个事务没提交的时候，它做的变更就能被别的事务看到\n读提交：一个事务提交之后，做的变更才能被其他事务看到\n可重复读：一个事务执行时看到的数据，总是跟这个事务启动时看到的数据时一致的。\n串行化：顾名思义，对于同一行记录，写会加写锁，读也会加读锁。当读写锁冲突时，后访问的事务，必须等前一个事务完成。\n在实现的时候，数据库会创建一个视图，访问的时候以视图的逻辑结果为准。\n 可重复读，这个视图是在事务启动时创建，整个事务存在期间都用这个视图。 读提交，这个视图在每个sql语句开始执行的时候创建 读未提交直接返回记录的最新值，没有视图的概念。 串行化是用加锁的方式。  mysql在每条记录更新的时候，都会记录一条回滚操作，记录上的最新值都可以通过回滚操作，得到前一个状态的值。当没有事务需要用到回滚日志时，就会被删除。所以不建议使用长事务，这样会占用存储空间和锁。\nmysql启动事务的方式\n 显式启动：begin或者start transaction。配套的提交语句是commit，回滚语句是rollback。 set autocommit=0这个命令会将这个线程的自动提交关闭。意味着如果只执行一个select语句，事务就启动了，而且不会自动关闭，除非主动执行commit或者rollback，或者断开连接。  因此一般set autocommit=1，打开显示启动的模式。","title":"mysql"},{"content":"graphql经常被认为是聚焦于前端的技术。\n核心概念 SDL：schema definition language（模式定义语言） 如：\ntype Person{ name: String! age: Int! } 这个类型有两个字段，name和age，他们的类型是String和Int。！的意思代表他们是必需的。\ntype Post{ title: String! author: Person! } 接下来的Post也有两个字段，其中Person也是可以作为一个类型。\n也可以这样，在Person中添加一个post：\ntype Person{ name: String! age: Int! posts: [Post!]! } 通过Query获取数据 基本查询 客户端发送下面的数据给服务器\n{ allPersons { name } } allPersons是根字段（root field），它下面的成为查询的payload，这里仅包含了一个name。\n服务器返回的结果会是这样的：\n{ \u0026#34;allPersons\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Johnny\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Sarah\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34; } ] } 可以看到只返回了name字段，age字段是不会返回的。\n如果使用如下的payload就会返回：\n{ allPersons { name age } } 还可以查询posts中的title：\n{ allPersons { name age posts { title } } } 带参数查询 在graphql中每个字段都有0或者更多个参数。比如allPerson有一个last参数，只返回最后两个人的信息，这里就是查询的语句：\n{ allPersons(last: 2) { name } } 通过Mutation写数据  创建 更新 删除  mutation和query类似，只是需要加上mutation关键字。如：\nmutation { createPerson(name: \u0026#34;Bob\u0026#34;, age: 36) { name age } } mutation也有一个根字段，叫createPerson。我们知道这个字段有两个参数name和age。返回值会像这样：\n{ \u0026#34;data\u0026#34;: { \u0026#34;createPerson\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Bob\u0026#34;, \u0026#34;age\u0026#34;: 36 } } } graphql会给每个记录新增一个唯一的ID字段，我们也可以这样设置Person类型：\ntype Person { id: ID! name: String! age: Int! } 然后当一个新的Person对象创建时，就可以访问到id。\n通过订阅实时更新 graphql提供了实时订阅更新。\n当客户端订阅一个事件的时候，将会保持一个和服务器的稳定连接，当有变化时会告诉客户端。\nsubscription { newPerson { name age } } 因此当有个用户创建或者修改时都会告诉客户端：\n{ \u0026quot;newPerson\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;Jane\u0026quot;, \u0026quot;age\u0026quot;: 23 } } 定义一个模式 有几个特殊的根类型：\ntype Query { ... } type Mutation { ... } type Subscription { ... } API的根字段都是在上面这三个之下，如：\ntype Query { allPersons: [Person!]! } allPersons也可以有参数：\ntype Query { allPersons(last: Int): [Person!]! } 类似的mutation也是：\ntype Mutation { createPerson(name: String!, age: Int!): Person! } 订阅也是：\ntype Subscription { newPerson: Person! } 把他们放在一起就是：\ntype Query { allPersons(last: Int): [Person!]! } type Mutation { createPerson(name: String!, age: Int!): Person! } type Subscription { newPerson: Person! } type Person { name: String! age: Int! posts: [Post!]! } type Post { title: String! author: Person! } 架构图 graphql直连数据库\ngraphql连接层连接多个服务\ngraphql混连数据库和服务\n解析函数 每个字段其实都有一个解析器，叫resolver。\n当服务器收到一个请求时，会调用字段的resolver函数，一旦resolver函数有返回，服务器就会把数据包装成要返回的字段。\n有这样一个类型：\ntype Query { author(id: ID!): Author } type Author { posts: [Post] } type Post { title: String content: String } 当执行一个query的时候：\nquery { author(id: \u0026#34;abc\u0026#34;) { posts { title content } } } 会以如下的方式执行：\nQuery.author(root, { id: \u0026#39;abc\u0026#39; }, context) -\u0026gt; author Author.posts(author, null, context) -\u0026gt; posts for each post in posts Post.title(post, null, context) -\u0026gt; title Post.content(post, null, context) -\u0026gt; content 实战 ","permalink":"https://zhenfeng-zhu.github.io/posts/graphql/","summary":"graphql经常被认为是聚焦于前端的技术。\n核心概念 SDL：schema definition language（模式定义语言） 如：\ntype Person{ name: String! age: Int! } 这个类型有两个字段，name和age，他们的类型是String和Int。！的意思代表他们是必需的。\ntype Post{ title: String! author: Person! } 接下来的Post也有两个字段，其中Person也是可以作为一个类型。\n也可以这样，在Person中添加一个post：\ntype Person{ name: String! age: Int! posts: [Post!]! } 通过Query获取数据 基本查询 客户端发送下面的数据给服务器\n{ allPersons { name } } allPersons是根字段（root field），它下面的成为查询的payload，这里仅包含了一个name。\n服务器返回的结果会是这样的：\n{ \u0026#34;allPersons\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;Johnny\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Sarah\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Alice\u0026#34; } ] } 可以看到只返回了name字段，age字段是不会返回的。\n如果使用如下的payload就会返回：\n{ allPersons { name age } } 还可以查询posts中的title：","title":"graphql"},{"content":" 短变量名称在声明和上次使用之间的距离很短时效果很好。 长变量名称需要证明自己的合理性; 名称越长，需要提供的价值越高。冗长的名称与页面上的重量相比，信号量较小。 请勿在变量名称中包含类型名称。 常量应该描述它们持有的值，而不是该如何使用。 对于循环和分支使用单字母变量，参数和返回值使用单个字，函数和包级别声明使用多个单词 方法、接口和包使用单个词。 请记住，包的名称是调用者用来引用名称的一部分，因此要好好利用这一点。  变量的名称应描述其内容，而不是内容的类型。\n典型错误：\nvar usersMap map[string]*User 如果users的描述性都不够用，那么usersMap也不会。\n声明变量但没有初始化时，请使用var。\n在声明和初始化时，使用:=。\n关于变量和常量的注释应描述其内容而非其目的  任何既不明显也不简短的公共功能必须予以注释。 无论长度或复杂程度如何，对库中的任何函数都必须进行注释  在编写函数之前，请编写描述函数的注释。 如果你发现很难写出注释，那么这就表明你将要编写的代码很难理解。\n以包所提供的内容来命名，而不是它包含的内容。\n避免使用类似base，common或util的包名称 尽早return而不是深度嵌套 使用internal包来减少公共API 不鼓励使用nil作为参数 首选可变参数函数而非[]T参数 通过消除错误来消除错误处理 使用github.com/pkg/errors包装errors 永远不要启动一个停止不了的goroutine。 ","permalink":"https://zhenfeng-zhu.github.io/posts/go-best-practice/","summary":" 短变量名称在声明和上次使用之间的距离很短时效果很好。 长变量名称需要证明自己的合理性; 名称越长，需要提供的价值越高。冗长的名称与页面上的重量相比，信号量较小。 请勿在变量名称中包含类型名称。 常量应该描述它们持有的值，而不是该如何使用。 对于循环和分支使用单字母变量，参数和返回值使用单个字，函数和包级别声明使用多个单词 方法、接口和包使用单个词。 请记住，包的名称是调用者用来引用名称的一部分，因此要好好利用这一点。  变量的名称应描述其内容，而不是内容的类型。\n典型错误：\nvar usersMap map[string]*User 如果users的描述性都不够用，那么usersMap也不会。\n声明变量但没有初始化时，请使用var。\n在声明和初始化时，使用:=。\n关于变量和常量的注释应描述其内容而非其目的  任何既不明显也不简短的公共功能必须予以注释。 无论长度或复杂程度如何，对库中的任何函数都必须进行注释  在编写函数之前，请编写描述函数的注释。 如果你发现很难写出注释，那么这就表明你将要编写的代码很难理解。\n以包所提供的内容来命名，而不是它包含的内容。\n避免使用类似base，common或util的包名称 尽早return而不是深度嵌套 使用internal包来减少公共API 不鼓励使用nil作为参数 首选可变参数函数而非[]T参数 通过消除错误来消除错误处理 使用github.com/pkg/errors包装errors 永远不要启动一个停止不了的goroutine。 ","title":"go-best-practice"},{"content":"docker 利用Linux的cgroups和namespace，构建一个沙箱运行环境。\ndocker镜像 其实就是一个压缩包，这个压缩包是由一个完整的操作系统的所有文件目录构成，包含了这个应用运行所需要的所有依赖，所以本地开发环境和测试环境是一样的。\n解决了应用打包的根本性问题。\n容器编排 对 Docker 容器的一系列定义、配置和创建动作的管理\n 容器本身没有价值，有价值的是“容器编排”。\n 原理 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造一个“边界”。\n在创建一个容器进程的时候，指定了这个进程所需要启动的一组Namespace参数，这样容器就只能看到当前Namespace所限定的资源、文件、设备、状态或配置。\nCgroups主要作用是为一个进程组设置资源上限，如CPU、内存、磁盘和带宽等。也可以设置进程优先级，审计，挂起，重启等。\n因此，一个正在运行的Docker容器，其实就是一个启用了多个Namespace的应用进程，而这个进程能够使用的资源是由Cgroups来限制。\n挂载在容器根目录上，用来为容器进程提供隔离后执行环境的文件系统，就是容器镜像，rootfs。\n 启动Namespace配置 设置Cgroups参数 切换进程根目录rootf  docker镜像设计时，引入了层（layer），用户制作镜像的每一步操作都会生成一个层，也就是一个增量的rootfs。AuFS，所以就有了共享层，镜像不用那么大。\n一个进程，可以选择加入到某个进程已有的 Namespace当中，从而达到进入这个进程所在的容器的目的，这正是docker exec的实现原理。\nvolume机制，允许你将宿主机上指定的目录或文件，挂载到容器里面进行读取和修改操作。\n主要依赖Linux依赖三大技术：  Namespace Cgroups rootfs  和虚拟机比较 虚拟机是通过硬件虚拟化功能，模拟一套操作系统所需要的各种硬件，如CPU、内存、IO设备等，然后安装一个新的操作系统。\ndocker是利用Linux的Namespace原理，帮助用户启动的还是系统的应用进程，只是加了一些参数，限制其能看到的资源。因此相对于虚拟机资源消耗更小，而且轻量级，敏捷高性能。\n不过缺点就是隔离不彻底，多个容器进程公用宿主机操作系统内核。有些资源和对象不可以被Namespace化的，如时间。\nkubernetes要解决的问题\n编排？调度？容器云？集群管理？\n master  kube-apiserver：API服务 kube-scheduler：调度 kube-controller-manager：编排   node  kubelet：同容器运行时打交道。依赖于CRI（container runtime interface容器运行接口）远程调用接口，这个接口定义了容器运行时的各项核心操作。    etcd  运行在大规模集群中的各种任务之间，实际存在各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。\nsudo\n 首先，通过一个编排对象，如pod，job或cronjob等，来描述你试图管理的应用； 然后，再为它定义一些服务对象，如service，secret，autoscaler等。这些对象，会负责具体的平台级功能。  这种使用方法，就是所谓的“声明式API”。这种API对应的编排对象和服务对象，都是k8s项目中的API对象。\n简单使用 $ kubectl create -f 我的配置文件 pod就是k8s世界中的应用，而一个应用可以由多个容器组成。\n使用一个API对象管理另一个API对象的方法，叫控制器模式。\n每个API对象都有一个metadata字段，这个字段是API对象的标识，即元数据。主要用到的是labels，spec.selector.matchLabels就是k8s过滤的规则。与labels同层级的是annotations，这是由k8s所感兴趣的，而不是用户。\n一个k8s的API对象都有metadata和spec两个部分。前者放的是对象的元数据，对所有API对象来讲，这部分的字段和格式基本一样；而后者存放的是属于这个对象独有的定义，用来描述它所要表达的功能。\n$ kubectl create -f nginx-deployment.yaml $ kubectl get pods -l app=nginx NAME READY STATUS RESTARTS AGE nginx-deployment-67594d6bf6-9gdvr 1/1 Running 0 10m nginx-deployment-67594d6bf6-v6j7w 1/1 Running 0 10m $ kubectl describe pod nginx-deployment-67594d6bf6-9gdvr Name: nginx-deployment-67594d6bf6-9gdvr Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: node-1/10.168.0.3 Start Time: Thu, 16 Aug 2018 08:48:42 +0000 Labels: app=nginx pod-template-hash=2315082692 Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.32.0.23 Controlled By: ReplicaSet/nginx-deployment-67594d6bf6 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned default/nginx-deployment-67594d6bf6-9gdvr to node-1 Normal Pulling 25s kubelet, node-1 pulling image \u0026quot;nginx:1.7.9\u0026quot; Normal Pulled 17s kubelet, node-1 Successfully pulled image \u0026quot;nginx:1.7.9\u0026quot; Normal Created 17s kubelet, node-1 Created container Normal Started 17s kubelet, node-1 Started container $ kubectl apply -f nginx-deployment.yaml # 修改 nginx-deployment.yaml 的内容 $ kubectl apply -f nginx-deployment.yaml 在命令行中，所有 key-value 格式的参数，都使用“=“而不是”：“表示。\n在k8s执行过程中，对API对象的所有重要操作，都会被记录在这个对象的events中。\n在线业务\nDeployment\nStatefunSet\nDaemonSet\n离线业务\nJob\nrestartPolicy在job对象里只被允许设置为never和onFailure；而在Deployment对象中，只被允许设置为always。\n声明式API和Kubernetes编程范式 创建一个两个Nginx容器的步骤：\n首先写一个Deployment的yaml文件：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 然后使用kubectl create命令在Kubernetes中创建一个Deployment对象：\n$ kubectl create -f nginx.yaml 这样两个Nginx pod就运行起来了。\n如果要更新的话，只需要修改yaml文件，然后使用kubectl apply命令更新，触发了滚动更新。\n这个apply命令就是声明式API。\nistio项目中，最根本的组件是运行在每个pod里的envoy容器。这个代理服务以sidecar容器的方式，把整个pod的进出流量接管下来。istio的控制层的pilot组件，通过调用每个envoy的API，实现微服务的治理。\n利用Kubernetes的Admission Control，也叫：Initializer，先创建一个Pod，然后istio就是在pod的yaml给Kubernetes之后，自动加上envoy的配置。\n 所谓的声明式，指的就是我只需要提交一个定义好的API对象来声明我所期望的状态是什么样子。 其次，声明式API允许有多个API写端，以PATCH的方式对API对象进行修改，而无需关心原始的YAML文件的内容。 最后，Kubernetes基于对API对象的增删改查，在无需外界干预的情况下，完成对实际状态和期望状态的调谐。  一个API对象在etcd中完整路径是由：group（API组），version（API版本）和Resource（API资源类型）三个部分组成的。\napiVersion: batch/v2 kind: CronJob batch是组，v2是版本，CronJob是类型。\n对于核心API对象：Pod，Node等，不需要group的。非核心对象是需要组。\n匹配规则就是：\n/apis/batch/v2/CronJob\n  首先yaml文件被提交给了APIServer\n过滤，授权，超时处理或审计等\n  进入路由流程\n根据yaml，按照匹配规则去找\n  根据定义，按照yaml中的字段，创建一个对象\n  进行Amission和Validation。\n  把验证过的对象，序列化存到etcd中\n  RBAC 基于角色的控制\nrole：角色，一组规则，定义Kubernetes API对象的操作权限\nsubject：被作用者，可以是人，也可以是机器，也可以是Kubernetes定义的用户\nrolebinding：定义被作用者和角色的绑定关系\nServiceAccount，会被自动创建分配一个secret对象。\n所谓角色就是一组权限规则列表，而我们分配这些权限的方式，就是通过创建rolebinding对象，将被作用者和权限列表进行绑定。\n另外，与之对应的ClusterRole和ClusterRoleBinding，则是Kubernetes集群级别的Role和RoleBinding，它们的作用范围不受Namespace限制。\n尽管被作用者有很多种（如User、Group），但在我们平常使用的时候，最普遍的还是ServiceAccount。\n网络模型 Veth Pair 常常被用作连接不同 Network Namespace的网线。veth pair虚拟设备。总是以两张虚拟网卡形式成对出现。并且，从一个网卡中发出的数据包，可以直接出现在另一张网卡上，哪怕这两个网卡在不同的network Namespace里。\n一旦一张虚拟网卡被插在网桥上，他就会变成该网桥的从设备。从设备会降级成为网桥的一个端口，不能处理数据包，只能接收流入的数据包交给对应的网桥。\n两个容器的虚拟网卡都插在宿主机的一个网桥上，这个网桥就扮演一个交换机的角色。当两个容器进行网络交互时，从一个容器的发出请求到宿主机，由于Veth Pair 的机制，另一个容器就看到有数据流入。\n因此默认情况下，被限制在network Namespace的容器进程，实际就是通过veth pair设备+宿主机网桥的方式，实现了跟其他容器的数据交换。\n跨主通信，需要有一个集群公用的网桥，所有容器都连接到该网桥上，就可以相互通信，这就是overlay network（覆盖网络）\n","permalink":"https://zhenfeng-zhu.github.io/posts/kubernetes/","summary":"docker 利用Linux的cgroups和namespace，构建一个沙箱运行环境。\ndocker镜像 其实就是一个压缩包，这个压缩包是由一个完整的操作系统的所有文件目录构成，包含了这个应用运行所需要的所有依赖，所以本地开发环境和测试环境是一样的。\n解决了应用打包的根本性问题。\n容器编排 对 Docker 容器的一系列定义、配置和创建动作的管理\n 容器本身没有价值，有价值的是“容器编排”。\n 原理 容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造一个“边界”。\n在创建一个容器进程的时候，指定了这个进程所需要启动的一组Namespace参数，这样容器就只能看到当前Namespace所限定的资源、文件、设备、状态或配置。\nCgroups主要作用是为一个进程组设置资源上限，如CPU、内存、磁盘和带宽等。也可以设置进程优先级，审计，挂起，重启等。\n因此，一个正在运行的Docker容器，其实就是一个启用了多个Namespace的应用进程，而这个进程能够使用的资源是由Cgroups来限制。\n挂载在容器根目录上，用来为容器进程提供隔离后执行环境的文件系统，就是容器镜像，rootfs。\n 启动Namespace配置 设置Cgroups参数 切换进程根目录rootf  docker镜像设计时，引入了层（layer），用户制作镜像的每一步操作都会生成一个层，也就是一个增量的rootfs。AuFS，所以就有了共享层，镜像不用那么大。\n一个进程，可以选择加入到某个进程已有的 Namespace当中，从而达到进入这个进程所在的容器的目的，这正是docker exec的实现原理。\nvolume机制，允许你将宿主机上指定的目录或文件，挂载到容器里面进行读取和修改操作。\n主要依赖Linux依赖三大技术：  Namespace Cgroups rootfs  和虚拟机比较 虚拟机是通过硬件虚拟化功能，模拟一套操作系统所需要的各种硬件，如CPU、内存、IO设备等，然后安装一个新的操作系统。\ndocker是利用Linux的Namespace原理，帮助用户启动的还是系统的应用进程，只是加了一些参数，限制其能看到的资源。因此相对于虚拟机资源消耗更小，而且轻量级，敏捷高性能。\n不过缺点就是隔离不彻底，多个容器进程公用宿主机操作系统内核。有些资源和对象不可以被Namespace化的，如时间。\nkubernetes要解决的问题\n编排？调度？容器云？集群管理？\n master  kube-apiserver：API服务 kube-scheduler：调度 kube-controller-manager：编排   node  kubelet：同容器运行时打交道。依赖于CRI（container runtime interface容器运行接口）远程调用接口，这个接口定义了容器运行时的各项核心操作。    etcd  运行在大规模集群中的各种任务之间，实际存在各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。\nsudo\n 首先，通过一个编排对象，如pod，job或cronjob等，来描述你试图管理的应用； 然后，再为它定义一些服务对象，如service，secret，autoscaler等。这些对象，会负责具体的平台级功能。  这种使用方法，就是所谓的“声明式API”。这种API对应的编排对象和服务对象，都是k8s项目中的API对象。\n简单使用 $ kubectl create -f 我的配置文件 pod就是k8s世界中的应用，而一个应用可以由多个容器组成。","title":"kubernetes"},{"content":"监视器\n监视器提供了一个外部世界和函数之间的非托管的通用接口。它的工作是收集从API网关来的HTTP请求，然后调用程序。监视器是一个小型的Golang服务——下图展示了它是如何工作的：\n 上图：一个小型的web服务，可以为每个传入的HTTP请求分配所需要的进程。\n 每个函数都需要嵌入这个二进制文件并将其作为ENTRYPOINT 或 CMD，实际上是把它作为容器的初始化进程。一旦你的进程被创建分支，监视器就会通过stdin 传递HTTP请求并从stdout中读取HTTP响应。这意味着你的程序无需知道web和HTTP的任何信息。\n轻松创建新函数 从CLI创建一个函数\n创建函数最简单的方法是使用FaaS CLI和模板。CLI抽象了所有Docker的知识，使得你只需要编写所支持语言的handler文件即可。\n 你的第一个使用OpenFaaS的无服务器Python函数 阅读有关FaaS CLI的教程  深入研究 Package your function打包你的函数\n如果你不想使用CLI或者现有的二进制文件或镜像，可以使用下面的方法去打包函数：\n 使用一个现有的或者一个新的Docker镜像作为基础镜像 FROM 通过curl 或 ADD https://从 Releases 页面 添加fwatchdog二进制文件 为每个你要运行的函数设置 fprocess(函数进程) 环境变量 Expose port 8080 暴露端口8080 Set the CMD to fwatchdog 设置 CMD为fwatchdog  一个echo函数的示例Dockerfile：\nFROM alpine:3.7 ADD https://github.com/openfaas/faas/releases/download/0.8.0/fwatchdog /usr/bin RUN chmod +x /usr/bin/fwatchdog # Define your binary here ENV fprocess=\u0026#34;/bin/cat\u0026#34; CMD [\u0026#34;fwatchdog\u0026#34;] Implementing a Docker healthcheck实现一个Docker健康检查\nDocke的健康检查不是必需的，但是它是最佳实践。这会确保监视器已经在API网关转发请求之前准备好接收请求。如果函数或者监视器遇到一个不可恢复的问题，Swarm也会重启容器。\nHere is an example of the echo function implementing a healthcheck with a 5-second checking interval.\n下面是实现了一个具有5秒间隔的健康检查的echo函数示例：\nFROM functions/alpine ENV fprocess=\u0026#34;cat /etc/hostname\u0026#34; HEALTHCHECK --interval=5s CMD [ -e /tmp/.lock ] || exit 1 监视器进程早启动内部Golang HTTP服务的时候会在 /tmp/下面创建一个.lock文件。[ -e file_name ]shell命令可以检查文件是否存在。在Windows容器中，这是一个不合法的路径，所以你可能需要设置suppress_lock 环境变量。\n有关健康检查，请阅读我的Docker Swarm教程：\n 10分钟内试用Docker的健康检查  环境变量重载:\n监视器可以通过环境变量来配置，你必须始终指定一个fprocess 变量\n高级/调整 (新)——子监视器和HTTP模式  部分的监视器  为每个请求创建一个新的进程分支具有进程隔离，可移植和简单的优点。任何进程都可以在没有任何附加代码的情况下变成一个函数。of-watchdog可和HTTP模式是一种优化，这样就可以在所有请求之间维护一个单一的进程。\n新版本的监视器正在openfaas-incubator/of-watchdog上测试。\n这种重写主要是生成一个可以持续维护的结构。它将会替代现有的监视器，也会有二进制的释放版。\n使用HTTP头 HTTP的头和其他请求信息以下面的格式注入到环境变量中：\nX-Forwarded-By`头变成了`Http_X_Forwarded_By  Http_Method - GET/POST etc Http_Method - GET/POST 等等 Http_Query - QueryString value Http_Query - 查询字符串的值 Http_ContentLength - gives the total content-length of the incoming HTTP request received by the watchdog. Http_ContentLength - 监视器收到的HTTP请求的内容长度。   默认情况下，通过cgi_headers 环境变量启用该行为。\n 以下是带有附加头和查询字符串的POST请求的示例：\n$ cgi_headers=true fprocess=env ./watchdog \u0026amp; 2017/06/23 17:02:58 Writing lock-file to: /tmp/.lock $ curl \u0026#34;localhost:8080?q=serverless\u0026amp;page=1\u0026#34; -X POST -H X-Forwarded-By:http://my.vpn.com 如果你再Linux系统下设置了fprocess 到 env中，会看到如下结果：\nHttp_User_Agent=curl/7.43.0 Http_Accept=*/* Http_X_Forwarded_By=http://my.vpn.com Http_Method=POST Http_Query=q=serverless\u0026amp;page=1 也可以使用GET请求：\n$ curl \u0026#34;localhost:8080?action=quote\u0026amp;qty=1\u0026amp;productId=105\u0026#34; 监视器的输出如下：\nHttp_User_Agent=curl/7.43.0 Http_Accept=*/* Http_Method=GET Http_Query=action=quote\u0026amp;qty=1\u0026amp;productId=105 现在就可以在程序中使用HTTP状态来做决策了。\nHTTP方法 监视器支持的HTTP方法有：\n带有请求体的：\n POST, PUT, DELETE, UPDATE  不带请求体的：\n GET   API网关现在支持函数的POST路由。\n 请求响应的内容类型 默认情况下，监视器会匹配客户端的\u0026quot;Content-Type\u0026quot;。\n 如果客户端发送Content-Type 为 application/json 的json形式的post请求，将会在响应的时候自动匹配。 如果客户端发送Content-Type 为 text/plain 的json形式的post请求，响应也会自动匹配。  若要重载所有响应的Content-Type ，需要设置content_type 环境变量。\nI don\u0026rsquo;t want to use the watchdog 我不想使用监视器 这种案例是OpenFaaS所不支持的，但是如果你的容器符合以下要求，那么OpenFaaS的网关和其他工具也会管理和伸缩服务。\n你需要提供一个锁文件 /tmp/.lock，以便业务流程系统可以在容器中运行健康检查。如果你正在使用swarm，那么请确保在Dockerfile中提供HEALTHCHECK指令——在 faas存储库中有示例。\n 在HTTP之上暴露TCP端口8080 创建/tmp/.lock 文件，或者在响应操作tempdir系统调用的任何位置。  调整自动伸缩 自动伸缩式从1个副本开始，以5个位一个单位进行升级：\n 1-\u0026gt;5 5-\u0026gt;10 10-\u0026gt;15 15-\u0026gt;20  你可以通过标签来覆盖一个函数minimum 和 maximum 。\n如果要在2到15之间的话，请在部署的时候配置以下标签：\ncom.openfaas.scale.min: \u0026#34;2\u0026#34; com.openfaas.scale.max: \u0026#34;15\u0026#34; 这些标签是可选的\n禁用自动伸缩\n如果要禁用某个函数的自动伸缩，将最小和最大的副本数设置为相同的值，即“1”。\n同样也可以删除AlertManager。\n","permalink":"https://zhenfeng-zhu.github.io/posts/watchdog/","summary":"监视器\n监视器提供了一个外部世界和函数之间的非托管的通用接口。它的工作是收集从API网关来的HTTP请求，然后调用程序。监视器是一个小型的Golang服务——下图展示了它是如何工作的：\n 上图：一个小型的web服务，可以为每个传入的HTTP请求分配所需要的进程。\n 每个函数都需要嵌入这个二进制文件并将其作为ENTRYPOINT 或 CMD，实际上是把它作为容器的初始化进程。一旦你的进程被创建分支，监视器就会通过stdin 传递HTTP请求并从stdout中读取HTTP响应。这意味着你的程序无需知道web和HTTP的任何信息。\n轻松创建新函数 从CLI创建一个函数\n创建函数最简单的方法是使用FaaS CLI和模板。CLI抽象了所有Docker的知识，使得你只需要编写所支持语言的handler文件即可。\n 你的第一个使用OpenFaaS的无服务器Python函数 阅读有关FaaS CLI的教程  深入研究 Package your function打包你的函数\n如果你不想使用CLI或者现有的二进制文件或镜像，可以使用下面的方法去打包函数：\n 使用一个现有的或者一个新的Docker镜像作为基础镜像 FROM 通过curl 或 ADD https://从 Releases 页面 添加fwatchdog二进制文件 为每个你要运行的函数设置 fprocess(函数进程) 环境变量 Expose port 8080 暴露端口8080 Set the CMD to fwatchdog 设置 CMD为fwatchdog  一个echo函数的示例Dockerfile：\nFROM alpine:3.7 ADD https://github.com/openfaas/faas/releases/download/0.8.0/fwatchdog /usr/bin RUN chmod +x /usr/bin/fwatchdog # Define your binary here ENV fprocess=\u0026#34;/bin/cat\u0026#34; CMD [\u0026#34;fwatchdog\u0026#34;] Implementing a Docker healthcheck实现一个Docker健康检查","title":"watchdog"},{"content":"queue-worker源码分析 异步函数和同步函数 在OpenFaaS中同步调用函数时，将会连接到网关，直到函数成功返回才会关闭连接。同步调用是阻塞的。\n 网关的路由是：/function/\u0026lt;function_name\u0026gt; 必须等待 在结束的时候得到结果 明确知道是成功还是失败  异步函数会有一些差异：\n 网关的路由是：/async-function/\u0026lt;function_name\u0026gt; 客户端获得202的即时响应码 从queue-worker中调用函数 默认情况下，结果是被丢弃的。  查看queue-worker的日志 docker service logs -f func_queue-worker 利用requestbin和X-Callback-Url获取异步函数的结果 如果需要获得异步函数的结果，有两个方法：\n 更改代码，将结果返回给端点或者消息系统 利用内置的回调 内置的回调将会允许函数提供一个url，queue-worker会报告函数的成功或失败。 requestbin会创建一个新的bin，这是互联网的一个url地址，可以从这里获取函数的结果。  源码分析 依赖项 github.com/nats-io/go-nats-streaming github.com/nats-io/go-nats github.com/openfaas/faas go-nats和go-nats-streaming是nats和nats-streaming的go版本的客户端。\nfaas这个依赖其实是只用到了queue包下面的types.go文件。这个文件是定义了异步请求的Request结构体和一个CanQueueRequests接口。如下所示：\npackage queue import \u0026quot;net/url\u0026quot; import \u0026quot;net/http\u0026quot; // Request for asynchronous processing type Request struct { Header http.Header Body []byte Method string QueryString string Function string CallbackURL *url.URL `json:\u0026quot;CallbackUrl\u0026quot;` } // CanQueueRequests can take on asynchronous requests type CanQueueRequests interface { Queue(req *Request) error } 从这里我们就可以明白作者的设计思路，只要是实现了这个CanQueueRequests接口，就可以作为一个queue-worker。\n接口实现类NatsQueue 接口的实现类NatsQueue是在handler包里。它的属性都是nats中常用到的，包括clientId，clusterId，url，连接，主题等，如下所示：\n// NatsQueue queue for work type NatsQueue struct { nc stan.Conn // nats的连接 ClientID string // nats的clientId ClusterID string // nats的clusterId NATSURL string // nats的URL Topic string // 主题 } 它的queue方法也很简单，主要做了两件事儿：\n 解析传入的Request对象，并转为json对象out 将out发布到队列里  // Queue request for processing func (q *NatsQueue) Queue(req *queue.Request) error { var err error fmt.Printf(\u0026quot;NatsQueue - submitting request: %s.\\n\u0026quot;, req.Function) out, err := json.Marshal(req) if err != nil { log.Println(err) } err = q.nc.Publish(q.Topic, out) return err } go语言没有构造方法，所以NatsQueue还用于创建NatsQueue的实例的方法，这里就成为工厂方法。这个工厂方法主要就是从配置文件中读取环境变量的值，然后创建一个nats的连接，相当于给NatsQueue的对象的每个属性进行赋值。\nfunc CreateNatsQueue(address string, port int, clientConfig NatsConfig) (*NatsQueue, error) { queue1 := NatsQueue{} var err error natsURL := fmt.Sprintf(\u0026quot;nats://%s:%d\u0026quot;, address, port) log.Printf(\u0026quot;Opening connection to %s\\n\u0026quot;, natsURL) clientID := clientConfig.GetClientID() clusterID := \u0026quot;faas-cluster\u0026quot; nc, err := stan.Connect(clusterID, clientID, stan.NatsURL(natsURL)) queue1.nc = nc return \u0026amp;queue1, err } 这个CreateNatsQueue方法是Gateway项目中进行调用，我们可以在Gateway项目的main.go中找到，如果Gateway的配置开启了异步函数支持，就会调用该方法，创建一个NatsQueue对象，然后把函数放到队列中，这里就不深入讲解：\nif config.UseNATS() { log.Println(\u0026quot;Async enabled: Using NATS Streaming.\u0026quot;) natsQueue, queueErr := natsHandler.CreateNatsQueue(*config.NATSAddress, *config.NATSPort, natsHandler.DefaultNatsConfig{}) if queueErr != nil { log.Fatalln(queueErr) } faasHandlers.QueuedProxy = handlers.MakeQueuedProxy(metricsOptions, true, natsQueue) faasHandlers.AsyncReport = handlers.MakeAsyncReport(metricsOptions) } 到这里，我相信读者也了解到，Gateway其实就是一个发布者，将异步请求扔到队列里。接下来肯定要有一个订阅者将请求消费处理。\n订阅者处理 我们都知道，nats streaming的订阅者订阅到消息之后，会把消息扔给一个回调函数去处理。queue-worker的订阅者实现也是这样，它的实现并不复杂，所有逻辑都在main.go的中。\n我们先看回调函数mcb都做了什么：\n 首先当然是将消息体反序列化成上面说到的用于异步处理的Request对象。 构造http请求的url和querystring，url的格式如下： functionURL := fmt.Sprintf(\u0026ldquo;http://%s%s:8080/%s\u0026rdquo;, req.Function, config.FunctionSuffix, queryString) 设置http的header，并以post的形式向functionURL发起请求。 如果请求失败，设置返回状态码为http.StatusServiceUnavailable，并分别处理CallbackURL是否存在的情况。 如果请求成功，同样也是要分别处理CallbackURL是否存在的情况。  当然在这个callback中会根据一些环境变量的存在，选择是否打印日志出来。\nmcb := func(msg *stan.Msg) { i++ printMsg(msg, i) started := time.Now() req := queue.Request{} unmarshalErr := json.Unmarshal(msg.Data, \u0026amp;req) if unmarshalErr != nil { log.Printf(\u0026quot;Unmarshal error: %s with data %s\u0026quot;, unmarshalErr, msg.Data) return } fmt.Printf(\u0026quot;Request for %s.\\n\u0026quot;, req.Function) if config.DebugPrintBody { fmt.Println(string(req.Body)) } queryString := \u0026quot;\u0026quot; if len(req.QueryString) \u0026gt; 0 { queryString = fmt.Sprintf(\u0026quot;?%s\u0026quot;, strings.TrimLeft(req.QueryString, \u0026quot;?\u0026quot;)) } functionURL := fmt.Sprintf(\u0026quot;http://%s%s:8080/%s\u0026quot;, req.Function, config.FunctionSuffix, queryString) request, err := http.NewRequest(http.MethodPost, functionURL, bytes.NewReader(req.Body)) defer request.Body.Close() copyHeaders(request.Header, \u0026amp;req.Header) res, err := client.Do(request) var status int var functionResult []byte if err != nil { status = http.StatusServiceUnavailable log.Println(err) timeTaken := time.Since(started).Seconds() if req.CallbackURL != nil { log.Printf(\u0026quot;Callback to: %s\\n\u0026quot;, req.CallbackURL.String()) resultStatusCode, resultErr := postResult(\u0026amp;client, res, functionResult, req.CallbackURL.String()) if resultErr != nil { log.Println(resultErr) } else { log.Printf(\u0026quot;Posted result: %d\u0026quot;, resultStatusCode) } } statusCode, reportErr := postReport(\u0026amp;client, req.Function, status, timeTaken, config.GatewayAddress) if reportErr != nil { log.Println(reportErr) } else { log.Printf(\u0026quot;Posting report - %d\\n\u0026quot;, statusCode) } return } if res.Body != nil { defer res.Body.Close() resData, err := ioutil.ReadAll(res.Body) functionResult = resData if err != nil { log.Println(err) } if config.WriteDebug { fmt.Println(string(functionResult)) } else { fmt.Printf(\u0026quot;Wrote %d Bytes\\n\u0026quot;, len(string(functionResult))) } } timeTaken := time.Since(started).Seconds() fmt.Println(res.Status) if req.CallbackURL != nil { log.Printf(\u0026quot;Callback to: %s\\n\u0026quot;, req.CallbackURL.String()) resultStatusCode, resultErr := postResult(\u0026amp;client, res, functionResult, req.CallbackURL.String()) if resultErr != nil { log.Println(resultErr) } else { log.Printf(\u0026quot;Posted result: %d\u0026quot;, resultStatusCode) } } statusCode, reportErr := postReport(\u0026amp;client, req.Function, res.StatusCode, timeTaken, config.GatewayAddress) if reportErr != nil { log.Println(reportErr) } else { log.Printf(\u0026quot;Posting report - %d\\n\u0026quot;, statusCode) } } postResult函数是用来处理callbackURL存在的情况，在这个函数中将结果，以post请求调用callbackURL发送出去。\npostReport函数用来处理callbackURL不存在的情况，这里是将结果发到Gateway网关的\u0026quot;http://\u0026quot; + gatewayAddress + \u0026quot;:8088/system/async-report\u0026quot;中，我们之后就可以从这个url里查询异步函数的执行结果了。\n总结 本文主要分析了NATS Streaming版本的queue worker的实现，通过分析源码我们可以看到OpenFaaS在架构的设计很有考究，充分的考虑到了可扩展性，通过定义接口规范，使得开发者很容易实现自定义。\n","permalink":"https://zhenfeng-zhu.github.io/posts/queue-worker/","summary":"queue-worker源码分析 异步函数和同步函数 在OpenFaaS中同步调用函数时，将会连接到网关，直到函数成功返回才会关闭连接。同步调用是阻塞的。\n 网关的路由是：/function/\u0026lt;function_name\u0026gt; 必须等待 在结束的时候得到结果 明确知道是成功还是失败  异步函数会有一些差异：\n 网关的路由是：/async-function/\u0026lt;function_name\u0026gt; 客户端获得202的即时响应码 从queue-worker中调用函数 默认情况下，结果是被丢弃的。  查看queue-worker的日志 docker service logs -f func_queue-worker 利用requestbin和X-Callback-Url获取异步函数的结果 如果需要获得异步函数的结果，有两个方法：\n 更改代码，将结果返回给端点或者消息系统 利用内置的回调 内置的回调将会允许函数提供一个url，queue-worker会报告函数的成功或失败。 requestbin会创建一个新的bin，这是互联网的一个url地址，可以从这里获取函数的结果。  源码分析 依赖项 github.com/nats-io/go-nats-streaming github.com/nats-io/go-nats github.com/openfaas/faas go-nats和go-nats-streaming是nats和nats-streaming的go版本的客户端。\nfaas这个依赖其实是只用到了queue包下面的types.go文件。这个文件是定义了异步请求的Request结构体和一个CanQueueRequests接口。如下所示：\npackage queue import \u0026quot;net/url\u0026quot; import \u0026quot;net/http\u0026quot; // Request for asynchronous processing type Request struct { Header http.Header Body []byte Method string QueryString string Function string CallbackURL *url.URL `json:\u0026quot;CallbackUrl\u0026quot;` } // CanQueueRequests can take on asynchronous requests type CanQueueRequests interface { Queue(req *Request) error } 从这里我们就可以明白作者的设计思路，只要是实现了这个CanQueueRequests接口，就可以作为一个queue-worker。","title":"queue-worker"},{"content":" 本文是阅读http://www.netkiller.cn/blockchain/ch01s10.html上的一些笔记。\n 理解区块链的分布式记账 http://www.netkiller.cn/blockchain/ch01s10.html\n区块链中提到的账本，记账等词汇是和会计无关的词汇。\n我们传统理解的账本是一个二维的表格，记录了某年某月某日的费用：\n   时间 用途 金额     2018-08-23 借 100   2018-08-22 还 200   2018-08-21 借 50   2018-08-20 还 1000    如果账目比较多，可以拆账，将不同分类的账目放在特定的账本中，而且二维表格还可以设置索引等，快速找到一笔交易。\n但是区块链的记账形式是：\n可以发现，区块链的这种记账方式是做了行列矩阵转换，节点之间收尾相互连接，成为链式结构，所有的账目都在一条链上。\n所谓分布式记账，其实就是上述链状的数据结构保存在所有的节点上，形成分布式集群。\n之所以采用区块链来做分布式记账，主要是区块链有如下好处：\n  去中心化\n传统的数据库存储是中心化的，通过暴露ip地址和端口号提供服务，后来分布式进群化之后，出现了主主从架构等。\n与数据库相比，区块链是多主架构，而且实现更为复杂，节点之间的数据之间不是简单的二进制日志同步，而是要通过加密技术，节点达成共识之后才存储。\n  可追溯\n  安全\n安全分为很多层，区块链只能做到存储层的安全。\n区块链无法解决用户层，应用层，逻辑层等安全问题，他只能保证存储在硬盘上的区块不被修改。\n  不可篡改\n很多人认为区块链数据一旦创建之后就不能修改，所以采用区块链技术很安全。其实不然，数据是可以修改的，但是不能篡改。\n撰改是指非法修改区块链数据，而修改则是合法变更数据。\n通常撰改区块链数据多指数据存储层面的修改。而修改则是通过合约提供的修改函数变更区块链里面的数据。\n多数区块链平台没有用户认证权限管理模块。所以无法控制区块中的哪些数据能被修改，哪些不能修改，哪些用户可以修改等等。即使有些区块链平台具备权限控制，颗粒度也无法达到目前的数据库控制的那么细。\n  采用区块链作为账本的时候，会面临如下几个问题：\n  不能建立索引，无法快速搜索出区块中的数据，必须依赖区块链以外的技术，如搜索引擎，数据库等。例如；etherscan.io就是把以太坊上的区块重新入库，借助数据库实现数据检索。\n  区块链只能顺序检索，运算成本高。例如在中心化账本中汇总求和操作，区块链必须从头向后遍历。\n  所有账目均在一条链上，不同的分类混在一起，彼此相连。\n  无法归档。\n传统的数据库，我们可以归档一段时间内的数据，而这些归档的数据基本都是冷数据，不会再被查询，归档数据的备份到存储介质上的解决方案也有很多。\n但是区块链的数据都是热数据，任何新增的节点都必须从0开始同步，并且保证同步到最新区块，否则可能无法完成交易，数据会一直膨胀下去。\n虽然有算法能够减少同步的量，但是现阶段的体验仍然不好。\n  没有事务处理。\n因为区块链是首尾相连的链式结构，所以只能在尾部加区块，无法修改中间的区块。假设有个区块回滚，该区块的hash产生变化，后面的区块都要作废。\n所以当并发执行的时候，可能会出现混乱。因此我们要在应用层做一些处理。\n  性能问题\n  由于是异步执行，无法预测何时完成。\n  交易容易阻塞\n  gas费用。\n  区块链落地的一些问题和解决方案 如果要在企业中落地区块链，尝尝有如下解决方案：\n  解决性能问题\n目前区块链只适合做低频高价值业务。\n读取性能通常是没有问题的，但是写入实际上无论你用多少个服务器节点都不能提升，因为写入区块需要做共识算法，这步操作，会在所有节点上进行，同时还需要加密运算，这些操作都是 CPU 密集型操作。\n方案：\n  通过消息队列技术异步写入，将需要写入的区块放入队列，异步完成上链操作。\n  并行写入，我们可以建设多个区块链平台。多个平台同时服务于业务。\n为了达到去中心化并行写入，我们将在客户端通过算法，匹配服务器。因为如果在平台前面增加负载均衡，加因为这样又回到了中心化系统。\n    溯源颗粒度\n对于所要被溯源的物品或者交易来讲，有四种情况，低频低价值，低频高价值，高频高价值，高频低价值 。\n对于低频高价值和高频高价值的业务，尽量做到最小颗粒度。\n对于低频低价值和高频低价值的业务，可以颗粒度更粗。\n  和传统数据库互补\n区块链技术本身是一种追求分布一致性的数据库。\n我们都知道CAP理论。CAP理论是指的是在一个分布式系统中， Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性），三者不可得兼。大多数区块链，放弃了一些可用性，偏向了一致性和分区容错。\n区块链并非能解决所有问题，虽然他也算是一种数据库，它能解决问题十分有限，它的数据管理和查询能力还打不到 NoSQL 的水平，更别提 SQL 的复杂应用。所以在实际的应用中，区块链不能替代传统数据库，只能互补。\n  链上链下数据一致性问题\n既然区块链替代不了传统数据库，那么必然要在项目中同时使用两种技术。这样问题来了，会有两份数据，一份存储在链下，即传统数据库，另外一部分数据上链，这样就有两份重复的数据，那么怎样保证他们的一致性呢？\n区块链和比特币网络不同，比特币是在链上产生的，它与区块链密布可分，是一体的，所以它的数据安全性是自闭环的。而我们的链下数据并不是在区块链中产生的，因此我们在上链的时候，尽量采用如下的方案：\n 两端都做一次哈希，可快速对比数据是否一致 以链上数据为准，因为数据库的数据更容易被篡改 前端业务走链，后端业务走数据库，因为前台业务是为用户提供服务，所以要走链上数据，而后台是可以管理的，走数据库即可。 共享数据上链，私有数据不上链。    区块链的相关技术 区块链的技术模型自下而上分为；数据层，网路层，共识层，激励层，合约层以及应用层。\n  数据层，封装了底层数据区块的链式结构，以及相关的非对称公私钥数据加密技术和时间戳等技术，这是整个区块链技术中最底层的数据结构。\n这些技术是构建全球金融系统的基础，数十年的使用证明了它非常安全的可靠性。而区块链，正式巧妙地把这些技术结合在了一起。\n  网络层，包括P2P组网机制、数据传播机制和数据验证机制等。\nP2P组网技术早期应用在BT这类P2P下载软件中，这就意味着区块链具有自动组网功能。\n  共识层，封装了网络节点的各类共识机制算法。共识机制算法是区块链的核心技术，因为这决定了到底是谁来进行记账，而记账决定方式将会影响整个系统的安全性和可靠性。\n数据层、网络层、共识层是构建区块链技术的必要元素，缺少任何一层都将不能称之为真正意义上的区块链技术。\n目前已经出现了十余种共识机制算法，其中比较最为知名的有工作量证明机制（PoW，Proof of Work）、权益证明机制（PoS，Proof ofStake）、股份授权证明机制（DPoS，Delegated ProofofStake）等。\n  激励层，将经济因素集成到区块链技术体系中来，包括经济激励的发行机制和分配机制等，主要出现在公有链当中。\n在公有链中必须激励遵守规则参与记账的节点，并且惩罚不遵守规则的节点，才能让整个系统朝着良性循环的方向发展。而在私有链当中，则不一定需要进行激励，因为参与记账的节点往往是在链外完成了博弈，通过强制力或自愿来要求参与记账。\n  合约层，封装各类脚本、算法和智能合约，是区块链可编程特性的基础。\n比特币本身就具有简单脚本的编写功能，而以太坊极大的强化了编程语言协议，理论上可以编写实现任何功能的应用。如果把比特币看成是全球账本的话，以太坊可以看作是一台“全球计算机”，任何人都可以上传和执行任意的应用程序，并且程序的有效执行能得到保证。\n  应用层，封装了区块链的各种应用场景和案例，比如搭建在以太坊上的各类区块链应用即部署在应用层，而未来的可编程金融和可编程社会也将会是搭建在应用层。\n  共识算法\n pow pos dpos poa pbft raft  HD Wallet\nBIP32 定义 Hierarchical Deterministic wallet (简称 \u0026ldquo;HD Wallet\u0026rdquo;)，是一个系统可以从单个seed产生树状结构储存多组 keypairs（私钥和公钥）。\nBIP39 定义钱包助记词和seed生成规则，一般由 12 -24个单字组成，称为 mnemonic。\nBIP44 基于 BIP32 的系统，赋予树状结构中的各层特殊的意义。让同一个 seed 可以支援多币种、多帐户等 。\n使用助记词生成确定性钱包。\nHD Wallet 采用 2048 个单词，或者汉字作为助记词，这些词库对外公开，很多钱包仅仅使用path第一个地址并且没有加密。如果你知道某个用户的助记词中的11各词的排列顺序，那么我们就可以通过穷举方法，算出所有地址的私钥，如果碰巧找到了已经在使用的地址。就可以将里面的ETH全部转走。\n以太坊常用操作 计算gas费用\nvar estimateGas = eth.estimateGas({from:eth.accounts[1], to: eth.accounts[2], value: web3.toWei(1)}) var cost = estimateGas * gasPrice 解锁账户\npersonal.unlockAccount(eth.accounts[3], \u0026quot;12345678\u0026quot;) 转账\nvar txnHash = eth.sendTransaction({from: eth.accounts[3], to: eth.accounts[5], value: eth.getBalance(eth.accounts[3]) - cost, gas: estimateGas}) 查看交易细节\nweb3.eth.getTransaction(txnHash) 获取余额\neth.getBalance(eth.accounts[3]) keystore文件\n以太坊的每个外部账户都是由一对密钥（一个公钥和一个私钥）定义的。账户以地址为索引，地址由公钥衍生而来，取公钥的最后 20个字节。\n每对私钥 /地址都编码在一个钥匙文件里，也就是我们说的keystore文件。该文件是 JSON 文本文件，可以用任何文本编辑器打开和浏览。钥匙文件的关键部分——账户私钥，通常用你创建帐户时设置的密码进行加密。如果你丢失了这个文件，你就丢失了私钥，意味着你失去了签署交易的能力，意味着你的资金被永久的锁定在了你的账户里。\n批量转账\n有时我们需要将Token发送到多个地址上去。通常的做法就是使用web.js写一个循环程序，但是实际使用过程中发现这种做法存在很多问题，常会发生转账失败情况。\n经过分析造成发送失败原因是，频率太高，因为web.js 是异步操作，当前程序还未完成交易，交易尚未确认的情况下第二笔交易发送出去，这时就会出错。\nweb3.eth.getTransactionCount(from).then(function(nonce){ // 问题就出在 nonce ， nonce 如同数据库中的序列主键，如果上一个交易没有完成，下一个交易取得 nonce + 1 后与上一个 pending 的交易相同，产生冲突 }\t最好的解方案是在合约中实现批量转账功能，这样需要一个交易 txhash 完成多比交易。\n如果你的Token已经在使用了，且没有批量转账的功能怎么解决呢？我们可以在写一个Token 继承原来的Token，在新的 Token 中实现批量转账功能。\n代币兑换\n兑换代币通常是指使用 ETH 或者其他币兑换Token。通常是这样实现的，使用智能合约，将 ETH 达到指定合约地址，合约会打回代币给用户。\n这种方式不用人工参与，也不用开发程序。缺点不能做到实时汇率，需要人工设置汇率。\nERC20 Token\nERC20 “描述了实现代币合约的标准功能”，ERC20 是各个代币的标准接口。ERC20 代币仅仅是以太坊代币的子集。为了充分兼容 ERC20，开发者需要将一组特定的函数集成到他们的智能合约中，以便在高层面能够执行以下操作：\n 获得代币总供应量 获得账户余额 转让代币 批准花费代币  最简单的合约如下：\npragma solidity ^0.4.24; contract EncryptToken { uint256 INITIAL_SUPPLY = 666666; mapping(address =\u0026gt; uint256) balances; constructor() public { balances[msg.sender] = INITIAL_SUPPLY; } function transfer(address to, uint256 amount) public{ assert(balances[msg.sender] \u0026gt; amount); balances[msg.sender] -= amount; balances[to] += amount; } function balanceOf(address owner) constant public returns (uint256){ return balances[owner]; } } 使用了openzeppeline-solidity的安全标准代币合约是：\n pragma solidity ^0.4.24; import \u0026quot;openzeppelin-solidity/contracts/token/ERC20/StandardToken.sol\u0026quot;; contract BloggerCoin is StandardToken{ string public name = \u0026quot;BloggerCoin\u0026quot;; // 名字 string public symbol = \u0026quot;BLC\u0026quot;; // 简称 uint8 public decimals = 4; // 10的4次方 uint256 public INITIAL_SUPPLY = 666666; constructor() public{ totalSupply_ = INITIAL_SUPPLY; balances[msg.sender] = INITIAL_SUPPLY; } } ","permalink":"https://zhenfeng-zhu.github.io/posts/blockchain/","summary":"本文是阅读http://www.netkiller.cn/blockchain/ch01s10.html上的一些笔记。\n 理解区块链的分布式记账 http://www.netkiller.cn/blockchain/ch01s10.html\n区块链中提到的账本，记账等词汇是和会计无关的词汇。\n我们传统理解的账本是一个二维的表格，记录了某年某月某日的费用：\n   时间 用途 金额     2018-08-23 借 100   2018-08-22 还 200   2018-08-21 借 50   2018-08-20 还 1000    如果账目比较多，可以拆账，将不同分类的账目放在特定的账本中，而且二维表格还可以设置索引等，快速找到一笔交易。\n但是区块链的记账形式是：\n可以发现，区块链的这种记账方式是做了行列矩阵转换，节点之间收尾相互连接，成为链式结构，所有的账目都在一条链上。\n所谓分布式记账，其实就是上述链状的数据结构保存在所有的节点上，形成分布式集群。\n之所以采用区块链来做分布式记账，主要是区块链有如下好处：\n  去中心化\n传统的数据库存储是中心化的，通过暴露ip地址和端口号提供服务，后来分布式进群化之后，出现了主主从架构等。\n与数据库相比，区块链是多主架构，而且实现更为复杂，节点之间的数据之间不是简单的二进制日志同步，而是要通过加密技术，节点达成共识之后才存储。\n  可追溯\n  安全\n安全分为很多层，区块链只能做到存储层的安全。\n区块链无法解决用户层，应用层，逻辑层等安全问题，他只能保证存储在硬盘上的区块不被修改。\n  不可篡改\n很多人认为区块链数据一旦创建之后就不能修改，所以采用区块链技术很安全。其实不然，数据是可以修改的，但是不能篡改。\n撰改是指非法修改区块链数据，而修改则是合法变更数据。\n通常撰改区块链数据多指数据存储层面的修改。而修改则是通过合约提供的修改函数变更区块链里面的数据。\n多数区块链平台没有用户认证权限管理模块。所以无法控制区块中的哪些数据能被修改，哪些不能修改，哪些用户可以修改等等。即使有些区块链平台具备权限控制，颗粒度也无法达到目前的数据库控制的那么细。\n  采用区块链作为账本的时候，会面临如下几个问题：\n  不能建立索引，无法快速搜索出区块中的数据，必须依赖区块链以外的技术，如搜索引擎，数据库等。例如；etherscan.io就是把以太坊上的区块重新入库，借助数据库实现数据检索。\n  区块链只能顺序检索，运算成本高。例如在中心化账本中汇总求和操作，区块链必须从头向后遍历。","title":"区块链学习笔记"},{"content":"在这篇文章不考虑人工智能，谈谈我对聊天机器人框架实现机制的理解。\n聊天机器人  聊天机器人（Chatterbot）是经由对话或文字进行交谈的计算机程序[1]。能够模拟人类对话，通过图灵测试。\n 我们可以看到现有的IM工具上已经有了很多机器人，其实聊天机器人不只是单纯的和用户进行聊天，他其实还可以做很多事情，例如根据用户输入的一些话，可以帮用户订餐。另外在运维领域，也出现了chatops，通过和机器人聊天，进行运维操作。\n机器人开发框架 作为聊天机器人开发者，面对如此多的IM工具和SDK，常会感到无所适从。Bot 开发框架就是对聊天机器人开发过程中的人工内容做抽象化处理。简单地解释，机器人开发框架就是用来制造机器人并定义其行为。\n然而尽管很多机器人框架宣称「代码一旦写好可部署到任何地方」，但是还会是出现为每一个IM工具开发一个单独的聊天机器人。而一个良好的机器人框架主要包含开发SDK，连接器和模拟器等。\n使用机器人框架其实并不适合初学者学习聊天机器人开发。它们尝试自动化太多工作，对初学者掩盖了基础机制。\n实现方式  webhook事件回调 FSM状态机 workflow工作流  最简单的机器人是没有上下文的语义理解的一问一答，仅仅是对用户的对话进行响应，这种就可以采用webhook的方式进行开发。不需要采用什么开发框架。\n那么对于多轮对话的时候，就需要进行一定的对话管理。由此引入了FSM状态机。\n可能有人不是很懂有限状态机，这里做一下简单说明。\n 有限状态机在现实生活中其实随处可见，伸缩式圆珠笔其实就是一个有限状态机（两种状态互相转换）。\n有限状态机，缩写为FSM，又称为有限状态自动机，简称状态机。是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。\n可以总结为：f(state, action) =\u0026gt; state’\n也就是说，这个函数采用当前的状态和一次行动（即更改状态的方法），之后将该行动应用于这种状态并返回新的状态。\n可以认为状态机是图灵完备的。\n 我们可以将对话看做是在有限状态内跳转的过程，每个状态都有对应的动作和回复，如果能从开始节点顺利的流转到终止节点，任务就完成了。\n我们可以将对话的过程，分为一个个的状态，然后使用DSL来实现一个FSM，对于开发者来讲，我们只需要关注一个个状态函数即可。\n特点是：\n 人为定义对话流程 完全有系统主导，系统问用户答 答非所问的情况直接忽略 建模简单，能清晰明了的把交互匹配到模型 难以扩展，很容易变的复杂 适用于简单的任务，难以处理复杂问题 缺少灵活性，表达能力有限，输入有限，对话结构和流转路径有限  示例：\nconst {startWith, when, goto, stay, stop} = botkit.DSL(fsm); startWith(MyStates.IDLE, {counter: 0}); when(MyStates.IDLE)(async (sender, content, data) =\u0026gt; { }); when(MyStates.UI)((sender, content, data) =\u0026gt; { }); when(MyStates.STEP1)((sender, content, data) =\u0026gt; { }); when(MyStates.STEP2)((sender, content, data) =\u0026gt; { }); when(MyStates.DONE)((sender, content, data) =\u0026gt; { }); when(MyStates.EMPTY)((sender, content, data) =\u0026gt; { }); when(MyStates.LOOP)((sender, content, data) =\u0026gt; { }); 从示例中可以发现，基于fsm的机器人框架需要使用类似DSL领域特定语言一样的描述语言，定义各种各样的状态，每一个状态都有触发点。当满足某个状态条件时，进入该状态，执行该状态的逻辑。这种基于状态机的机器人框架，对于简单的场景比较容易写，但是如果是遇到了复杂的场景，比如多轮对话中还附带上下文信息，就会写起来非常复杂。\n于是引入了基于工作流的chatbot框架。其实工作流是对fsm的一种简化封装，本质上来讲，工作流能做到的，fsm状态机也能做到，而且fsm状态机或许能拆的更细，但是工作流的一个个function，或者是function的集合dialog，可以互相组合，开发起来更符合大部分人的直觉。\n  routing dialog\n// hotels.js module.exports = [ // Destination function (session) { session.send('Welcome to the Hotels finder!'); builder.Prompts.text(session, 'Please enter your destination'); }, function (session, results, next) { session.dialogData.destination = results.response; session.send('Looking for hotels in %s', results.response); next(); }, ... ]; // app.js var bot = new builder.UniversalBot(connector, [ function (session) { // ... }, // ... ]); bot.dialog('hotels', require('./hotels')); bot.dialog('flights', require('./flights')); 通过routing dialog，我们可以实现dialog的复用。\n  waterfall dialog\n一个瀑布流的dialog，可以让我们在一个dialog中像流一样完成一系列的动作。就像fsm的多种状态的集合。\n[ // Destination function (session) { session.send('Welcome to the Hotels finder!'); builder.Prompts.text(session, 'Please enter your destination'); }, function (session, results, next) { session.dialogData.destination = results.response; session.send('Looking for hotels in %s', results.response); next(); }, ... function (session) { var destination = session.dialogData.destination; var checkIn = new Date(session.dialogData.checkIn); var checkOut = checkIn.addDays(session.dialogData.nights); session.send( 'Ok. Searching for Hotels in %s from %d/%d to %d/%d...', destination, checkIn.getMonth() + 1, checkIn.getDate(), checkOut.getMonth() + 1, checkOut.getDate()); // Async search Store .searchHotels(destination, checkIn, checkOut) .then(function (hotels) { // Results session.send('I found in total %d hotels for your dates:', hotels.length); var message = new builder.Message() .attachmentLayout(builder.AttachmentLayout.carousel) .attachments(hotels.map(hotelAsAttachment)); session.send(message); // End session.endDialog(); }); } ]   state\n在一个dialog上下文中共享的数据，或者在多个dialog中共享的数据。对于微软的botbuilder来讲，他们提供了如下几个API：\n   Field Use Cases     userData Stores information globally for the user across all conversations.   conversationData Stores information globally for a single conversation. This data is visible to everyone within the conversation so care should be used to what’s stored there. It’s disabled by default and needs to be enabled using the bots persistConversationData setting.   privateConversationData Stores information globally for a single conversation but its private data for the current user. This data spans all dialogs so it’s useful for storing temporary state that you want cleaned up when the conversation ends.   dialogData Persists information for a single dialog instance. This is essential for storing temporary information in between the steps of a waterfall.      Conversation UI 对话式 UI（Conversation UI，下文简称 CUI）。\nCUI 到底是什么？很好理解，我们日常跟人聊天的微信、短信界面就是。由一条条消息组成，按时间先后展示出来，就可以看作 CUI。\nchatbot在与用户交流时，不单单是只有文字，还会需要用户进行互动，这时候就是CUI的用武之地了。我们可以和移动端进行约定，对一些特定的消息格式进行渲染，这样就可以做出按钮，列表等。\nBot Service 作为一个机器人框架，开发完成之后，还需要和telegram，Facebook messenger，slack等IM平台进行对接，如果要开发者一个个对接的话，将会特别麻烦。作为chatbot开发框架的一部分，bot service的工作就是对接IM平台。\nBot Builder源码阅读 微软的botbuilder-js出到了V4版本，在新版本的机器人框架有着很大的变动，相比于V3目录结构变化了，而且机器人编写流程也有了一定的差异。\n项目结构\n├── botbuilder ├── botbuilder-ai ├── botbuilder-azure ├── botbuilder-core ├── botbuilder-dialogs ├── botframework-config ├── botframework-connector ├── botframework-schema 目录结构更加的组件化。\n如果我们不使用微软的服务，那么botbuilder-ai和botbuilder-azure其实不重要。\nbotbuilder botbuilder是框架的入口，在这个package中做的事情比较简单：\nexport * from \u0026#39;./botFrameworkAdapter\u0026#39;; export * from \u0026#39;./fileTranscriptStore\u0026#39;; export * from \u0026#39;../../botbuilder-core/lib\u0026#39;; 导出botbuilder-core和继承了botAdapter的子类botFrameworkAdapter。\nfileTranscriptStore是存储每个activity的transcript到文件中，Transcript是人和bot的对话动作的日志。\n如果我们要定制自己的bot动作，其实就可以继承botAdapter，然后对接自己的IM等等。botAdapter也是botbuilder-core中的，所以botbuilder-core是核心，只要读懂了botbuilder-core，就可以说是理解了微软的机器人框架。\nbotbuilder-core 看botbuilder-core，也从index.ts开始。\nexport * from \u0026#39;../../botframework-schema/lib\u0026#39;; export * from \u0026#39;./autoSaveStateMiddleware\u0026#39;; export * from \u0026#39;./botAdapter\u0026#39;; export * from \u0026#39;./botState\u0026#39;; export * from \u0026#39;./botStatePropertyAccessor\u0026#39;; export * from \u0026#39;./botStateSet\u0026#39;; export * from \u0026#39;./browserStorage\u0026#39;; export * from \u0026#39;./cardFactory\u0026#39;; export * from \u0026#39;./conversationState\u0026#39;; export * from \u0026#39;./memoryStorage\u0026#39;; export * from \u0026#39;./memoryTranscriptStore\u0026#39;; export * from \u0026#39;./messageFactory\u0026#39;; export * from \u0026#39;./middlewareSet\u0026#39;; export * from \u0026#39;./privateConversationState\u0026#39;; export * from \u0026#39;./propertyManager\u0026#39;; export * from \u0026#39;./recognizerResult\u0026#39;; export * from \u0026#39;./showTypingMiddleware\u0026#39;; export * from \u0026#39;./storage\u0026#39;; export * from \u0026#39;./testAdapter\u0026#39;; export * from \u0026#39;./transcriptLogger\u0026#39;; export * from \u0026#39;./turnContext\u0026#39;; export * from \u0026#39;./userState\u0026#39;; 这里引入了一个botframework-schema，通过名字可以看出来，这就是一个类型定义的包，主要是机器人Activity的Schema。Activity是人和bot所做的会话的程序级别的表示，该schema中包含了文本协议、多媒体和非内容动作（如社交互动和打字指示符）的规定。\n","permalink":"https://zhenfeng-zhu.github.io/posts/botbuilder/","summary":"在这篇文章不考虑人工智能，谈谈我对聊天机器人框架实现机制的理解。\n聊天机器人  聊天机器人（Chatterbot）是经由对话或文字进行交谈的计算机程序[1]。能够模拟人类对话，通过图灵测试。\n 我们可以看到现有的IM工具上已经有了很多机器人，其实聊天机器人不只是单纯的和用户进行聊天，他其实还可以做很多事情，例如根据用户输入的一些话，可以帮用户订餐。另外在运维领域，也出现了chatops，通过和机器人聊天，进行运维操作。\n机器人开发框架 作为聊天机器人开发者，面对如此多的IM工具和SDK，常会感到无所适从。Bot 开发框架就是对聊天机器人开发过程中的人工内容做抽象化处理。简单地解释，机器人开发框架就是用来制造机器人并定义其行为。\n然而尽管很多机器人框架宣称「代码一旦写好可部署到任何地方」，但是还会是出现为每一个IM工具开发一个单独的聊天机器人。而一个良好的机器人框架主要包含开发SDK，连接器和模拟器等。\n使用机器人框架其实并不适合初学者学习聊天机器人开发。它们尝试自动化太多工作，对初学者掩盖了基础机制。\n实现方式  webhook事件回调 FSM状态机 workflow工作流  最简单的机器人是没有上下文的语义理解的一问一答，仅仅是对用户的对话进行响应，这种就可以采用webhook的方式进行开发。不需要采用什么开发框架。\n那么对于多轮对话的时候，就需要进行一定的对话管理。由此引入了FSM状态机。\n可能有人不是很懂有限状态机，这里做一下简单说明。\n 有限状态机在现实生活中其实随处可见，伸缩式圆珠笔其实就是一个有限状态机（两种状态互相转换）。\n有限状态机，缩写为FSM，又称为有限状态自动机，简称状态机。是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。\n可以总结为：f(state, action) =\u0026gt; state’\n也就是说，这个函数采用当前的状态和一次行动（即更改状态的方法），之后将该行动应用于这种状态并返回新的状态。\n可以认为状态机是图灵完备的。\n 我们可以将对话看做是在有限状态内跳转的过程，每个状态都有对应的动作和回复，如果能从开始节点顺利的流转到终止节点，任务就完成了。\n我们可以将对话的过程，分为一个个的状态，然后使用DSL来实现一个FSM，对于开发者来讲，我们只需要关注一个个状态函数即可。\n特点是：\n 人为定义对话流程 完全有系统主导，系统问用户答 答非所问的情况直接忽略 建模简单，能清晰明了的把交互匹配到模型 难以扩展，很容易变的复杂 适用于简单的任务，难以处理复杂问题 缺少灵活性，表达能力有限，输入有限，对话结构和流转路径有限  示例：\nconst {startWith, when, goto, stay, stop} = botkit.DSL(fsm); startWith(MyStates.IDLE, {counter: 0}); when(MyStates.IDLE)(async (sender, content, data) =\u0026gt; { }); when(MyStates.UI)((sender, content, data) =\u0026gt; { }); when(MyStates.STEP1)((sender, content, data) =\u0026gt; { }); when(MyStates.","title":"谈谈聊天机器人框架的实现原理"},{"content":"公司项目中使用公网上的以太坊私链，交易速度比较慢，于是这几天都在鼓捣基于以太坊的联盟链，parity是可以构建出一个基于PoA共识的私链，而且兼容以太坊的合约。这篇文章主要是记录自己的踩坑经历，主要实现了节点的搭建，合约的部署以及本地以太坊浏览器的启动。\n部署联盟链 parity的文档：https://wiki.parity.io/Demo-PoA-tutorial\n安装 首先是下载parity，在mac下是直接brew安装即可。\nbrew tap paritytech/paritytech brew install parity 创世区块 创世区块的配置文件：\n// demo-spec.json { \u0026quot;name\u0026quot;: \u0026quot;DemoPoA\u0026quot;, \u0026quot;engine\u0026quot;: { \u0026quot;authorityRound\u0026quot;: { \u0026quot;params\u0026quot;: { \u0026quot;stepDuration\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;validators\u0026quot;: { \u0026quot;list\u0026quot;: [ \u0026quot;0x00bd138abd70e2f00903268f3db08f2d25677c9e\u0026quot;, \u0026quot;0x00aa39d30f0d20ff03a22ccfc30b7efbfca597c2\u0026quot; ] } } } }, \u0026quot;params\u0026quot;: { \u0026quot;gasLimitBoundDivisor\u0026quot;: \u0026quot;0x400\u0026quot;, \u0026quot;maximumExtraDataSize\u0026quot;: \u0026quot;0x20\u0026quot;, \u0026quot;minGasLimit\u0026quot;: \u0026quot;0x1388\u0026quot;, \u0026quot;networkID\u0026quot;: \u0026quot;0x2323\u0026quot;, \u0026quot;eip155Transition\u0026quot;: 0, \u0026quot;validateChainIdTransition\u0026quot;: 0, \u0026quot;eip140Transition\u0026quot;: 0, \u0026quot;eip211Transition\u0026quot;: 0, \u0026quot;eip214Transition\u0026quot;: 0, \u0026quot;eip658Transition\u0026quot;: 0 }, \u0026quot;genesis\u0026quot;: { \u0026quot;seal\u0026quot;: { \u0026quot;authorityRound\u0026quot;: { \u0026quot;step\u0026quot;: \u0026quot;0x0\u0026quot;, \u0026quot;signature\u0026quot;: \u0026quot;0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\u0026quot; } }, \u0026quot;difficulty\u0026quot;: \u0026quot;0x20000\u0026quot;, \u0026quot;gasLimit\u0026quot;: \u0026quot;0x5B8D80\u0026quot; }, \u0026quot;accounts\u0026quot;: { \u0026quot;0x0000000000000000000000000000000000000001\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;ecrecover\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 3000, \u0026quot;word\u0026quot;: 0 } } } }, \u0026quot;0x0000000000000000000000000000000000000002\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;sha256\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 60, \u0026quot;word\u0026quot;: 12 } } } }, \u0026quot;0x0000000000000000000000000000000000000003\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;ripemd160\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 600, \u0026quot;word\u0026quot;: 120 } } } }, \u0026quot;0x0000000000000000000000000000000000000004\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;identity\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 15, \u0026quot;word\u0026quot;: 3 } } } }, \u0026quot;0x004ec07d2329997267ec62b4166639513386f32e\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;10000000000000000000000\u0026quot; } } } node0 node0节点：\n## node0.toml [parity] chain = \u0026#34;demo-spec.json\u0026#34; base_path = \u0026#34;parity0\u0026#34; [network] port = 30300 [rpc] port = 8546 apis = [\u0026#34;web3\u0026#34;, \u0026#34;eth\u0026#34;, \u0026#34;net\u0026#34;, \u0026#34;personal\u0026#34;, \u0026#34;parity\u0026#34;, \u0026#34;parity_set\u0026#34;, \u0026#34;traces\u0026#34;, \u0026#34;rpc\u0026#34;, \u0026#34;parity_accounts\u0026#34;] interface = \u0026#34;0.0.0.0\u0026#34; cors = [\u0026#34;*\u0026#34;] hosts = [\u0026#34;all\u0026#34;] [websockets] port = 8456 [account] password = [\u0026#34;node.pwds\u0026#34;] [mining] engine_signer = \u0026#34;0x00bd138abd70e2f00903268f3db08f2d25677c9e\u0026#34; reseal_on_txs = \u0026#34;none\u0026#34; node1 node1节点：\n## node1.toml [parity] chain = \u0026#34;demo-spec.json\u0026#34; base_path = \u0026#34;parity1\u0026#34; [network] port = 30301 [rpc] port = 8541 apis = [\u0026#34;web3\u0026#34;, \u0026#34;eth\u0026#34;, \u0026#34;net\u0026#34;, \u0026#34;personal\u0026#34;, \u0026#34;parity\u0026#34;, \u0026#34;parity_set\u0026#34;, \u0026#34;traces\u0026#34;, \u0026#34;rpc\u0026#34;, \u0026#34;parity_accounts\u0026#34;] [websockets] port = 8451 [ipc] disable = true [account] password = [\u0026#34;node.pwds\u0026#34;] [mining] engine_signer = \u0026#34;0x00aa39d30f0d20ff03a22ccfc30b7efbfca597c2\u0026#34; reseal_on_txs = \u0026#34;none\u0026#34; [ui] disable = true 启动并创建账户 启动\nparity --config node0.toml --fat-db=on parity --config node1.toml --fat-db=on 创建账户：\ncurl --data '{\u0026quot;jsonrpc\u0026quot;:\u0026quot;2.0\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;parity_newAccountFromPhrase\u0026quot;,\u0026quot;params\u0026quot;:[\u0026quot;node0\u0026quot;, \u0026quot;node0\u0026quot;],\u0026quot;id\u0026quot;:0}' -H \u0026quot;Content-Type: application/json\u0026quot; -X POST localhost:8546 curl --data '{\u0026quot;jsonrpc\u0026quot;:\u0026quot;2.0\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;parity_newAccountFromPhrase\u0026quot;,\u0026quot;params\u0026quot;:[\u0026quot;user\u0026quot;, \u0026quot;user\u0026quot;],\u0026quot;id\u0026quot;:0}' -H \u0026quot;Content-Type: application/json\u0026quot; -X POST localhost:8546 curl --data '{\u0026quot;jsonrpc\u0026quot;:\u0026quot;2.0\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;parity_newAccountFromPhrase\u0026quot;,\u0026quot;params\u0026quot;:[\u0026quot;node1\u0026quot;, \u0026quot;node1\u0026quot;],\u0026quot;id\u0026quot;:0}' -H \u0026quot;Content-Type: application/json\u0026quot; -X POST localhost:8541 这样就创建了3个账户，其中node0和node1是见证者 user是初始发钱的。\n因为parity ui是要连接8546端口，所以这里就让node0的rpc的端口是8546。\n节点互通和转账 让node0和node1节点相通，其实就是让两个节点成为一个网络：\n// 获取node0的encode curl --data '{\u0026quot;jsonrpc\u0026quot;:\u0026quot;2.0\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;parity_enode\u0026quot;,\u0026quot;params\u0026quot;:[],\u0026quot;id\u0026quot;:0}' -H \u0026quot;Content-Type: application/json\u0026quot; -X POST localhost:8546 // 调用node1的rpc，将node0加入， RESULT就是上一步获取的 curl --data '{\u0026quot;jsonrpc\u0026quot;:\u0026quot;2.0\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;parity_addReservedPeer\u0026quot;,\u0026quot;params\u0026quot;:[\u0026quot;enode://RESULT\u0026quot;],\u0026quot;id\u0026quot;:0}' -H \u0026quot;Content-Type: application/json\u0026quot; -X POST localhost:8541 我们先给两个账户转账：\ncurl --data '{\u0026quot;jsonrpc\u0026quot;:\u0026quot;2.0\u0026quot;,\u0026quot;method\u0026quot;:\u0026quot;personal_sendTransaction\u0026quot;,\u0026quot;params\u0026quot;:[{\u0026quot;from\u0026quot;:\u0026quot;0x004ec07d2329997267Ec62b4166639513386F32e\u0026quot;,\u0026quot;to\u0026quot;:\u0026quot;0x00Bd138aBD70e2F00903268F3Db08f2D25677C9e\u0026quot;,\u0026quot;value\u0026quot;:\u0026quot;0xde0b6b3a7640000\u0026quot;}, \u0026quot;user\u0026quot;],\u0026quot;id\u0026quot;:0}' -H \u0026quot;Content-Type: application/json\u0026quot; -X POST localhost:8540 从user中转了1个以太坊到了node0账户中，同样再转给node1。\n部署合约 再使用truffle开发完合约之后，把账户部署到我们刚刚起来的联盟链。部署合约需要消耗一定的gas，truffle使用的是HD wallet的Provider，所以我们要先给一个钱包转一些以太币。\n因为这里用的是metamask，在最初创建钱包的时候有设置12个助记词，所以先让钱包连接到node0节点：\n创建一个账户，向那个账户转几个以太币。\n然后在truffle中，配置如下：\n// truffle.js module.exports = { // See \u0026lt;http://truffleframework.com/docs/advanced/configuration\u0026gt; // to customize your Truffle configuration! networks: { development: { host: \u0026quot;127.0.0.1\u0026quot;, port: 8545, network_id: \u0026quot;*\u0026quot; }, parity: { provider: function () { return new HDWalletProvider('这里写助记词', \u0026quot;http://127.0.0.1:8546\u0026quot;) }, network_id: 3, gas: 4700000 } } }; 然后在执行部署合约的时候，指定parity即可：\ntruffle migrate --network parity 部署以太坊浏览器 以太坊的浏览器找了好几个，最后选中了etherchain-light。部署起来简单。\n首先clone代码到本地，然后npm安装依赖。\ngit clone https://github.com/gobitfly/etherchain-light --recursive cd etherchain-light \u0026amp;\u0026amp; yarn 一定要用—-recursive，将所有git的子模块都下载下来。\n修改config.js.example文件为config.js，然后把Provider改为HttpProvider，连接到node0的节点即可。\n// config.js var web3 = require('web3'); var net = require('net'); var config = function () { this.logFormat = \u0026quot;combined\u0026quot;; // this.ipcPath = process.env[\u0026quot;HOME\u0026quot;] + \u0026quot;/.local/share/io.parity.ethereum/jsonrpc.ipc\u0026quot;; // this.provider = new web3.providers.IpcProvider(this.ipcPath, net); this.provider = new web3.providers.HttpProvider(\u0026quot;http://127.0.0.1:8546\u0026quot;) // ... 省略其余代码 } module.exports = config; 执行npm start之后即可将以太坊浏览器运行起来。然后在浏览器中访问http://localhost:3000。\n思考 PoA共识基于权威的共识机制，和基于raft协议的共识机制具体哪个更快？\nParity文档中没有找到和权限控制相关的模块，用它来做联盟链还有待确定。\nQuorum是JP摩根开源的基于以太坊的联盟链，使用的raft算法，可以研究研究。\n还不是很清楚，fabric已经是联盟链主流的情况下，选择以太坊做联盟链的好处有多大。\n","permalink":"https://zhenfeng-zhu.github.io/posts/parity/","summary":"公司项目中使用公网上的以太坊私链，交易速度比较慢，于是这几天都在鼓捣基于以太坊的联盟链，parity是可以构建出一个基于PoA共识的私链，而且兼容以太坊的合约。这篇文章主要是记录自己的踩坑经历，主要实现了节点的搭建，合约的部署以及本地以太坊浏览器的启动。\n部署联盟链 parity的文档：https://wiki.parity.io/Demo-PoA-tutorial\n安装 首先是下载parity，在mac下是直接brew安装即可。\nbrew tap paritytech/paritytech brew install parity 创世区块 创世区块的配置文件：\n// demo-spec.json { \u0026quot;name\u0026quot;: \u0026quot;DemoPoA\u0026quot;, \u0026quot;engine\u0026quot;: { \u0026quot;authorityRound\u0026quot;: { \u0026quot;params\u0026quot;: { \u0026quot;stepDuration\u0026quot;: \u0026quot;5\u0026quot;, \u0026quot;validators\u0026quot;: { \u0026quot;list\u0026quot;: [ \u0026quot;0x00bd138abd70e2f00903268f3db08f2d25677c9e\u0026quot;, \u0026quot;0x00aa39d30f0d20ff03a22ccfc30b7efbfca597c2\u0026quot; ] } } } }, \u0026quot;params\u0026quot;: { \u0026quot;gasLimitBoundDivisor\u0026quot;: \u0026quot;0x400\u0026quot;, \u0026quot;maximumExtraDataSize\u0026quot;: \u0026quot;0x20\u0026quot;, \u0026quot;minGasLimit\u0026quot;: \u0026quot;0x1388\u0026quot;, \u0026quot;networkID\u0026quot;: \u0026quot;0x2323\u0026quot;, \u0026quot;eip155Transition\u0026quot;: 0, \u0026quot;validateChainIdTransition\u0026quot;: 0, \u0026quot;eip140Transition\u0026quot;: 0, \u0026quot;eip211Transition\u0026quot;: 0, \u0026quot;eip214Transition\u0026quot;: 0, \u0026quot;eip658Transition\u0026quot;: 0 }, \u0026quot;genesis\u0026quot;: { \u0026quot;seal\u0026quot;: { \u0026quot;authorityRound\u0026quot;: { \u0026quot;step\u0026quot;: \u0026quot;0x0\u0026quot;, \u0026quot;signature\u0026quot;: \u0026quot;0x0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\u0026quot; } }, \u0026quot;difficulty\u0026quot;: \u0026quot;0x20000\u0026quot;, \u0026quot;gasLimit\u0026quot;: \u0026quot;0x5B8D80\u0026quot; }, \u0026quot;accounts\u0026quot;: { \u0026quot;0x0000000000000000000000000000000000000001\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;ecrecover\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 3000, \u0026quot;word\u0026quot;: 0 } } } }, \u0026quot;0x0000000000000000000000000000000000000002\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;sha256\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 60, \u0026quot;word\u0026quot;: 12 } } } }, \u0026quot;0x0000000000000000000000000000000000000003\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;ripemd160\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 600, \u0026quot;word\u0026quot;: 120 } } } }, \u0026quot;0x0000000000000000000000000000000000000004\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;builtin\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;identity\u0026quot;, \u0026quot;pricing\u0026quot;: { \u0026quot;linear\u0026quot;: { \u0026quot;base\u0026quot;: 15, \u0026quot;word\u0026quot;: 3 } } } }, \u0026quot;0x004ec07d2329997267ec62b4166639513386f32e\u0026quot;: { \u0026quot;balance\u0026quot;: \u0026quot;10000000000000000000000\u0026quot; } } } node0 node0节点：","title":"基于以太坊的Parity联盟链部署"},{"content":"redis持久化，\n机制有两种：\n 快照：全量备份，二进制序列化，存储紧凑 AOF日志：连续的增量备份，内存数据修改的文本  ","permalink":"https://zhenfeng-zhu.github.io/posts/dive-into-redis/","summary":"redis持久化，\n机制有两种：\n 快照：全量备份，二进制序列化，存储紧凑 AOF日志：连续的增量备份，内存数据修改的文本  ","title":"dive-into-redis"},{"content":"一  x509 error when using HTTPS inside a Docker container\n 因为docker中没有CA证书。\n普通的镜像解决办法\nFROM ubuntu:14.04.1 RUN apt-get update RUN apt-get install -y ca-certificates CMD curl https://www.google.com 如果是alpine的参考这个：\nFROM docker.finogeeks.club/base/alpine MAINTAINER \u0026quot;zhuzhenfeng@finogeeks.club\u0026quot; RUN set -ex \\ \u0026amp;\u0026amp; apk add --no-cache ca-certificates COPY src/wallet/wallet /opt/wallet ENTRYPOINT /opt/wallet 二  panic: runtime error: invalid memory address or nil pointer dereference [signal 0xb code=0x1 addr=0x38 pc=0x26df]\n \u0026ldquo;An error is returned if caused by client policy (such as CheckRedirect), or if there was an HTTP protocol error. A non-2xx response doesn\u0026rsquo;t cause an error.\nWhen err is nil, resp always contains a non-nil resp.Body.\u0026quot;\n是http请求的时候，defer res.Body.Close()引起的，应该在err检查之后。\nThe defer only defers the function call. The field and method are accessed immediately.\n","permalink":"https://zhenfeng-zhu.github.io/posts/golang%E8%B8%A9%E5%9D%91/","summary":"一  x509 error when using HTTPS inside a Docker container\n 因为docker中没有CA证书。\n普通的镜像解决办法\nFROM ubuntu:14.04.1 RUN apt-get update RUN apt-get install -y ca-certificates CMD curl https://www.google.com 如果是alpine的参考这个：\nFROM docker.finogeeks.club/base/alpine MAINTAINER \u0026quot;zhuzhenfeng@finogeeks.club\u0026quot; RUN set -ex \\ \u0026amp;\u0026amp; apk add --no-cache ca-certificates COPY src/wallet/wallet /opt/wallet ENTRYPOINT /opt/wallet 二  panic: runtime error: invalid memory address or nil pointer dereference [signal 0xb code=0x1 addr=0x38 pc=0x26df]\n \u0026ldquo;An error is returned if caused by client policy (such as CheckRedirect), or if there was an HTTP protocol error.","title":"golang踩坑"},{"content":"最近因公司项目需要，做为一个打杂工程师，操起键盘和笔记本开始了以太坊的踩坑之旅。以太坊的开发比较新，变化也比较多，还好有@cctanfujun的手把手带领下，半只脚踏入了以太坊的开发的大门。\n在这篇文章中，我将会简单介绍一下以太坊的基本概念，以及我现在用到的一些工具，还有具体的一个开发流程。因为我还没有接触到如何上主链，所以这些都是基于测试链讲解。希望能给大家带来一些帮助。\n什么是区块链\n相信大家对区块链都有自己的理解，不仅仅是互联网公司，传统企业也在“币改转型”。\n**简言之，区块链就是数据库。**它是特定数据的数据库，里面的数据不断增长，具有非凡特性：\n 一旦数据存储于数据库，永远都无法被修改或删除。区块链上的每个记录会被永久保存下来。 没有单独的个人或组织能维护该数据库。必须要上千个人才行，每个人都有数据库的副本。  什么是以太坊？\n 以太坊（英语：Ethereum）是一个开源的有智能合约功能的公共区块链平台[1][2]。通过其专用加密货币以太币（Ether，又称“以太币”）提供去中心化的虚拟机（称为“以太虚拟机”Ethereum Virtual Machine）来处理点对点合约。\n 为什么选择以太坊？\n  智能合约\n  代币\n  资料相对完善，相对容易开发\n  大佬对以太坊比较熟悉\n  大佬对以太坊比较熟悉\n  大佬对以太坊比较熟悉\n  重要的事情说三遍，有一个经验丰富的人带领，做东西肯定事半功倍。\n自己动手写区块链\n这里提供两个教程，一个是书，一个是视频。其中视频和书是对应的，不清楚是不是同一个作者。\nBlockchain Tutorial\n私有区块链，我们一起GO\n以太坊开发\n由于我是专注于后端的开发，现在的技术栈是\n node go  正式进入以太坊的开发。这是我这段时间接触到的一些资源：\n  go-ethereum：也就是geth，官方的go版本的客户端\n  solidity：智能合约编程语言\n  truffle：智能合约的编程框架，基于nodejs\n  Ganache：启动了多个节点本地私链\n  Rinkeby：以太坊测试链\n  Etherscan：以太坊区块链浏览器，可以查询交易\n  MetaMask：chrome的钱包插件\n  web3：官方封装的开发Dapp的库，可以调用合约\n  truffle-hdwallet-provider：web3的确定性钱包provider\n  概念\n账户和钱包\n在以太坊中，一个账号就是一个地址（address），里面有余额。\n钱包是保管私钥的地址， 私钥-\u0026gt;公钥-\u0026gt;地址 这是一个一一对应的关系，钱包里面可以有多个账户。\n私钥不同的生成方法，对应着不同的钱包结构，因此分为了确定性钱包和非确定性钱包。\n 比特币最早的客户端（Satoshi client）就是非确定性钱包，钱包是一堆随机生成的私钥的集合。 客户端会预先生成 100 个随机私钥，并且每个私钥只使用一次。 确定性钱包则不需要每次转账都要备份，确定性钱包的私钥是对种子进行单向哈希运算生成的，种子是一串由随机数生成器生成的随机数。在确定性钱包中，只要有这个种子，就可以找回所有私钥  HD 钱包是目前常用的确定性钱包 ，说到 HD 钱包，大家可能第一反应会想到硬件钱包 （Hardware Wallet），其实这里的 HD 是 Hierarchical Deterministic（分层确定性）的缩写。\n 所谓分层，就是一个大公司可以为每个子部门分别生成不同的私钥，子部门还可以再管理子子部门的私钥，每个部门可以看到所有子部门里的币，也可以花这里面的币。也可以只给会计人员某个层级的公钥，让他可以看见这个部门及子部门的收支记录，但不能花里面的钱，使得财务管理更方便了。\n 生成规则是：\n 生成一个助记词（参见 BIP39） 该助记词使用 PBKDF2 转化为种子（参见 BIP39） 种子用于使用 HMAC-SHA512 生成根私钥（参见 BIP32） 从该根私钥，导出子私钥（参见 BIP32），其中节点布局由BIP44设置  DAPP\n以太坊与其他加密货币的主要不同在于，以太坊不是单纯的货币，而是一个环境/平台。在这个平台上，任何人都可以利用区块链的技术，通过智能合约来构建自己的项目和DAPPS（去中心化应用）。DAPPS发布的方式通常是采用被称为“ICO”的众筹方式。简单来说，你需要用你的以太来购买相应DAPP的一些tokens。\n代币\n为什么不能在这些DAPPS中直接使用以太完成交易？为什么我们需要给DAPPS创造一种原生的货币？\n因为即使在现实生活中，我们也在使用某种形式的Token来代替现金。比如：在游乐场里，你先用现金兑换代币，然后用代币来支付各种服务。在这个例子中，现金就是以太，代币就是token。\nERC20：以太坊token标准\n简单来说，ERC20是开发者在自己的tokens中必须采用的一套具体的公式/方法，从而确保该token与ERC20兼容。在合约执行过程中，下面的四个行为是ERC20 tokens所需要完成的：\n 获得Token供给总量. 获得账户余额. 从一方向另一方转移Token. 认可Token作为货币性资产的使用.  大佬说：代币其实就是智能合约，而这个合约是发生了0个以太的转账。\nGas和挖矿\n不少小哥哥或小姐姐会认为挖矿就是挖以太币，其实代币不用挖的，当你挖到了区块，代币是给你的奖励。因为任何一笔交易都需要记录，一个区块的大小也就几M，存储不了那么多交易信息，所以要持续挖区块来记录交易，同时只要是你发起了交易，就得付手续费，这些手续费也成为Gas，会按照一定的算法奖励给挖出区块的人。\n接下来会讲一下，平时开发中如何创建钱包，如何转账，如何自己发代币，如何部署合约并调用。\n环境准备\n 安装Ganache并启动 安装truffle框架  创建钱包\ngolang\n依赖\ngithub.com/ethereum/go-ethereum 首先要连接到测试链，测试链可以是本地的也可以是公网的。\nfunc connectRPC() (*ethclient.Client, error) { // 连接测试链的节点 //rpcClient, err := rpc.Dial(\u0026quot;https://rinkeby.infura.io/v3/6c81fb1b66804f0698d49f2ec242afc9\u0026quot;) rpcClient, err := rpc.Dial(\u0026quot;http://127.0.0.1:7545\u0026quot;) if err != nil { log.Fatalln(err) return nil, err } conn := ethclient.NewClient(rpcClient) return conn, nil } 一般都选择以keystore的形式创建账户\nfunc CreateWallet() (key, addr string) { ks := keystore.NewKeyStore(\u0026quot;~/Documents/github/gowork/src/geth-demo/\u0026quot;, keystore.StandardScryptN, keystore.StandardScryptP) account, _ := ks.NewAccount(\u0026quot;password\u0026quot;) key_json, err := ks.Export(account, \u0026quot;password\u0026quot;, \u0026quot;password\u0026quot;) if err != nil { log.Fatalln(\u0026quot;导出账户错误: \u0026quot;, err) panic(err) } key = string(key_json) addr = account.Address.Hex() return } 当然另一种创建账户的方式是用私钥：\nfunc CreateWallet() (string, error) { key, err := crypto.GenerateKey() if err != nil { log.Fatalln(err) return \u0026quot;\u0026quot;, nil } address := crypto.PubkeyToAddress(key.PublicKey).Hex() log.Println(\u0026quot;address: \u0026quot;, address) privateKey := hex.EncodeToString(key.D.Bytes()) log.Println(\u0026quot;privateKey: \u0026quot;, privateKey) return address, nil } Node\nnode一般使用web3。创建web3对象的时候要使用一个provider，这个provider用来连接到测试链，可以是钱包的，也可以是一个HttpProvider。\n创建web3\nconst web3 = new Web3(new Web3.providers.HttpProvider(\u0026quot;http://localhost:7545\u0026quot;)); 或者使用truffle-hdwallet-provider来创建，使用这个的前提是，自己已经创建了一个钱包，并且这个钱包是HD的。\nconst Web3 = require('web3'); const HDWalletProvider = require('truffle-hdwallet-provider'); const provider = new HDWalletProvider(助记词, 测试链url); const web3 = new Web3(provider); 创建账户\n ","permalink":"https://zhenfeng-zhu.github.io/posts/eth-tools/","summary":"最近因公司项目需要，做为一个打杂工程师，操起键盘和笔记本开始了以太坊的踩坑之旅。以太坊的开发比较新，变化也比较多，还好有@cctanfujun的手把手带领下，半只脚踏入了以太坊的开发的大门。\n在这篇文章中，我将会简单介绍一下以太坊的基本概念，以及我现在用到的一些工具，还有具体的一个开发流程。因为我还没有接触到如何上主链，所以这些都是基于测试链讲解。希望能给大家带来一些帮助。\n什么是区块链\n相信大家对区块链都有自己的理解，不仅仅是互联网公司，传统企业也在“币改转型”。\n**简言之，区块链就是数据库。**它是特定数据的数据库，里面的数据不断增长，具有非凡特性：\n 一旦数据存储于数据库，永远都无法被修改或删除。区块链上的每个记录会被永久保存下来。 没有单独的个人或组织能维护该数据库。必须要上千个人才行，每个人都有数据库的副本。  什么是以太坊？\n 以太坊（英语：Ethereum）是一个开源的有智能合约功能的公共区块链平台[1][2]。通过其专用加密货币以太币（Ether，又称“以太币”）提供去中心化的虚拟机（称为“以太虚拟机”Ethereum Virtual Machine）来处理点对点合约。\n 为什么选择以太坊？\n  智能合约\n  代币\n  资料相对完善，相对容易开发\n  大佬对以太坊比较熟悉\n  大佬对以太坊比较熟悉\n  大佬对以太坊比较熟悉\n  重要的事情说三遍，有一个经验丰富的人带领，做东西肯定事半功倍。\n自己动手写区块链\n这里提供两个教程，一个是书，一个是视频。其中视频和书是对应的，不清楚是不是同一个作者。\nBlockchain Tutorial\n私有区块链，我们一起GO\n以太坊开发\n由于我是专注于后端的开发，现在的技术栈是\n node go  正式进入以太坊的开发。这是我这段时间接触到的一些资源：\n  go-ethereum：也就是geth，官方的go版本的客户端\n  solidity：智能合约编程语言\n  truffle：智能合约的编程框架，基于nodejs\n  Ganache：启动了多个节点本地私链\n  Rinkeby：以太坊测试链\n  Etherscan：以太坊区块链浏览器，可以查询交易\n  MetaMask：chrome的钱包插件","title":"以太坊开发总结"},{"content":"参与了公司的一个项目，上了以太坊，这里简单记录一下踩坑。\n首先先把go的依赖下载下来：\ngo get -u -v github.com/ethereum/go-ethereum 有时候下载的很慢，可以从github上拉下来代码。\n账户 以太坊的地址在离线状态下也可以创建到。\n创建账户有两种方式：\n以公钥和私钥的形式创建 func CreateAccount() (string, error) { key, err := crypto.GenerateKey() if err != nil { log.Fatalln(err) return \u0026quot;\u0026quot;, nil } address := crypto.PubkeyToAddress(key.PublicKey).Hex() log.Println(\u0026quot;address: \u0026quot;, address) privateKey := hex.EncodeToString(key.D.Bytes()) log.Println(\u0026quot;privateKey: \u0026quot;, privateKey) return address, nil } 这种方式一般用的比较少。\n以keystore的形式创建 keystore会创建一个文件，这个文件如下所示：\n{ \u0026quot;address\u0026quot;: \u0026quot;d93688757810e644f0b9c162102d9c598813f0dd\u0026quot;, \u0026quot;crypto\u0026quot;: { \u0026quot;cipher\u0026quot;: \u0026quot;aes-128-ctr\u0026quot;, \u0026quot;ciphertext\u0026quot;: \u0026quot;71ae7c8144729b2f9e0c51d95c6dfb73e63f14b5332b3594e8a1f325237c27ed\u0026quot;, \u0026quot;cipherparams\u0026quot;: { \u0026quot;iv\u0026quot;: \u0026quot;620c73001081c014a862ce80003a4648\u0026quot; }, \u0026quot;kdf\u0026quot;: \u0026quot;scrypt\u0026quot;, \u0026quot;kdfparams\u0026quot;: { \u0026quot;dklen\u0026quot;: 32, \u0026quot;n\u0026quot;: 262144, \u0026quot;p\u0026quot;: 1, \u0026quot;r\u0026quot;: 8, \u0026quot;salt\u0026quot;: \u0026quot;bd272aa37271ef9913eb095a4d143be238e348c48fce6459896e1bb1b0236741\u0026quot; }, \u0026quot;mac\u0026quot;: \u0026quot;2b3ade771645090a2b34c214906c592a1300d529e459faefb1421ba496b6fe1d\u0026quot; }, \u0026quot;id\u0026quot;: \u0026quot;e4dd5384-56a8-4ec7-b6e0-492dcd3742e9\u0026quot;, \u0026quot;version\u0026quot;: 3 } 在生成这个文件的时候，会让你输一个密码，这个文件加密码其实就是一个私钥。\n// 理论上来讲，这个函数应该只被创建一次即可 // 创建一个账户 func CreateWallet() (key, addr string) { ks := keystore.NewKeyStore(\u0026quot;/Users/zhuzhenfeng/Documents/github/gowork/src/geth-demo/\u0026quot;, keystore.StandardScryptN, keystore.StandardScryptP) account, _ := ks.NewAccount(\u0026quot;password\u0026quot;) key_json, err := ks.Export(account, \u0026quot;password\u0026quot;, \u0026quot;password\u0026quot;) if err != nil { log.Fatalln(\u0026quot;导出账户错误: \u0026quot;, err) panic(err) } key = string(key_json) addr = account.Address.Hex() return } 这个key和address，\u0026ldquo;password\u0026quot;是这个文件的密码。\n私链 一种方式是连接互联网上的测试链，一种是连接本地的私链。\n本地私链的启动 启动本地私链最简单的一种方式是用Truffle提供的Ganache，只要将它下载下来，启动起来即可。\n可以看到已经启动了，连接的地址是:\nhttp://127.0.0.1:7545 互联网上的测试链地址 https://rinkeby.infura.io/v3/6c81fb1b66804f0698d49f2ec242afc9 连接 我们用geth的rpc连接上面的私链地址即可：\nfunc connectRPC() (*ethclient.Client, error) { // 连接测试链的节点 //rpcClient, err := rpc.Dial(\u0026quot;https://rinkeby.infura.io/v3/6c81fb1b66804f0698d49f2ec242afc9\u0026quot;) rpcClient, err := rpc.Dial(\u0026quot;http://127.0.0.1:7545\u0026quot;) if err != nil { log.Fatalln(err) return nil, err } conn := ethclient.NewClient(rpcClient) return conn, nil } 其他操作 在上一步中连接rpc中，拿到了client。用这个client就可以做很多事儿：\n获取余额 func GetBalance(address string) (float64, error) { client, err := connectRPC() if err != nil { log.Fatalln(\u0026quot;err: \u0026quot;, err) panic(err) } balance, err := client.BalanceAt(context.TODO(), common.HexToAddress(address), nil) if err != nil { log.Fatalln(balance) return 0, err } balanceV := float64(balance.Int64()) * math.Pow(10, -18) return balanceV, nil } 代币转账 要生成代币，需要写一个token的合约。可以用这一个最简单的token.abi。复杂一般可以用truffle框架来编写。\n// token.abi [ { \u0026quot;anonymous\u0026quot;: false, \u0026quot;inputs\u0026quot;: [ { \u0026quot;indexed\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;from\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;address\u0026quot; }, { \u0026quot;indexed\u0026quot;: true, \u0026quot;name\u0026quot;: \u0026quot;to\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;address\u0026quot; }, { \u0026quot;indexed\u0026quot;: false, \u0026quot;name\u0026quot;: \u0026quot;value\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;uint256\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;Transfer\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;event\u0026quot; }, { \u0026quot;constant\u0026quot;: true, \u0026quot;inputs\u0026quot;: [], \u0026quot;name\u0026quot;: \u0026quot;totalSupply\u0026quot;, \u0026quot;outputs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;uint256\u0026quot; } ], \u0026quot;payable\u0026quot;: false, \u0026quot;stateMutability\u0026quot;: \u0026quot;view\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot; }, { \u0026quot;constant\u0026quot;: false, \u0026quot;inputs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;_to\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;address\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;_value\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;uint256\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;transfer\u0026quot;, \u0026quot;outputs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bool\u0026quot; } ], \u0026quot;payable\u0026quot;: false, \u0026quot;stateMutability\u0026quot;: \u0026quot;nonpayable\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot; }, { \u0026quot;constant\u0026quot;: true, \u0026quot;inputs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;_owner\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;address\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;balanceOf\u0026quot;, \u0026quot;outputs\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;uint256\u0026quot; } ], \u0026quot;payable\u0026quot;: false, \u0026quot;stateMutability\u0026quot;: \u0026quot;view\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;function\u0026quot; } ] 然后将其转换为go文件：\nabigen --abi token.abi --pkg main --type Token --out token.go 生成的token.go文件，才是可以被操作的文件。\nfunc TransferToken() { key, to_address := CreateWallet() client, err := connectRPC() if err != nil { log.Fatalln(err) panic(err) } auth, err := bind.NewTransactor(strings.NewReader(key), \u0026quot;password\u0026quot;) if err != nil { log.Fatalln(\u0026quot;读取keystore失败\u0026quot;, err) panic(err) } token, err := cont.NewToken(common.HexToAddress(\u0026quot;0x75a26aaaecda412bd914e8fbfaed586a467fa8b5\u0026quot;), client) if err != nil { log.Fatalln(\u0026quot;获取token失败\u0026quot;, err) panic(err) } balance, err := token.BalanceOf(nil, common.HexToAddress(to_address)) if err != nil { log.Fatalln(\u0026quot;token balance of\u0026quot;, err) } log.Println(\u0026quot;to address balance: \u0026quot;, balance) amount := big.NewFloat(10.00) //这是处理位数的代码段 tenDecimal := big.NewFloat(math.Pow(10, 18)) convertAmount, _ := new(big.Float).Mul(tenDecimal, amount).Int(\u0026amp;big.Int{}) tx, err := token.Transfer(auth, common.HexToAddress(to_address), convertAmount) if nil != err { fmt.Printf(\u0026quot;err: %v \\n\u0026quot;, err) return } fmt.Printf(\u0026quot;result: %v\\n\u0026quot;, tx) }  不知为何会出现failed to estimate gas needed的情况\n 代币转账的代码就如上所示，有了这个，其实我们就可以发币了。\n至于上面的一句代码中的：\ntoken, err := cont.NewToken(common.HexToAddress(\u0026quot;0x75a26aaaecda412bd914e8fbfaed586a467fa8b5\u0026quot;), client) 这个0x75a26aaaecda412bd914e8fbfaed586a467fa8b5地址，就是合约的地址。关于合约的部署，其实如果是在本地调试的话，可以用truffle。truffle在部署的时候，会将合约地址返回。\n如下图所示：\n关于如何在以太坊上写代币的合约，下次再写一下笔记。\n总结 可以用web3去做这些事儿，会更简单一些。因为服务端选型用的go，所以就用go踩了一些坑。\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E4%BB%A5%E5%A4%AA%E5%9D%8A/","summary":"参与了公司的一个项目，上了以太坊，这里简单记录一下踩坑。\n首先先把go的依赖下载下来：\ngo get -u -v github.com/ethereum/go-ethereum 有时候下载的很慢，可以从github上拉下来代码。\n账户 以太坊的地址在离线状态下也可以创建到。\n创建账户有两种方式：\n以公钥和私钥的形式创建 func CreateAccount() (string, error) { key, err := crypto.GenerateKey() if err != nil { log.Fatalln(err) return \u0026quot;\u0026quot;, nil } address := crypto.PubkeyToAddress(key.PublicKey).Hex() log.Println(\u0026quot;address: \u0026quot;, address) privateKey := hex.EncodeToString(key.D.Bytes()) log.Println(\u0026quot;privateKey: \u0026quot;, privateKey) return address, nil } 这种方式一般用的比较少。\n以keystore的形式创建 keystore会创建一个文件，这个文件如下所示：\n{ \u0026quot;address\u0026quot;: \u0026quot;d93688757810e644f0b9c162102d9c598813f0dd\u0026quot;, \u0026quot;crypto\u0026quot;: { \u0026quot;cipher\u0026quot;: \u0026quot;aes-128-ctr\u0026quot;, \u0026quot;ciphertext\u0026quot;: \u0026quot;71ae7c8144729b2f9e0c51d95c6dfb73e63f14b5332b3594e8a1f325237c27ed\u0026quot;, \u0026quot;cipherparams\u0026quot;: { \u0026quot;iv\u0026quot;: \u0026quot;620c73001081c014a862ce80003a4648\u0026quot; }, \u0026quot;kdf\u0026quot;: \u0026quot;scrypt\u0026quot;, \u0026quot;kdfparams\u0026quot;: { \u0026quot;dklen\u0026quot;: 32, \u0026quot;n\u0026quot;: 262144, \u0026quot;p\u0026quot;: 1, \u0026quot;r\u0026quot;: 8, \u0026quot;salt\u0026quot;: \u0026quot;bd272aa37271ef9913eb095a4d143be238e348c48fce6459896e1bb1b0236741\u0026quot; }, \u0026quot;mac\u0026quot;: \u0026quot;2b3ade771645090a2b34c214906c592a1300d529e459faefb1421ba496b6fe1d\u0026quot; }, \u0026quot;id\u0026quot;: \u0026quot;e4dd5384-56a8-4ec7-b6e0-492dcd3742e9\u0026quot;, \u0026quot;version\u0026quot;: 3 } 在生成这个文件的时候，会让你输一个密码，这个文件加密码其实就是一个私钥。","title":"以太坊"},{"content":"类型 Solidity是静态类型的语言。\n值类型  bool int/uint fixed/unfixed address  balance和transfer send call, callcode和delegatecall   byte bytes 和 string 十六进制hex\u0026quot;0012\u0026quot; enum function  引用类型   数组\nuint[]\n  结构体\nstruct\n  映射\nmapping(key =\u0026gt; value)\n  单元和全局变量   以太币的单位\n在数字后面加上 wei、 finney、 szabo 或 ether。默认是wei\n  时间单位\n数字后面带有 seconds、 minutes、 hours、 days、 weeks 和 years。默认是秒。\n  区块和交易\n block.blockhash(uint blockNumber) returns (bytes32)：指定区块的区块哈希。 block.coinbase (address): 挖出当前区块的矿工地址 block.difficulty (uint): 当前区块难度 block.gaslimit (uint): 当前区块 gas 限额 block.number (uint): 当前区块号 block.timestamp (uint): 自 unix epoch 起始当前区块以秒计的时间戳 gasleft() returns (uint256)：剩余的 gas msg.data (bytes): 完整的 calldata msg.gas (uint): 剩余 gas - 自 0.4.21 版本开始已经不推荐使用，由 gesleft() 代替 msg.sender (address): 消息发送者（当前调用） msg.sig (bytes4): calldata 的前 4 字节（也就是函数标识符） msg.value (uint): 随消息发送的 wei 的数量 now (uint): 目前区块时间戳（block.timestamp） tx.gasprice (uint): 交易的 gas 价格 tx.origin (address): 交易发起者（完全的调用链）    地址相关\n  \u0026lt;address\u0026gt;.balance (uint256):\n以 Wei 为单位的 地址类型 的余额。\n  \u0026lt;address\u0026gt;.transfer(uint256 amount):\n向 地址类型 发送数量为 amount 的 Wei，失败时抛出异常，发送 2300 gas 的矿工费，不可调节。\n  \u0026lt;address\u0026gt;.send(uint256 amount) returns (bool):\n向 地址类型 发送数量为 amount 的 Wei，失败时返回 false，发送 2300 gas 的矿工费用，不可调节。\n  \u0026lt;address\u0026gt;.call(...) returns (bool):\n发出低级函数 CALL，失败时返回 false，发送所有可用 gas，可调节。\n  \u0026lt;address\u0026gt;.callcode(...) returns (bool)：\n发出低级函数 CALLCODE，失败时返回 false，发送所有可用 gas，可调节。\n  \u0026lt;address\u0026gt;.delegatecall(...) returns (bool):\n发出低级函数 DELEGATECALL，失败时返回 false，发送所有可用 gas，可调节。\n    合约相关\nthis (current contract\u0026rsquo;s type):\n当前合约，可以明确转换为 地址类型。\nselfdestruct(address recipient):\n销毁合约，并把余额发送到指定 地址类型。\nsuicide(address recipient):\n与 selfdestruct 等价，但已不推荐使用。\n  控制结构 输入参数和我们常见的函数的参数相同\n输出参数必须要在returns后面，和go的类似\nfunction arithmetics(uint _a, uint _b) returns (uint o_sum, uint o_product) { o_sum = _a + _b; o_product = _a * _b; } function arithmetics(uint _a, uint _b) returns (uint , uint ) { o_sum = _a + _b; o_product = _a * _b; return o_sum, o_product; } 不能用switch和goto\n内部函数调用，就和普通的方法调用一样。\n从外部调用合约的函数，先创建一个合约实例（和类的对象一样），然后调用实例方法。\n调函数要发送wei和gas，就像下图所示：\npragma solidity ^0.4.0; contract InfoFeed { function info() public payable returns (uint ret) { return 42; } } contract Consumer { InfoFeed feed; function setFeed(address addr) public { feed = InfoFeed(addr); } function callFeed() public { feed.info.value(10).gas(800)(); } } payable 修饰符要用于修饰 info，否则，.value() 选项将不可用。\n可以通过new创建一个合约，和new出一个对象一样。\n合约结构   状态变量\n状态变量是永久存储在合约中的值，其实可以理解为类的成员变量。\n  函数\n函数是合约的可执行单元，可以理解为类的成员函数。\n  函数修饰器\n以声明的形式改良函数语义。\n  事件\n以太坊的日志工具接口。\nevent HighestBidIncreased(address bidder, uint amount); // 事件 emit HighestBidIncreased(msg.sender, msg.value); // 触发事件   结构体\n理解为数据类。\n  枚举\n  合约函数可见性修饰\n","permalink":"https://zhenfeng-zhu.github.io/posts/contract/","summary":"类型 Solidity是静态类型的语言。\n值类型  bool int/uint fixed/unfixed address  balance和transfer send call, callcode和delegatecall   byte bytes 和 string 十六进制hex\u0026quot;0012\u0026quot; enum function  引用类型   数组\nuint[]\n  结构体\nstruct\n  映射\nmapping(key =\u0026gt; value)\n  单元和全局变量   以太币的单位\n在数字后面加上 wei、 finney、 szabo 或 ether。默认是wei\n  时间单位\n数字后面带有 seconds、 minutes、 hours、 days、 weeks 和 years。默认是秒。\n  区块和交易\n block.blockhash(uint blockNumber) returns (bytes32)：指定区块的区块哈希。 block.coinbase (address): 挖出当前区块的矿工地址 block.","title":"contract"},{"content":"faas-provider是一个模板，只要实现了这个模板的接口，就可以自定义实现自己的provider。\nfaas-provider OpenFaaS官方提供了两套后台provider：\n Docker Swarm Kubernetes  这两者在部署和调用函数的时候流程图如下：\n部署一个函数\n调用一个函数\nprovider要提供的一些API有：\n List / Create / Delete 一个函数  /system/functions\n方法: GET / POST / DELETE\n 获取一个函数  /system/function/{name:[-a-zA-Z_0-9]+}\n方法: GET\n 伸缩一个函数  /system/scale-function/{name:[-a-zA-Z_0-9]+}\n方法: POST\n 调用一个函数  /function/{name:[-a-zA-Z_0-9]+}\n方法: POST\n在provider的server.go的serve方法，可以看到这个serve方法创建了几个路由，接受一个FaaSHandler对象。\n// Serve load your handlers into the correct OpenFaaS route spec. This function is blocking. func Serve(handlers *types.FaaSHandlers, config *types.FaaSConfig) { r.HandleFunc(\u0026quot;/system/functions\u0026quot;, handlers.FunctionReader).Methods(\u0026quot;GET\u0026quot;) r.HandleFunc(\u0026quot;/system/functions\u0026quot;, handlers.DeployHandler).Methods(\u0026quot;POST\u0026quot;) r.HandleFunc(\u0026quot;/system/functions\u0026quot;, handlers.DeleteHandler).Methods(\u0026quot;DELETE\u0026quot;) r.HandleFunc(\u0026quot;/system/functions\u0026quot;, handlers.UpdateHandler).Methods(\u0026quot;PUT\u0026quot;) r.HandleFunc(\u0026quot;/system/function/{name:[-a-zA-Z_0-9]+}\u0026quot;, handlers.ReplicaReader).Methods(\u0026quot;GET\u0026quot;) r.HandleFunc(\u0026quot;/system/scale-function/{name:[-a-zA-Z_0-9]+}\u0026quot;, handlers.ReplicaUpdater).Methods(\u0026quot;POST\u0026quot;) r.HandleFunc(\u0026quot;/function/{name:[-a-zA-Z_0-9]+}\u0026quot;, handlers.FunctionProxy) r.HandleFunc(\u0026quot;/function/{name:[-a-zA-Z_0-9]+}/\u0026quot;, handlers.FunctionProxy) r.HandleFunc(\u0026quot;/system/info\u0026quot;, handlers.InfoHandler).Methods(\u0026quot;GET\u0026quot;) if config.EnableHealth { r.HandleFunc(\u0026quot;/healthz\u0026quot;, handlers.Health).Methods(\u0026quot;GET\u0026quot;) } // 省略 } 因此在自定义的provider，只需实现FaaSHandlers中的几个路由处理函数即可。这几个handler是：\n// FaaSHandlers provide handlers for OpenFaaS type FaaSHandlers struct { FunctionReader http.HandlerFunc DeployHandler http.HandlerFunc DeleteHandler http.HandlerFunc ReplicaReader http.HandlerFunc FunctionProxy http.HandlerFunc ReplicaUpdater http.HandlerFunc // Optional: Update an existing function UpdateHandler http.HandlerFunc Health http.HandlerFunc InfoHandler http.HandlerFunc } 我们以官方实现的faas-netes为例，讲解一下这几个hander的实现过程。\nfaas-netes 我们看下在faas-netes的中的FaaSHandlers实现：\nbootstrapHandlers := bootTypes.FaaSHandlers{ FunctionProxy: handlers.MakeProxy(functionNamespace, cfg.ReadTimeout), DeleteHandler: handlers.MakeDeleteHandler(functionNamespace, clientset), DeployHandler: handlers.MakeDeployHandler(functionNamespace, clientset, deployConfig), FunctionReader: handlers.MakeFunctionReader(functionNamespace, clientset), ReplicaReader: handlers.MakeReplicaReader(functionNamespace, clientset), ReplicaUpdater: handlers.MakeReplicaUpdater(functionNamespace, clientset), UpdateHandler: handlers.MakeUpdateHandler(functionNamespace, clientset), Health: handlers.MakeHealthHandler(), InfoHandler: handlers.MakeInfoHandler(version.BuildVersion(), version.GitCommit), } 因为是Kubernetes上的provider实现，所以这些函数都带有一个namespace的参数。\nFunctionProxy 这里最重要的就是FunctionProxy，它主要负责调用函数。这个handler其实也是起到了一个代理转发的作用，在这个函数中，只接受get和post。调用函数只接受post和get请求\n  创建一个http的client对象\n  只处理get和post请求。\n  组装代理转发的watchdog的地址\nurl := forwardReq.ToURL(fmt.Sprintf(\u0026quot;%s.%s\u0026quot;, service, functionNamespace), watchdogPort) 所以最后请求的格式就会形如：\nhttp://函数名.namespace:监视器的端口/路径   将请求发出去\n  设置http响应的头\n  ReplicaReader和ReplicaUpdater 这两个是和副本数相关的，所以放在一起对比讲解。这两个的实现依赖于Kubernetes的客户端，获取代码如下：\nclientset, err := kubernetes.NewForConfig(config) 这个config主要满足以下几个条件就行：\nConfig{ // TODO: switch to using cluster DNS. Host: \u0026quot;https://\u0026quot; + net.JoinHostPort(host, port), BearerToken: string(token), TLSClientConfig: tlsClientConfig, } Kubernetes的所有操作都可以通过rest api来完成，这两个handler也是通过调用Kubernetes的api来做的。\nReplicaReader MakeReplicaReader函数是获取当前的副本数：\n  通过mux从路由中获取到name参数\n  调用getService方法获取副本数，getService的核心代码就一句：\nitem, err := clientset.ExtensionsV1beta1().Deployments(functionNamespace).Get(functionName, getOpts)   序列化之后，把结果返回\n  ReplicaUpdater MakeReplicaUpdater是解析从gateway传过来的post请求，调用k8s的API设置副本数。\n  从请求中取出body\n  首先获取该函数的已部署的deployment对象\n  然后将deployment的副本数量设置为应设数量，这样做的目的是为了仅仅修改副本数，而不修改别的属性。\n_, err = clientset.ExtensionsV1beta1().Deployments(functionNamespace).Update(deployment)    注：mux做路由的时候，如果成功的时候不对w做任何处理，是会默认状态码为200，空字符串。\n DeleteHandler，DeployHandler，FunctionReader和UpdateHandler 这几个都是对函数的操作，其实就是调用一下Kubernetes的API进行操作。\n这几个是核心的几句代码：\nclientset.ExtensionsV1beta1().Deployments(functionNamespace).Delete(request.FunctionName, opts) deploy := clientset.Extensions().Deployments(functionNamespace) res, err := clientset.ExtensionsV1beta1().Deployments(functionNamespace).List(listOpts) _, updateErr := clientset.CoreV1().Services(functionNamespace).Update(service) 总结 官方还提供了一个faas-swarm，其实现思路也是这样，操作swarm的api来做对容器的操作。至于如何调用一个函数，都是在函数的watchdog中实现。\n","permalink":"https://zhenfeng-zhu.github.io/posts/faas-provider/","summary":"faas-provider是一个模板，只要实现了这个模板的接口，就可以自定义实现自己的provider。\nfaas-provider OpenFaaS官方提供了两套后台provider：\n Docker Swarm Kubernetes  这两者在部署和调用函数的时候流程图如下：\n部署一个函数\n调用一个函数\nprovider要提供的一些API有：\n List / Create / Delete 一个函数  /system/functions\n方法: GET / POST / DELETE\n 获取一个函数  /system/function/{name:[-a-zA-Z_0-9]+}\n方法: GET\n 伸缩一个函数  /system/scale-function/{name:[-a-zA-Z_0-9]+}\n方法: POST\n 调用一个函数  /function/{name:[-a-zA-Z_0-9]+}\n方法: POST\n在provider的server.go的serve方法，可以看到这个serve方法创建了几个路由，接受一个FaaSHandler对象。\n// Serve load your handlers into the correct OpenFaaS route spec. This function is blocking. func Serve(handlers *types.FaaSHandlers, config *types.FaaSConfig) { r.HandleFunc(\u0026quot;/system/functions\u0026quot;, handlers.FunctionReader).Methods(\u0026quot;GET\u0026quot;) r.HandleFunc(\u0026quot;/system/functions\u0026quot;, handlers.","title":"faas-provider"},{"content":"OpenFaaS的Gateway是一个golang实现的请求转发的网关，在这个网关服务中，主要有以下几个功能：\n UI 部署函数 监控 自动伸缩  架构分析 从图中可以发现，当Gateway作为一个入口，当CLI或者web页面发来要部署或者调用一个函数的时候，Gateway会将请求转发给Provider，同时会将监控指标发给Prometheus。AlterManager会根据需求，调用API自动伸缩函数。\n源码分析 依赖 github.com/gorilla/mux github.com/nats-io/go-nats-streaming github.com/nats-io/go-nats github.com/openfaas/nats-queue-worker github.com/prometheus/client_golang mux 是一个用来执行http请求的路由和分发的第三方扩展包。\ngo-nats-streaming，go-nats，nats-queue-worker这三个依赖是异步函数的时候才会用到，在分析queue-worker的时候有说到Gateway也是一个发布者。\nclient_golang是Prometheus的客户端。\n项目结构 ├── Dockerfile ├── Dockerfile.arm64 ├── Dockerfile.armhf ├── Gopkg.lock ├── Gopkg.toml ├── README.md ├── assets ├── build.sh ├── handlers │ ├── alerthandler.go │ ├── alerthandler_test.go │ ├── asyncreport.go │ ├── baseurlresolver_test.go │ ├── basic_auth.go │ ├── basic_auth_test.go │ ├── callid_middleware.go │ ├── cors.go │ ├── cors_test.go │ ├── forwarding_proxy.go │ ├── forwarding_proxy_test.go │ ├── function_cache.go │ ├── function_cache_test.go │ ├── infohandler.go │ ├── metrics.go │ ├── queueproxy.go │ ├── scaling.go │ └── service_query.go ├── metrics │ ├── add_metrics.go │ ├── add_metrics_test.go │ ├── externalwatcher.go │ ├── metrics.go │ └── prometheus_query.go ├── plugin │ ├── external.go │ └── external_test.go ├── queue │ └── types.go ├── requests │ ├── forward_request.go │ ├── forward_request_test.go │ ├── prometheus.go │ ├── prometheus_test.go │ └── requests.go ├── server.go ├── tests │ └── integration ├── types │ ├── handler_set.go │ ├── inforequest.go │ ├── load_credentials.go │ ├── proxy_client.go │ ├── readconfig.go │ └── readconfig_test.go ├── vendor │ └── github.com └── version └── version.go Gateway的目录明显多了很多，看源码的时候，首先要找到的是main包，从main函数看起，就能很容易分析出来项目是如何运行的。\n从server.go的main函数中我们可以看到，其实有如下几个模块：\n 基本的安全验证 和函数相关的代理转发  同步函数  列出函数 部署函数 删除函数 更新函数   异步函数   Prometheus的监控 ui 自动伸缩  基本的安全验证 如果配置了开启基本安全验证，会从磁盘中读取密钥：\nvar credentials *types.BasicAuthCredentials if config.UseBasicAuth { var readErr error reader := types.ReadBasicAuthFromDisk{ SecretMountPath: config.SecretMountPath, } credentials, readErr = reader.Read() if readErr != nil { log.Panicf(readErr.Error()) } } 在Gateway的配置相关的，都会有一个read()方法，进行初始化赋值。\n如果credentials被赋值之后，就会对一些要加密的API handler进行一个修饰，被修饰的API有：\n UpdateFunction DeleteFunction DeployFunction ListFunctions ScaleFunction  if credentials != nil { faasHandlers.UpdateFunction = handlers.DecorateWithBasicAuth(faasHandlers.UpdateFunction, credentials) faasHandlers.DeleteFunction = handlers.DecorateWithBasicAuth(faasHandlers.DeleteFunction, credentials) faasHandlers.DeployFunction = handlers.DecorateWithBasicAuth(faasHandlers.DeployFunction, credentials) faasHandlers.ListFunctions = handlers.DecorateWithBasicAuth(faasHandlers.ListFunctions, credentials) faasHandlers.ScaleFunction = handlers.DecorateWithBasicAuth(faasHandlers.ScaleFunction, credentials) } 这个DecorateWithBasicAuth()方法是一个路由中间件：\n 调用mux路由的BasicAuth()，从http的header中取到用户名和密码 然后给请求头上设置一个字段WWW-Authenticate，值为Basic realm=\u0026quot;Restricted\u0026quot; 如果校验失败，则返回错误，成功的话调用next方法继续进入下一个handler。  // DecorateWithBasicAuth enforces basic auth as a middleware with given credentials func DecorateWithBasicAuth(next http.HandlerFunc, credentials *types.BasicAuthCredentials) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { user, password, ok := r.BasicAuth() w.Header().Set(\u0026quot;WWW-Authenticate\u0026quot;, `Basic realm=\u0026quot;Restricted\u0026quot;`) if !ok || !(credentials.Password == password \u0026amp;\u0026amp; user == credentials.User) { w.WriteHeader(http.StatusUnauthorized) w.Write([]byte(\u0026quot;invalid credentials\u0026quot;)) return } next.ServeHTTP(w, r) } } 代理转发 Gateway本身不做任何和部署发布函数的事情，它只是作为一个代理，把请求转发给相应的Provider去处理，所有的请求都要通过这个网关。\n同步函数转发 主要转发的API有：\n RoutelessProxy ListFunctions DeployFunction DeleteFunction UpdateFunction  faasHandlers.RoutelessProxy = handlers.MakeForwardingProxyHandler(reverseProxy, forwardingNotifiers, urlResolver) faasHandlers.ListFunctions = handlers.MakeForwardingProxyHandler(reverseProxy, forwardingNotifiers, urlResolver) faasHandlers.DeployFunction = handlers.MakeForwardingProxyHandler(reverseProxy, forwardingNotifiers, urlResolver) faasHandlers.DeleteFunction = handlers.MakeForwardingProxyHandler(reverseProxy, forwardingNotifiers, urlResolver) faasHandlers.UpdateFunction = handlers.MakeForwardingProxyHandler(reverseProxy, forwardingNotifiers, urlResolver) MakeForwardingProxyHandler()有三个参数：\n  proxy\n这是一个http的客户端，作者把这个客户端抽成一个类，然后使用该类的NewHTTPClientReverseProxy方法创建实例，这样就简化了代码，不用每次都得写一堆相同的配置。\n  notifiers\n这个其实是要打印的日志，这里是一个HTTPNotifier的接口。而在这个MakeForwardingProxyHandler中其实有两个实现类，一个是LoggingNotifier，一个是PrometheusFunctionNotifier，分别用来打印和函数http请求相关的日志以及和Prometheus监控相关的日志。\n  baseURLResolver\n这个就是Provider的url地址。\n  在这个MakeForwardingProxyHandler中主要做了三件事儿：\n  解析要转发的url\n  调用forwardRequest方法转发请求，\nforwardRequest方法的逻辑比较简单，只是把请求发出去。这里就不深入分析了。\n  打印日志\n  // MakeForwardingProxyHandler create a handler which forwards HTTP requests func MakeForwardingProxyHandler(proxy *types.HTTPClientReverseProxy, notifiers []HTTPNotifier, baseURLResolver BaseURLResolver) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { baseURL := baseURLResolver.Resolve(r) requestURL := r.URL.Path start := time.Now() statusCode, err := forwardRequest(w, r, proxy.Client, baseURL, requestURL, proxy.Timeout) seconds := time.Since(start) if err != nil { log.Printf(\u0026quot;error with upstream request to: %s, %s\\n\u0026quot;, requestURL, err.Error()) } for _, notifier := range notifiers { notifier.Notify(r.Method, requestURL, statusCode, seconds) } } } 异步函数转发 前面说过，如果是异步函数，Gateway就作为一个发布者，将函数放到队列里。MakeQueuedProxy方法就是做这件事儿的：\n 读取请求体 将X-Callback-Url参数从参数中http的header中读出来 实例化用于异步处理的Request对象 调用canQueueRequests.Queue(req)，将请求发布到队列中  // MakeQueuedProxy accepts work onto a queue func MakeQueuedProxy(metrics metrics.MetricOptions, wildcard bool, canQueueRequests queue.CanQueueRequests) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { defer r.Body.Close() body, err := ioutil.ReadAll(r.Body) // 省略错误处理代码 vars := mux.Vars(r) name := vars[\u0026quot;name\u0026quot;] callbackURLHeader := r.Header.Get(\u0026quot;X-Callback-Url\u0026quot;) var callbackURL *url.URL if len(callbackURLHeader) \u0026gt; 0 { urlVal, urlErr := url.Parse(callbackURLHeader) // 省略错误处理代码 callbackURL = urlVal } req := \u0026amp;queue.Request{ Function: name, Body: body, Method: r.Method, QueryString: r.URL.RawQuery, Header: r.Header, CallbackURL: callbackURL, } err = canQueueRequests.Queue(req) // 省略错误处理代码 w.WriteHeader(http.StatusAccepted) } } 自动伸缩 伸缩性其实有两种，一种是可以通过调用API接口，来将函数进行缩放。另外一种就是通过AlertHandler。\n自动伸缩是OpenFaaS的一大特点，触发自动伸缩主要是根据不同的指标需求。\n  根据每秒请求数来做伸缩\nOpenFaaS附带了一个自动伸缩的规则，这个规则是在AlertManager配置文件中定义。AlertManager从Prometheus中读取使用情况（每秒请求数），然后在满足一定条件时向Gateway发送警报。\n可以通过删除AlertManager，或者将部署扩展的环境变量设置为0，来禁用此方式。\n  最小/最大副本数\n通过向函数添加标签, 可以在部署时设置最小 (初始) 和最大副本数。\n com.openfaas.scale.min 默认是 1 com.openfaas.scale.max 默认是 20 com.openfaas.scale.factor 默认是 20% ，在0-100之间，这是每次扩容的时候，新增实例的百分比，若是100的话，会瞬间飙升到副本数的最大值。  com.openfaas.scale.min 和 com.openfaas.scale.max值一样的时候，可以关闭自动伸缩。\ncom.openfaas.scale.factor是0时，也会关闭自动伸缩。\n  通过内存和CPU的使用量。\n使用k8s内置的HPA，也可以触发AlertManager。\n  手动指定伸缩的值 可以从这句代码中发现，调用这个路由，转发给了provider处理。\nr.HandleFunc(\u0026quot;/system/scale-function/{name:[-a-zA-Z_0-9]+}\u0026quot;, faasHandlers.ScaleFunction).Methods(http.MethodPost) 处理AlertManager的伸缩请求 Prometheus将监控指标发给AlertManager之后，会触发AlterManager调用/system/alert接口，这个接口的handler是由handlers.MakeAlertHandler方法生成。\nMakeAlertHandler方法接收的参数是ServiceQuery。ServiceQuery是一个接口，它有两个函数，用来get或者ser最大的副本数。Gateway中实现这个接口的类是ExternalServiceQuery，这个实现类是在plugin包中，我们也可以直接定制这个实现类，用来实现满足特定条件。\n// ServiceQuery provides interface for replica querying/setting type ServiceQuery interface { GetReplicas(service string) (response ServiceQueryResponse, err error) SetReplicas(service string, count uint64) error } // ExternalServiceQuery proxies service queries to external plugin via HTTP type ExternalServiceQuery struct { URL url.URL ProxyClient http.Client } 这个ExternalServiceQuery有一个NewExternalServiceQuery方法，这个方法也是一个工厂方法，用来创建实例。这个url其实就是provider的url，proxyClient就是一个http的client对象。\n  GetReplicas方法\n从system/function/:name接口获取到函数的信息，组装一个ServiceQueryResponse对象即可。\n  SetReplicas方法\n调用system/scale-function/:name接口，设置副本数。\n  MakeAlertHandler的函数主要是从http.Request中读取body，然后反序列化成PrometheusAlert对象：\n// PrometheusAlert as produced by AlertManager type PrometheusAlert struct { Status string `json:\u0026quot;status\u0026quot;` Receiver string `json:\u0026quot;receiver\u0026quot;` Alerts []PrometheusInnerAlert `json:\u0026quot;alerts\u0026quot;` } 可以发现，这个Alerts是一个数组对象，所以可以是对多个函数进行缩放。反序列化之后，调用handleAlerts方法，而handleAlerts对Alerts进行遍历，针对每个Alerts调用了scaleService方法。scaleService才是真正处理伸缩服务的函数。\nfunc scaleService(alert requests.PrometheusInnerAlert, service ServiceQuery) error { var err error serviceName := alert.Labels.FunctionName if len(serviceName) \u0026gt; 0 { queryResponse, getErr := service.GetReplicas(serviceName) if getErr == nil { status := alert.Status newReplicas := CalculateReplicas(status, queryResponse.Replicas, uint64(queryResponse.MaxReplicas), queryResponse.MinReplicas, queryResponse.ScalingFactor) log.Printf(\u0026quot;[Scale] function=%s %d =\u0026gt; %d.\\n\u0026quot;, serviceName, queryResponse.Replicas, newReplicas) if newReplicas == queryResponse.Replicas { return nil } updateErr := service.SetReplicas(serviceName, newReplicas) if updateErr != nil { err = updateErr } } } return err } 从代码总就可以看到，scaleService做了三件事儿：\n  获取现在的副本数\n  计算新的副本数\n新副本数的计算方法是根据com.openfaas.scale.factor计算步长：\nstep := uint64((float64(maxReplicas) / 100) * float64(scalingFactor))   设置为新的副本数\n  从0增加副本到的最小值 我们在调用函数的时候，用的路由是：/function/:name。如果环境变量里有配置scale_from_zero为true，先用MakeScalingHandler()方法对proxyHandler进行一次包装。\nMakeScalingHandler接受参数主要是：\n  next：就是下一个httpHandlerFunc，中间件都会有这样一个参数\n  config：ScalingConfig的对象：\n// ScalingConfig for scaling behaviours type ScalingConfig struct { MaxPollCount uint // 查到的最大数量 FunctionPollInterval time.Duration // 函数调用时间间隔 CacheExpiry time.Duration // 缓存过期时间 ServiceQuery ServiceQuery // 外部服务调用的一个接口 }   这个MakeScalingHandler中间件主要做了如下的事情：\n 先从FunctionCache缓存中获取该函数的基本信息，从这个缓存可以拿到每个函数的副本数量。 为了加快函数的启动速度，如果缓存中可以获该得函数，且函数的副本数大于0，满足条件，return即可。 如果不满足上一步，就会调用SetReplicas方法设置副本数，并更新FunctionCache的缓存。  // MakeScalingHandler creates handler which can scale a function from // zero to 1 replica(s). func MakeScalingHandler(next http.HandlerFunc, upstream http.HandlerFunc, config ScalingConfig) http.HandlerFunc { cache := FunctionCache{ Cache: make(map[string]*FunctionMeta), Expiry: config.CacheExpiry, } return func(w http.ResponseWriter, r *http.Request) { functionName := getServiceName(r.URL.String()) if serviceQueryResponse, hit := cache.Get(functionName); hit \u0026amp;\u0026amp; serviceQueryResponse.AvailableReplicas \u0026gt; 0 { next.ServeHTTP(w, r) return } queryResponse, err := config.ServiceQuery.GetReplicas(functionName) cache.Set(functionName, queryResponse) // 省略错误处理 if queryResponse.AvailableReplicas == 0 { minReplicas := uint64(1) if queryResponse.MinReplicas \u0026gt; 0 { minReplicas = queryResponse.MinReplicas } err := config.ServiceQuery.SetReplicas(functionName, minReplicas) // 省略错误处理代码 for i := 0; i \u0026lt; int(config.MaxPollCount); i++ { queryResponse, err := config.ServiceQuery.GetReplicas(functionName) cache.Set(functionName, queryResponse) // 省略错误处理 time.Sleep(config.FunctionPollInterval) } } next.ServeHTTP(w, r) } } 监控 监控是一个定时任务，开启了一个新协程，利用go的ticker.C的间隔不停的去调用/system/functions接口。反序列化到MetricOptions对象中。\nfunc AttachExternalWatcher(endpointURL url.URL, metricsOptions MetricOptions, label string, interval time.Duration) { ticker := time.NewTicker(interval) quit := make(chan struct{}) proxyClient := // 省略创建一个http.Client对象 go func() { for { select { case \u0026lt;-ticker.C: get, _ := http.NewRequest(http.MethodGet, endpointURL.String()+\u0026quot;system/functions\u0026quot;, nil) services := []requests.Function{} res, err := proxyClient.Do(get) // 省略反序列的代码 for _, service := range services { metricsOptions.ServiceReplicasCounter. WithLabelValues(service.Name). Set(float64(service.Replicas)) } break case \u0026lt;-quit: return } } }() } UI UI的代码很简单，主要就是一些前端的代码，调用上面的讲的一些API接口即可，这里就略去不表。\n总结 Gateway是OpenFaaS最为重要的一个组件。回过头看整个项目的结构，Gateway就是一个rest转发服务，一个一个的handler，每个模块之间的耦合性不是很高，可以很容易的去拆卸，自定义实现相应的模块。\n","permalink":"https://zhenfeng-zhu.github.io/posts/gateway-reading/","summary":"OpenFaaS的Gateway是一个golang实现的请求转发的网关，在这个网关服务中，主要有以下几个功能：\n UI 部署函数 监控 自动伸缩  架构分析 从图中可以发现，当Gateway作为一个入口，当CLI或者web页面发来要部署或者调用一个函数的时候，Gateway会将请求转发给Provider，同时会将监控指标发给Prometheus。AlterManager会根据需求，调用API自动伸缩函数。\n源码分析 依赖 github.com/gorilla/mux github.com/nats-io/go-nats-streaming github.com/nats-io/go-nats github.com/openfaas/nats-queue-worker github.com/prometheus/client_golang mux 是一个用来执行http请求的路由和分发的第三方扩展包。\ngo-nats-streaming，go-nats，nats-queue-worker这三个依赖是异步函数的时候才会用到，在分析queue-worker的时候有说到Gateway也是一个发布者。\nclient_golang是Prometheus的客户端。\n项目结构 ├── Dockerfile ├── Dockerfile.arm64 ├── Dockerfile.armhf ├── Gopkg.lock ├── Gopkg.toml ├── README.md ├── assets ├── build.sh ├── handlers │ ├── alerthandler.go │ ├── alerthandler_test.go │ ├── asyncreport.go │ ├── baseurlresolver_test.go │ ├── basic_auth.go │ ├── basic_auth_test.go │ ├── callid_middleware.go │ ├── cors.go │ ├── cors_test.go │ ├── forwarding_proxy.go │ ├── forwarding_proxy_test.","title":"gateway-reading"},{"content":"市面上常见到的和Nats功能类似的消息通信系统有：\nActiveMQ（Java编写）、KafKa（Scala编写）、RabbitMq（Ruby编写）、Nats（之前是Ruby编写现已修改为Go）、Redis（C语言编写）、Kestrel（Scala编写不常用）、NSQ（Go语言编写），这些消息通信系统在Broker吞吐量方面的比较：\n可以看到NATS的吞吐量特别高， NATS原来是使用Ruby编写，可以实现每秒150k消息，后来使用Go语言重写，能够达到每秒8-11百万个消息，整个程序很小只有3M Docker image，它不支持持久化消息，如果你离线，你就不能获得消息。关于NATS的详细介绍，请参考上篇文章：NATS简介\nNATS Streaming NATS Streaming是由NATS驱动的数据流系统，也是由go语言写成的，在保证吞吐量和时延的基础上，解决了Nats消息投递一致性的问题。nats streaming可以和核心nats平台无缝嵌入，扩展和互动。\n功能 除了nats平台的一些功能，nats streaming还支持以下的：\n 增强的消息协议 消息/事件持久化 至少一次投递 发布者速率限制 每个订阅者的速率匹配/限制 可重复消费 持久订阅  使用 首先安装nats-streaming-server服务，有多种方式，这里介绍两种：\n  homebrew\n直接在命令行启动\nbrew install nats-streaming-server   go get\n这种方式可以让我们直接运行源码启动\ngo get github.com/nats-io/nats-streaming-server   启动nats-streaming-server\n有三种启动方式\n  直接启动\nnats-streaming-server   开启nats监控的启动\nnats-streaming-server -m 8222   源码方式启动\ncd $GOPATH/src/github.com/nats-io/nats-streaming-server go run nats-streaming-server.go   客户端 直接下载go的客户端\ngo get github.com/nats-io/go-nats-streaming 运行发布者\ncd $GOPATH/src/github.com/nats-io/go-nats-streaming/examples/stan-pub go run main.go foo \u0026quot;msg one\u0026quot; go run main.go foo \u0026quot;msg two\u0026quot; go run main.go foo \u0026quot;msg three\u0026quot; 如下图所示：\n运行订阅者\ncd $GOPATH/src/github.com/nats-io/go-nats-streaming/examples/stan-sub go run main.go --all -c test-cluster -id myID foo 实例 首先在本地启动nats-streaming-server，然后下面的代码展示了发布订阅的过程：\npackage main import ( \u0026quot;github.com/nats-io/go-nats-streaming\u0026quot; \u0026quot;github.com/nats-io/go-nats-streaming/pb\u0026quot; \u0026quot;log\u0026quot; \u0026quot;strconv\u0026quot; \u0026quot;time\u0026quot; ) func main() { var clusterId string = \u0026quot;test-cluster\u0026quot; var clientId string = \u0026quot;test-client\u0026quot; sc, err := stan.Connect(clusterId, clientId, stan.NatsURL(\u0026quot;nats://localhost:4222\u0026quot;)) if err != nil { log.Fatal(err) return } // 开启一个协程，不停的生产数据 go func() { m := 0 for { m++ sc.Publish(\u0026quot;foo1\u0026quot;, []byte(\u0026quot;hello message \u0026quot;+strconv.Itoa(m))) time.Sleep(time.Second) } }() // 消费数据 i := 0 mcb := func(msg *stan.Msg) { i++ log.Println(i, \u0026quot;----\u0026gt;\u0026quot;, msg.Subject, msg) } startOpt := stan.StartAt(pb.StartPosition_LastReceived) //_, err = sc.QueueSubscribe(\u0026quot;foo1\u0026quot;, \u0026quot;\u0026quot;, mcb, startOpt) // 也可以用queue subscribe _, err = sc.Subscribe(\u0026quot;foo1\u0026quot;, mcb, startOpt) if err != nil { sc.Close() log.Fatal(err) } // 创建一个channel，阻塞着 signalChan := make(chan int) \u0026lt;-signalChan } 运行结果如下：\n2018/07/30 18:04:01 2 ----\u0026gt; foo1 sequence:546 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 1\u0026quot; timestamp:1532945041825538757 2018/07/30 18:04:02 3 ----\u0026gt; foo1 sequence:547 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 2\u0026quot; timestamp:1532945042828881383 2018/07/30 18:04:03 4 ----\u0026gt; foo1 sequence:548 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 3\u0026quot; timestamp:1532945043833360222 2018/07/30 18:04:04 5 ----\u0026gt; foo1 sequence:549 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 4\u0026quot; timestamp:1532945044833810697 2018/07/30 18:04:05 6 ----\u0026gt; foo1 sequence:550 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 5\u0026quot; timestamp:1532945045838056450 2018/07/30 18:04:06 7 ----\u0026gt; foo1 sequence:551 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 6\u0026quot; timestamp:1532945046838585417 2018/07/30 18:04:07 8 ----\u0026gt; foo1 sequence:552 subject:\u0026quot;foo1\u0026quot; data:\u0026quot;hello message 7\u0026quot; timestamp:1532945047840775810 源码在：https://github.com/zhenfeng-zhu/nats-demo\n总结 NATS Streaming的高级功能类似于 Apache Kafka 的功能，但当你考虑简单性而非复杂性时前者更优。由于 NATS Streaming 相对来说是一项新技术，与 Apache Kafka 相比，它在某些领域需要改进，尤其是为负载均衡场景提供更好的解决方案。\n","permalink":"https://zhenfeng-zhu.github.io/posts/nats-streaming/","summary":"市面上常见到的和Nats功能类似的消息通信系统有：\nActiveMQ（Java编写）、KafKa（Scala编写）、RabbitMq（Ruby编写）、Nats（之前是Ruby编写现已修改为Go）、Redis（C语言编写）、Kestrel（Scala编写不常用）、NSQ（Go语言编写），这些消息通信系统在Broker吞吐量方面的比较：\n可以看到NATS的吞吐量特别高， NATS原来是使用Ruby编写，可以实现每秒150k消息，后来使用Go语言重写，能够达到每秒8-11百万个消息，整个程序很小只有3M Docker image，它不支持持久化消息，如果你离线，你就不能获得消息。关于NATS的详细介绍，请参考上篇文章：NATS简介\nNATS Streaming NATS Streaming是由NATS驱动的数据流系统，也是由go语言写成的，在保证吞吐量和时延的基础上，解决了Nats消息投递一致性的问题。nats streaming可以和核心nats平台无缝嵌入，扩展和互动。\n功能 除了nats平台的一些功能，nats streaming还支持以下的：\n 增强的消息协议 消息/事件持久化 至少一次投递 发布者速率限制 每个订阅者的速率匹配/限制 可重复消费 持久订阅  使用 首先安装nats-streaming-server服务，有多种方式，这里介绍两种：\n  homebrew\n直接在命令行启动\nbrew install nats-streaming-server   go get\n这种方式可以让我们直接运行源码启动\ngo get github.com/nats-io/nats-streaming-server   启动nats-streaming-server\n有三种启动方式\n  直接启动\nnats-streaming-server   开启nats监控的启动\nnats-streaming-server -m 8222   源码方式启动\ncd $GOPATH/src/github.com/nats-io/nats-streaming-server go run nats-streaming-server.go   客户端 直接下载go的客户端\ngo get github.com/nats-io/go-nats-streaming 运行发布者","title":"NATS streaming"},{"content":"nats是一个开源的，云原生的消息系统。Apcera，百度，西门子，VMware，HTC和爱立信等公司都有在使用。\n核心基于EventMachine开发，原理是基于消息发布订阅机制，每台服务器上的每个模块会根据自己的消息类别向MessageBus发布多个消息主题，而同时也向自己需要交互的模块，按照需要的主题订阅消息。能够达到每秒8-11百万个消息，整个程序很小只有3M Docker image，它不支持持久化消息，如果你离线，你就不能获得消息。使用nats streaming可以做到持久化，缓存等功能。\nNATS server nats提供了一个go编写的轻量级服务器。发行版包括二进制和docker镜像\nNATS clients\nnats官方提供的客户端有Go，Node，Ruby，Java，C，C＃，NGINX等。\nNATS 设计目标\n核心原则是性能，可伸缩和易用性。\n 高效 始终在线和可用 非常轻巧 支持多种质量的服务 支持各种消息传递模型和使用场景  NATS 使用场景 nats是一个简单且强大的消息系统，为支持现代云原生架构设计。由于可伸缩性的复杂性，nats旨在容易使用和实现，且能提供多种质量的服务。\n一些适用nats的场景有：\n 高吞吐量的消息分散 —— 少数的生产者需要将数据发送给很多的消费者。 寻址和发现 —— 将数据发送给特定的应用实例，设备或者用户，也可用于发现并连接到基础架构中的实例，设备或用户。 命令和控制（控制面板）—— 向程序或设备发送指令，并从程序/设备中接收状态，如SCADA，卫星遥感，物联网等。 负载均衡 —— 主要应用于程序会生成大量的请求，且可动态伸缩程序实例。 N路可扩展性 —— 通信基础架构能够充分利用go的高效并发/调度机制，以增强水平和垂直的扩展性。 位置透明 —— 程序在各个地理位置上分布者大量实例，且你无法了解到程序之间的端点配置详情，及他们所生产或消费的数据。 容错  使用nats-streaming的附加场景有：\n 从特定时间或顺序消费 持久性 有保证的消息投递  NATS消息传递模型  发布订阅 请求回复 排队  NATS的特点 nats的独特功能有：\n 纯净的pub-sub 集群模式的server 订阅者的自动裁剪 基于文本的协议 多种服务质量  最多一次投递 至少一次投递   持久 缓存  ","permalink":"https://zhenfeng-zhu.github.io/posts/nats/","summary":"nats是一个开源的，云原生的消息系统。Apcera，百度，西门子，VMware，HTC和爱立信等公司都有在使用。\n核心基于EventMachine开发，原理是基于消息发布订阅机制，每台服务器上的每个模块会根据自己的消息类别向MessageBus发布多个消息主题，而同时也向自己需要交互的模块，按照需要的主题订阅消息。能够达到每秒8-11百万个消息，整个程序很小只有3M Docker image，它不支持持久化消息，如果你离线，你就不能获得消息。使用nats streaming可以做到持久化，缓存等功能。\nNATS server nats提供了一个go编写的轻量级服务器。发行版包括二进制和docker镜像\nNATS clients\nnats官方提供的客户端有Go，Node，Ruby，Java，C，C＃，NGINX等。\nNATS 设计目标\n核心原则是性能，可伸缩和易用性。\n 高效 始终在线和可用 非常轻巧 支持多种质量的服务 支持各种消息传递模型和使用场景  NATS 使用场景 nats是一个简单且强大的消息系统，为支持现代云原生架构设计。由于可伸缩性的复杂性，nats旨在容易使用和实现，且能提供多种质量的服务。\n一些适用nats的场景有：\n 高吞吐量的消息分散 —— 少数的生产者需要将数据发送给很多的消费者。 寻址和发现 —— 将数据发送给特定的应用实例，设备或者用户，也可用于发现并连接到基础架构中的实例，设备或用户。 命令和控制（控制面板）—— 向程序或设备发送指令，并从程序/设备中接收状态，如SCADA，卫星遥感，物联网等。 负载均衡 —— 主要应用于程序会生成大量的请求，且可动态伸缩程序实例。 N路可扩展性 —— 通信基础架构能够充分利用go的高效并发/调度机制，以增强水平和垂直的扩展性。 位置透明 —— 程序在各个地理位置上分布者大量实例，且你无法了解到程序之间的端点配置详情，及他们所生产或消费的数据。 容错  使用nats-streaming的附加场景有：\n 从特定时间或顺序消费 持久性 有保证的消息投递  NATS消息传递模型  发布订阅 请求回复 排队  NATS的特点 nats的独特功能有：\n 纯净的pub-sub 集群模式的server 订阅者的自动裁剪 基于文本的协议 多种服务质量  最多一次投递 至少一次投递   持久 缓存  ","title":"nats简介"},{"content":"OpenFaaS概览  无服务器函数变得简单。\n 函数监视器  你可以通过添加函数监视器 (一个小型的Golang HTTP服务)把任何一个Docker镜像变成无服务器函数。 函数监视器是允许HTTP请求通过STDIN转发到目标进程的入口点。响应会从你应用写入STDOUT返回给调用者。  API网关/UI门户  API网关为你的函数提供外部路由，并通过Prometheus收集云原生指标。 你的API网关将会根据需求更改Docker Swarm 或 Kubernetes API中的服务副本数来实现伸缩性。 UI是允许你在浏览器中调用函数或者根据需要创建新的函数。   API网关是一个RESTful形式的微服务，你可以在这里查看Swagger文档。\n 命令行 Docker中的任何容器或者进程都可以是FaaS中的一个无服务器函数。使用FaaS CLI ，你可以快速的部署函数。\n可以从Node.js, Python, Go 或者更多的语言模板中创建新的函数。如果你无法找到一个合适的模板，甚至可以使用一个Dockerfile。\n CLI实际上是API网关的一个RESTful客户端。\n 在配置好OpenFaaS之后，你可以在这里开始学习CLI开始学习CLI\n函数示例 你可以通过 使用FaaS-CLI和其内置的模板创建新函数，也可以在Docker中使用Windows或Linux的二进制文件。\n Python示例：  import requests def handle(req): r = requests.get(req, timeout = 1) print(req +\u0026quot; =\u0026gt; \u0026quot; + str(r.status_code)) handler.py\n Node.js示例：  \u0026quot;use strict\u0026quot; module.exports = (callback, context) =\u0026gt; { callback(null, {\u0026quot;message\u0026quot;: \u0026quot;You said: \u0026quot; + context}) } handler.js\n在Github仓库中提供了一系列编程语言的其他示例函数 。\n","permalink":"https://zhenfeng-zhu.github.io/posts/overview-of-openfaas/","summary":"OpenFaaS概览  无服务器函数变得简单。\n 函数监视器  你可以通过添加函数监视器 (一个小型的Golang HTTP服务)把任何一个Docker镜像变成无服务器函数。 函数监视器是允许HTTP请求通过STDIN转发到目标进程的入口点。响应会从你应用写入STDOUT返回给调用者。  API网关/UI门户  API网关为你的函数提供外部路由，并通过Prometheus收集云原生指标。 你的API网关将会根据需求更改Docker Swarm 或 Kubernetes API中的服务副本数来实现伸缩性。 UI是允许你在浏览器中调用函数或者根据需要创建新的函数。   API网关是一个RESTful形式的微服务，你可以在这里查看Swagger文档。\n 命令行 Docker中的任何容器或者进程都可以是FaaS中的一个无服务器函数。使用FaaS CLI ，你可以快速的部署函数。\n可以从Node.js, Python, Go 或者更多的语言模板中创建新的函数。如果你无法找到一个合适的模板，甚至可以使用一个Dockerfile。\n CLI实际上是API网关的一个RESTful客户端。\n 在配置好OpenFaaS之后，你可以在这里开始学习CLI开始学习CLI\n函数示例 你可以通过 使用FaaS-CLI和其内置的模板创建新函数，也可以在Docker中使用Windows或Linux的二进制文件。\n Python示例：  import requests def handle(req): r = requests.get(req, timeout = 1) print(req +\u0026quot; =\u0026gt; \u0026quot; + str(r.status_code)) handler.py\n Node.js示例：  \u0026quot;use strict\u0026quot; module.exports = (callback, context) =\u0026gt; { callback(null, {\u0026quot;message\u0026quot;: \u0026quot;You said: \u0026quot; + context}) } handler.","title":"overview-of-openfaas"},{"content":"这是一篇关于如何在Rancher 2.0上创建OpenFaaS栈的文章。我假设你已经准备好了Rancher 2.0集群，如果没有请按照官方文档创建一个。\n下面的视频展示了如何创建OpenFaaS栈，并在实际中使用：\nhttps://www.youtube.com/watch?v=kX8mXv5d1qg\u0026amp;feature=youtu.be\n这里是创建栈的compose.yml文件：\nversion: \u0026#34;2\u0026#34; services: alertmanager: image: functions/alertmanager:latest labels: io.rancher.container.pull_image: always stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 faas-rancher: environment: - CATTLE_URL=${CATTLE_URL} - CATTLE_ACCESS_KEY=${CATTLE_ACCESS_KEY} - CATTLE_SECRET_KEY=${CATTLE_SECRET_KEY} - FUNCTION_STACK_NAME=faas-functions image: kenfdev/faas-rancher:v3 labels: io.rancher.container.pull_image: always stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 gateway: environment: - functions_provider_url=http://faas-rancher:8080/ image: functions/gateway:0.6.6-beta1 labels: io.rancher.container.pull_image: always ports: - 8080:8080/tcp stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 prometheus: command: [-config.file=/etc/prometheus/prometheus.yml, -storage.local.path=/prometheus, -storage.local.memory-chunks=10000, \u0026#39;--alertmanager.url=http://alertmanager:9093\u0026#39;] image: kenfdev/prometheus:latest-cattle labels: io.rancher.container.pull_image: always stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 我在Rancher 2.0中找到一个比较酷的点是compose.yml文件中的变量都可以在UI中进行配置，如下图所示：\n新的faas-rancher项目已经转换为使用Rancher的v3版本的API，而且基本上已经通过了测试。欢迎贡献和反馈。\n","permalink":"https://zhenfeng-zhu.github.io/posts/openfaas-on-rancher/","summary":"这是一篇关于如何在Rancher 2.0上创建OpenFaaS栈的文章。我假设你已经准备好了Rancher 2.0集群，如果没有请按照官方文档创建一个。\n下面的视频展示了如何创建OpenFaaS栈，并在实际中使用：\nhttps://www.youtube.com/watch?v=kX8mXv5d1qg\u0026amp;feature=youtu.be\n这里是创建栈的compose.yml文件：\nversion: \u0026#34;2\u0026#34; services: alertmanager: image: functions/alertmanager:latest labels: io.rancher.container.pull_image: always stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 faas-rancher: environment: - CATTLE_URL=${CATTLE_URL} - CATTLE_ACCESS_KEY=${CATTLE_ACCESS_KEY} - CATTLE_SECRET_KEY=${CATTLE_SECRET_KEY} - FUNCTION_STACK_NAME=faas-functions image: kenfdev/faas-rancher:v3 labels: io.rancher.container.pull_image: always stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 gateway: environment: - functions_provider_url=http://faas-rancher:8080/ image: functions/gateway:0.6.6-beta1 labels: io.rancher.container.pull_image: always ports: - 8080:8080/tcp stop_signal: SIGTERM restart: always stdin_open: true tty: true scale: 1 prometheus: command: [-config.","title":"OpenFaaS on Rancher 2.0"},{"content":"Lab 4 - 深入函数 在开始本实验之前，创建一个新的文件夹，把 lab3 的文件拷贝到 lab4 里：\n$ cp -r lab3 lab4 \\ \u0026amp;\u0026amp; cd lab4 通过环境变量注入配置 It is useful to be able to control how a function behaves at runtime, we can do that in at least two ways:\n控制函数在运行时的行为很有用，我们至少可以通过两种方式来实现：\n在部署时  在部署时设置环境变量  我们在 Lab3 时用了 write_debug 来做——你也可以在这里设置你想要的任何自定义的环境变量。例如：如果你想为 hello world 函数配置一种语言，可以引入一个 spoken_language 变量。\n使用 HTTP 上下文——querystring / headers  使用 querystring 和 HTTP headers  另一个更为动态的选项是可以在每个请求级别上进行修改，即使用 querystrings 和 HTTP headers，这两者都可以通过 faas-cli 或者 curl 传递。\n这些 headers 通过环境变量暴露出来，因此你可以很容易的在函数中使用。所有的 header 都以 Http*为前缀，并且所有的-都被替换为了*下划线\n让我们用一个 querystring 和一个列出所有环境变量的函数来尝试一下：\n 部署一个函数，此函数使用内置的 BusyBox 命令来打印环境变量  $ faas-cli deploy --name env --fprocess=\u0026quot;env\u0026quot; --image=\u0026quot;functions/alpine:latest\u0026quot; --network=func_functions  用一个 querystring 去调用函数：  $ echo \u0026quot;\u0026quot; | faas-cli invoke env --query workshop=1 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=05e8db360c5a fprocess=env HOME=/root Http_Connection=close Http_Content_Type=text/plain Http_X_Call_Id=cdbed396-a20a-43fe-9123-1d5a122c976d Http_X_Forwarded_For=10.255.0.2 Http_X_Start_Time=1519729562486546741 Http_User_Agent=Go-http-client/1.1 Http_Accept_Encoding=gzip Http_Method=POST Http_ContentLength=-1 Http_Path=/function/env ... Http_Query=workshop=1 ... 在 python 代码中，你应该输入 os.getenv(\u0026ldquo;Http_Query\u0026rdquo;)\n 现在用一个 header 调用函数：  $ echo \u0026quot;\u0026quot; | curl http://127.0.0.1:8080/function/env --header \u0026quot;X-Output-Mode: json\u0026quot; PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=05e8db360c5a fprocess=env HOME=/root Http_X_Call_Id=8e597bcf-614f-4ca5-8f2e-f345d660db5e Http_X_Forwarded_For=10.255.0.2 Http_X_Start_Time=1519729577415481886 Http_Accept=*/* Http_Accept_Encoding=gzip Http_Connection=close Http_User_Agent=curl/7.55.1 Http_Method=GET Http_ContentLength=0 Http_Path=/function/env ... Http_X_Output_Mode=json ... 在 python 代码中，你应该输入 os.getenv(\u0026ldquo;Http_X_Outout_Mode\u0026rdquo;)。\n你可以看到，当 Http_Method 是 POST 方法时， 所有的其他 HTTP 上下文也被提供了出来，比如 Content-Length，User_Agent， Cookies 和其他 HTTP 请求中应有的参数。\n利用日志 OpenFaaS 的 watchdog 是通过标准 IO 流的 stdin 和 stdout 处理 HTTP 请求和读取 HTTP 响应。这意味着作为一个函数的进程不需要知道任何 web 和 HTTP 的信息。\n一个有趣的情况是当函数以非零的退出且 stderr 不为空。默认情况下，函数的 stdout/stderr 被合并了，且 stderr 不会被打印到日志里。\n让我们使用 Lab3 的函数 hello-openfaas 验证一下。\n修改 handler.py 为：\nimport sys import json def handle(req): sys.stderr.write(\u0026quot;This should be an error message.\\n\u0026quot;) return json.dumps({\u0026quot;Hello\u0026quot;: \u0026quot;OpenFaaS\u0026quot;}) 构建并部署：\n$ faas-cli build -f hello-openfaas.yml \\  \u0026amp;\u0026amp; faas-cli push -f hello-openfaas.yml \\  \u0026amp;\u0026amp; faas-cli deploy -f hello-openfaas.yml 然后调用函数：\n$ echo | faas-cli invoke hello-openfaas 你应该可以看到合并之后的输出：\nThis should be an error message. {\u0026quot;Hello\u0026quot;: \u0026quot;OpenFaaS\u0026quot;}  说明：如果使用docker service logs hello-openfaas检查容器的日志，你应该看不到 stderr 输出。\n 在这个例子中，我们需要函数返回一个可被解析的合法 JSON。不幸的是日志信息使得输出不可用，所以我们需要重定向 stderr 的信息到容器的日志中。OpenFaaS 提供了一个解决方案即：只返回 stdout，因此你可以打印日志的错误信息并且保证函数的返回是清晰的。\n为了达到目的，你应该使用combine_output参数：\n让我们尝试一下。打开hello-openfaas.yaml文件，然后添加下面这几行：\nenvironment: combine_output: false 推送部署并调用函数。\n输出应该是：\n{\u0026quot;Hello\u0026quot;: \u0026quot;OpenFaaS\u0026quot;} 检查容易的 stderr 日志。你应该可以看到如下类似的消息：\nhello-openfaas.1.2xtrr2ckkkth@linuxkit-025000000001 | 2018/04/03 08:35:24 stderr: This should be an error message. 创建工作流程 在某些情况下，将一个函数的输出作为另一个函数的输入是很有用的。通过客户端或者 API 网关都可以实现。\n在客户端的函数链 你可以使用 curl，faas-cli 或者你自己的其他代码将一个函数的结果传给另一个函数。这是一个例子：\n优点：\n requires no code - can be done with CLI programs 无需代码——可以使用 CLI 程序完成 fast for development and testing 快速开发测试 easy to model in code 易于在代码中建模  缺点：\n additional latency - each function goes back to the server 额外的延迟——每个函数都要返回到服务器 chatty (more messages) 繁琐（更多的消息）  例子：\n  从函数商店中部署一个 Nodeinfo 函数\n  把 NodeInfo 的输出传给 Markdown。\n  $ echo -n \u0026#34;\u0026#34; | faas-cli invoke nodeinfo | faas-cli invoke func_markdown \u0026lt;p\u0026gt;Hostname: 64767782518c\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Platform: linux Arch: x64 CPU count: 4 Uptime: 1121466\u0026lt;/p\u0026gt; 你现在将会看到 NodeInfo 函数的输出被 HTML 标签装饰了。\n客户端函数链的另一个例子是生成一个图片，然后把它传给另一个加水印的函数。\n从一个函数中调用另一个函数 最简单的函数间调用是通过 OpenFaaS 的 API 网关做一个 HTTP 调用。这个调用是不用知道外部域名或 IP 地址，他可以简单通过 DNS 条目把 API 网关作为一个 gateway。\n在一个函数中访问 API 网关服务时，最好使用环境变量来配置主机名，这是很重要的，原因有两个——名字可能会改变，并且在 Kubernetes 中有时需要后缀\n优点：\n  函数可以直接使用对方\n  因为是在同一网络中互相访问，延迟较低\n  缺点：\n 需要一个 HTTP 请求的库  例子：\n在 Lab3 中，我们介绍了 requests 包并且使用它去调用 ISS 获取一个宇航员的名字。我们可以使用相同的技术调用部署在 OpenFaaS 中的其他函数。\n 打开函数商店然后部署Sentiment Analysis函数。  Sentiment Analysis 函数将会告诉你任意一个句子的主动性和倾向（积极性评级）。这个函数的结果是一个格式化的 JSON，如下面例子所示：\n$ echo -n \u0026#34;California is great, it\u0026#39;s always sunny there.\u0026#34; | faas-cli invoke sentimentanalysis {\u0026#34;polarity\u0026#34;: 0.8, \u0026#34;sentence_count\u0026#34;: 1, \u0026#34;subjectivity\u0026#34;: 0.75} 这个结果显示我们的测试句子非常有主动性（75%）且很积极（80%）。这两个字段的值在-1.00 和 1.00 之间。\n下面的代码被用于在任何一个函数中调用 Sentiment Analysis函数：\n test_sentence = \u0026quot;California is great, it's always sunny there.\u0026quot; r = requests.get(\u0026quot;http://gateway:8080/function/sentimentanalysis\u0026quot;, text= test_sentence) 或者通过一个环境变量：\n gateway_hostname = os.getenv(\u0026quot;gateway_hostname\u0026quot;, \u0026quot;gateway\u0026quot;) # uses a default of \u0026quot;gateway\u0026quot; for when \u0026quot;gateway_hostname\u0026quot; is not set test_sentence = \u0026quot;California is great, it's always sunny there.\u0026quot; r = requests.get(\u0026quot;http://\u0026quot; + gateway_hostname + \u0026quot;:8080/function/sentimentanalysis\u0026quot;, text= test_sentence) 因为结果总是 JSON 格式的，所示我们可以使用.json()转化响应。\n result = r.json() if result[\u0026quot;polarity\u0026quot;] \u0026gt; 0.45: return \u0026quot;That was probably positive\u0026quot; else: return \u0026quot;That was neutral or negative\u0026quot; 现在创建一个 Python 函数，然后合并在一起：\nimport os import requests import sys def handle(req): \u0026quot;\u0026quot;\u0026quot;handle a request to the function Args: req (str): request body \u0026quot;\u0026quot;\u0026quot; gateway_hostname = os.getenv(\u0026quot;gateway_hostname\u0026quot;, \u0026quot;gateway\u0026quot;) # uses a default of \u0026quot;gateway\u0026quot; for when \u0026quot;gateway_hostname\u0026quot; is not set test_sentence = req r = requests.get(\u0026quot;http://\u0026quot; + gateway_hostname + \u0026quot;:8080/function/sentimentanalysis\u0026quot;, data= test_sentence) if r.status_code != 200: sys.exit(\u0026quot;Error with sentimentanalysis, expected: %d, got: %d\\n\u0026quot; % (200, r.status_code)) result = r.json() if result[\u0026quot;polarity\u0026quot;] \u0026gt; 0.45: return \u0026quot;That was probably positive\u0026quot; else: return \u0026quot;That was neutral or negative\u0026quot;  记得把 requests 添加到 requirements.txt 文件里  说明：你不需要修改 SentimentAnalysis 函数的源码，我们已经把它部署了，可以通过 API 网关获取。\n现在进入 Lab 5。\n","permalink":"https://zhenfeng-zhu.github.io/posts/openfaas-workshop-lab4/","summary":"Lab 4 - 深入函数 在开始本实验之前，创建一个新的文件夹，把 lab3 的文件拷贝到 lab4 里：\n$ cp -r lab3 lab4 \\ \u0026amp;\u0026amp; cd lab4 通过环境变量注入配置 It is useful to be able to control how a function behaves at runtime, we can do that in at least two ways:\n控制函数在运行时的行为很有用，我们至少可以通过两种方式来实现：\n在部署时  在部署时设置环境变量  我们在 Lab3 时用了 write_debug 来做——你也可以在这里设置你想要的任何自定义的环境变量。例如：如果你想为 hello world 函数配置一种语言，可以引入一个 spoken_language 变量。\n使用 HTTP 上下文——querystring / headers  使用 querystring 和 HTTP headers  另一个更为动态的选项是可以在每个请求级别上进行修改，即使用 querystrings 和 HTTP headers，这两者都可以通过 faas-cli 或者 curl 传递。","title":"openfaas-workshop-lab4"},{"content":"sudo chown \u0026quot;$USER\u0026quot;:\u0026quot;$USER\u0026quot; /home/\u0026quot;$USER\u0026quot;/.docker -R sudo chmod g+rwx \u0026quot;/home/$USER/.docker\u0026quot; -R https://api.finochat.com/api/v1/platform/apps/RETAIL/profiles/@custom:finolabs.com.cn/avatar?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmY2lkIjoiQHBjdXN0b20tMTA6Zmlub2xhYnMuY29tLmNuIiwiaXNzIjoieUNkNXVhaWRhWU4zc1pwTTdHU2V5WWVqSGdlN3hSa1EiLCJpYXQiOjE1MzAyNjk5NDh9.UUsO2xw1f8cA6FiG1bNAGyYQh-vh32hKHKSJ2EKZicI http://localhost:3000/api/v1/platform/apps/RETAIL/profiles/@custom:finolabs.com.cn/avatar?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmY2lkIjoiQHBjdXN0b20tMTA6Zmlub2xhYnMuY29tLmNuIiwiaXNzIjoieUNkNXVhaWRhWU4zc1pwTTdHU2V5WWVqSGdlN3hSa1EiLCJpYXQiOjE1MzAyNjk5NDh9.UUsO2xw1f8cA6FiG1bNAGyYQh-vh32hKHKSJ2EKZicI ","permalink":"https://zhenfeng-zhu.github.io/posts/ubuntu-docker-sudo/","summary":"sudo chown \u0026quot;$USER\u0026quot;:\u0026quot;$USER\u0026quot; /home/\u0026quot;$USER\u0026quot;/.docker -R sudo chmod g+rwx \u0026quot;/home/$USER/.docker\u0026quot; -R https://api.finochat.com/api/v1/platform/apps/RETAIL/profiles/@custom:finolabs.com.cn/avatar?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmY2lkIjoiQHBjdXN0b20tMTA6Zmlub2xhYnMuY29tLmNuIiwiaXNzIjoieUNkNXVhaWRhWU4zc1pwTTdHU2V5WWVqSGdlN3hSa1EiLCJpYXQiOjE1MzAyNjk5NDh9.UUsO2xw1f8cA6FiG1bNAGyYQh-vh32hKHKSJ2EKZicI http://localhost:3000/api/v1/platform/apps/RETAIL/profiles/@custom:finolabs.com.cn/avatar?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJmY2lkIjoiQHBjdXN0b20tMTA6Zmlub2xhYnMuY29tLmNuIiwiaXNzIjoieUNkNXVhaWRhWU4zc1pwTTdHU2V5WWVqSGdlN3hSa1EiLCJpYXQiOjE1MzAyNjk5NDh9.UUsO2xw1f8cA6FiG1bNAGyYQh-vh32hKHKSJ2EKZicI ","title":"ubuntu-docker-sudo"},{"content":"Lab3 - 函数的介绍 在开始本实验之前，创建一个新文件夹：\n$ mkdir -p lab3 \\ \u0026amp;\u0026amp; cd lab3 创建一个新函数 创建函数有两种方式：\n 使用内置的或社区提供的代码末班创建一个函数脚手架（默认）   将现有的二进制文件作为函数（高级）  生成一个新函数 在使用模板创建一个新函数之前，首先确认你已经从 Github 上拉下来模板文件：\n$ faas-cli template pull Fetch templates from repository: https://github.com/openfaas/templates.git Attempting to expand templates from https://github.com/openfaas/templates.git Fetched 11 template(s) : [csharp dockerfile go go-armhf node node-arm64 node-armhf python python-armhf python3 ruby] 之后找到可用的语言：\n$ faas-cli new --list Languages available as templates: - csharp - dockerfile - go - go-armhf - node - node-arm64 - node-armhf - python - python-armhf - python3 - ruby Or alternatively create a folder containing a Dockerfile, then pick the \u0026quot;Dockerfile\u0026quot; lang type in your YAML file. 此时你可以创建 Python, Python 3, Ruby, Go, Node, CSharp 等函数。\n 关于我们例子的说明  在这个 workshop 中的所有例子都已经被 OpenFaas 社区使用 Python3 测试通过，应该也兼容 Python2.7。\n如果你倾向于 Python2.7 而不是 Python3，使用faas-cli new --lang python命令替换faas-cli new --lang python3。\nPython 里的 Hello World 我们将会用 Python 创建一个 hello-world 函数，然后也会学习附加的依赖项。\n 函数脚手架  $ faas-cli new --lang python3 hello-openfaas --prefix=\u0026quot;\u0026lt;your-docker-username-here\u0026gt;\u0026quot; —prefix 参数将会把hello-openfaas.yml文件里image: 变量中的 prefix 更新为你的 docker hub 的账户。OpenFaas 的镜像image: functions/hello-openfaas里的参数将会是--prefix=\u0026quot;functions\u0026quot;。\n如果你不想指定 prefix，那么在创建函数之后可以通过修改 YAML 文件。\n这将会在文件夹中创建 3 个文件：\n./hello-openfaas.yml ./hello-openfaas ./hello-openfaas/handler.py ./hello-openfaas/requirements.txt YAML(.yml)文件用于配置 CLI 的构建，推送和部署。\n 说明：无论何时你需要在 Kubernetes 或远程 OpenFaaS 实例部署一个函数时，都必须在你构建之后。\n在这个例子中，你也可以通过设置一个环境变量：export OPENFAAS_URL=127.0.0.1:31112覆盖掉默认的网关 URL：127.0.0.1：8080，\n 如下是 YAML 文件的内容：\nprovider: name: faas gateway: http://127.0.0.1:8080 functions: hello-openfaas: lang: python3 handler: ./hello-openfaas image: hello-openfaas  函数的名字是在functions的下面，如：hello-openfaas    语言是放在 lang 之下\n  handler 是用于构建的文件夹，一定得是文件夹，不能是文件\n  docker 的镜像名是放在 image 之下\n  请记住，网关的 URL 是可以在 YAML 文件中 provider 下的 gateway 字段覆写，或者是在 CLI 中使用—gateway 参数或是环境变量的的 OPENFAAS_URL。\n如下是handler.py文件的内容：\ndef handle(req): \u0026quot;\u0026quot;\u0026quot;handle a request to the function Args: req (str): request body \u0026quot;\u0026quot;\u0026quot; return req 这个函数返回输入的值，和 echo 函数很像。\n将返回值修改为Hello OpenFaaS，如：\n return \u0026quot;Hello OpenFaaS\u0026quot; 任何返回给标准输出的值随后都会被返回给调用的程序。或者说是一个 print()语句和显示的结果和调用的程序流程相似。\n这是本地的开发流程：\n$ faas-cli build -f ./hello-openfaas.yml $ faas-cli push -f ./hello-openfaas.yml $ faas-cli deploy -f ./hello-openfaas.yml 接下来通过 UI，CLI，curk 或者其他的程序去调用函数。\n这些函数都会被分配一个路由，比如：\nhttp://127.0.0.1:8080/function/\u0026lt;function_name\u0026gt; http://127.0.0.1:8080/function/figlet http://127.0.0.1:8080/function/hello-openfaas  专家提醒：如果你重命名了 YAML 文件为 stack.yml，不再需要是用-f 参数。\n 函数只能被 get 或 post 方法调用。\n 调用你的函数  使用 faas-cli invoke 测试函数，更多的命令请参考 faas-cli invoke \u0026ndash;help\n例子：发现宇航员 我们将会创建一个叫 astronaut-finder 函数，这个函数会从国际空间站上拉取一个随机的宇航员的名字。\n$ faas-cli new --lang python3 astronaut-finder --prefix=\u0026quot;\u0026lt;your-docker-username-here\u0026gt;\u0026quot; 这会为我们创建三个文件：\n./astronaut-finder/handler.py 该函数的 handler —— 你会得到一个原始的请求 req 对象，而且会在控制台上打印结果。\n./astronaut-finder/requirements.txt 在这个文件中列出你想要安装的 pip 模块，比如 requests 或 urllib\n./astronaut-finder.yml 这个文件用于管理函数——在里面有函数名，docker 镜像和其他自定义的字段。\n Edit ./astronaut-finder/requirements.txt 修改./astronaut-finder/requirements.txt  requests 这告诉函数，他需要使用一个名叫 requests 的第三方包用于使用 http 请求网站。\n 写函数代码：  我们将会从 http://api.open-notify.org/astros.json拉取数据\n这是返回结果的例子：\n{ \u0026quot;number\u0026quot;: 6, \u0026quot;people\u0026quot;: [ { \u0026quot;craft\u0026quot;: \u0026quot;ISS\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Alexander Misurkin\u0026quot; }, { \u0026quot;craft\u0026quot;: \u0026quot;ISS\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Mark Vande Hei\u0026quot; }, { \u0026quot;craft\u0026quot;: \u0026quot;ISS\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Joe Acaba\u0026quot; }, { \u0026quot;craft\u0026quot;: \u0026quot;ISS\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Anton Shkaplerov\u0026quot; }, { \u0026quot;craft\u0026quot;: \u0026quot;ISS\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Scott Tingle\u0026quot; }, { \u0026quot;craft\u0026quot;: \u0026quot;ISS\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;Norishige Kanai\u0026quot; } ], \u0026quot;message\u0026quot;: \u0026quot;success\u0026quot; } 更新 handler.py：\nimport requests import random def handle(req): r = requests.get(\u0026quot;http://api.open-notify.org/astros.json\u0026quot;) result = r.json() index = random.randint(0, len(result[\u0026quot;people\u0026quot;])-1) name = result[\u0026quot;people\u0026quot;][index][\u0026quot;name\u0026quot;] return \u0026quot;%s is in space\u0026quot; % (name)  备注：在这个例子中我们不需要使用 req 参数，但是必须确保他在函数头里。\n 现在构建函数：\n$ faas-cli build -f ./astronaut-finder.yml  提示：试着将 astronaut-finder.yml 重命名为 stack.yml，然后就可以使用 faas-cli build。\nstack.yml 是 CLI 的默认命名。\n 部署函数：\n$ faas-cli deploy -f ./astronaut-finder.yml 调用函数\n$ echo | faas-cli invoke astronaut-finder Anton Shkaplerov is in space $ echo | faas-cli invoke astronaut-finder Joe Acaba is in space 故障排除：找到容器的日志 通过容器的日志你可以找到函数每次调用的高级别的信息：\n$ docker service logs -f astronaut-finder astronaut-finder.1.1e1ujtsijf6b@nuc | 2018/02/21 14:53:25 Forking fprocess. astronaut-finder.1.1e1ujtsijf6b@nuc | 2018/02/21 14:53:26 Wrote 18 Bytes - Duration: 0.063269 seconds 故障排除：使用 write_debug 详细输出 让我们打开函数的详细输出。为了不让函数的日志和数据泛滥，这个功能默认是关闭的，在日志里有很多没有意义的二进制数据时，这点儿尤为重要。\n这是标准的 YAML 配置：\nprovider: name: faas gateway: http://127.0.0.1:8080 functions: astronaut-finder: lang: python3 handler: ./astronaut-finder image: astronaut-finder 编辑 YAML 文件，添加 environment。\nastronaut-finder: lang: python3 handler: ./astronaut-finder image: astronaut-finder environment: write_debug: true 现在再次使用faas-cli deploy -f ./astronaut-finder.yml部署。\n调用函数，然后观察函数的返回：\n$ docker service logs -f astronaut-finder astronaut-finder.1.1e1ujtsijf6b@nuc | 2018/02/21 14:53:25 Forking fprocess. astronaut-finder.1.szobw9pt3m60@nuc | 2018/02/26 14:49:57 Query astronaut-finder.1.szobw9pt3m60@nuc | 2018/02/26 14:49:57 Path /function/hello-openfaas astronaut-finder.1.1e1ujtsijf6b@nuc | 2018/02/21 14:53:26 Hello World astronaut-finder.1.1e1ujtsijf6b@nuc | 2018/02/21 14:53:26 Duration: 0.063269 seconds 管理多个函数 CLI 的 YAML 文件也可以把函数打组为一个 stack 里，当在几个相关联的函数工作时是很有用的。\n看一下如何生成两个函数：\n$ faas-cli new --lang python3 first 第二个函数使用\u0026ndash;apend 标志：\n$ faas-cli new --lang python3 second --append=./first.yml 为了方便我们把 first.yml 重命名为 example.yml。\n$ mv first.yml example.yml 现在看一下文件：\nprovider: name: faas gateway: http://127.0.0.1:8080 functions: first: lang: python3 handler: ./first image: first second: lang: python3 handler: ./second image: second 当一个函数栈工作时，这里有几个有用的标志。\n 并行构建  $ faas-cli build -f ./example.yml --parallel=2  只构建或者推送一个函数  $ faas-cli build -f ./example.yml --filter=second 更多信息请参考faas-cli build --help 和 faas-cli push --help 。\n 专家提醒：如果你不想传-f 参数，faas-cli 将会自动寻找 stack.yml 文件。\n 你也可以在使用faas-cli -f https://....将函数栈部署在 https 上。\n使用自定义模板 如果你有自己的语言模板或者是从社区中找到的模板，比如 PHP。你可以使用下面的命令添加进来：\n$ faas-cli template pull https://github.com/itscaro/openfaas-template-php ... $ faas-cli new --list | grep php - php - php5 社区模板列表维护在OpenFaaS CLI README页面。\n继续可选的练习或者进入 Lab 4。\n自定义二进制文件为函数（可选） 自定义二进制文件或者容器也可以被作为函数，但是大部分时间里使用语言模板文件应该可以涵盖大多数情况。\n通过使用 dockerfile 语言，可以用自定义二进制或者 Dockerfile 创建一个新函数函数：\n$ faas-cli new --lang dockerfile sorter --prefix=\u0026quot;\u0026lt;your-docker-username-here\u0026gt;\u0026quot; 你将会看到 sorter 文件夹和 sorter.yml 文件被创建。\n编辑 sorter/Docerfile，更新设置 fprocess 的那一行。让我们改变他为内置 bash 的 sort 命令。我们可以用这个命令对字符串按照字母数字顺序排序。\nENV fprocess=\u0026quot;sort\u0026quot; 构建，推送和部署函数：\n$ faas-cli build -f sorter.yml \\ \u0026amp;\u0026amp; faas-cli push -f sorter.yml \\ \u0026amp;\u0026amp; faas-cli deploy -f sorter.yml 从 UI 或者 CLI 中调用函数：\n$ echo -n ' elephant zebra horse ardvark monkey'| faas-cli invoke sorter ardvark elephant horse monkey zebra 在这个例子中我们使用内置于 BusyBox 的 sort 命令。也有其他有用的命令，比如 sha512sum，甚至可以是 bash 或者 shell 脚本，而且并不限制于这些命令。任何二进制或者存在的容器都可以通过添加到 OpenFaaS 的函数 watchdog 中来将它 serverless 化。\n 提示：你知道 OpenFaaS 也支持 WIndows 的二进制文件吗？比如 C#，VB 或者 PowerShell？\n 现在进入 Lab 4\n","permalink":"https://zhenfeng-zhu.github.io/posts/openfaas-workshop-lab3/","summary":"Lab3 - 函数的介绍 在开始本实验之前，创建一个新文件夹：\n$ mkdir -p lab3 \\ \u0026amp;\u0026amp; cd lab3 创建一个新函数 创建函数有两种方式：\n 使用内置的或社区提供的代码末班创建一个函数脚手架（默认）   将现有的二进制文件作为函数（高级）  生成一个新函数 在使用模板创建一个新函数之前，首先确认你已经从 Github 上拉下来模板文件：\n$ faas-cli template pull Fetch templates from repository: https://github.com/openfaas/templates.git Attempting to expand templates from https://github.com/openfaas/templates.git Fetched 11 template(s) : [csharp dockerfile go go-armhf node node-arm64 node-armhf python python-armhf python3 ruby] 之后找到可用的语言：\n$ faas-cli new --list Languages available as templates: - csharp - dockerfile - go - go-armhf - node - node-arm64 - node-armhf - python - python-armhf - python3 - ruby Or alternatively create a folder containing a Dockerfile, then pick the \u0026quot;Dockerfile\u0026quot; lang type in your YAML file.","title":"openfaas-workshop-lab3"},{"content":"Lab2 测试一下 在开始这个实验之前，首先创建一个新的文件夹：\n$ mkdir -p lab2 \\ \u0026amp;\u0026amp; cd lab2 使用UI界面 你现在可以打开http://127.0.0.1:8080测试OpenFaaS的UI。如果是在一台LInux虚拟机中部署，将127.0.0.1替换为该虚拟机的ip地址。\n 备注：我们使用的是127.0.0.1而不是localhost，因为在一些Linux发行版中IPv4/IPv6出现不兼容。\n 我们可以部署一些样例代码来进行测试：\nfaas-cli deploy -f https://raw.githubusercontent.com/openfaas/faas/master/stack.yml 你可以在UI里面选择Markdown函数进行测试，这个函数将markdown转换为html。\n在Request一栏中输入：\n## The **OpenFaaS** _workshop_ 点击INVOKE按钮，然后在屏幕下方观察响应。\n即：\n\u0026lt;h2\u0026gt;The \u0026lt;strong\u0026gt;OpenFaaS\u0026lt;/strong\u0026gt; \u0026lt;em\u0026gt;workshop\u0026lt;/em\u0026gt;\u0026lt;/h2\u0026gt; 你同时也会发现在其他栏中的值：\n Status - 函数是否可以运行的状态。如果在UI中的状态不可用，你将不能invoke函数。 Replicas - 运行在swarm集群中的函数副本。 Image - 发布在docker hub或其他docker仓库中的镜像名字。 Invocation count - 5秒更新一次，显示该函数已经被触发的次数。  点击INVOKE按钮几次，可以看到Invocation count在增加。\n通过函数商店进行部署 你也可以从OpenFaas商店中部署一个函数。这个商店是是社区整理的一些免费函数集合。\n 点击Deploy New Function 点击From Store 点击Figlet或者从搜索栏中进入figlet，然后点击Deploy  Figlet函数将会在左侧的函数列表中。稍等几分钟，等待函数从Docker Hub中下载。然后就像Markdown函数一样，输入一些文字，点击INVOKE。\n一个ASCII的logo将会生成：\n _ ___ ___ _ __ / |/ _ \\ / _ (_)/ / | | | | | | | |/ / | | |_| | |_| / /_ |_|\\___/ \\___/_/(_) 学习CLI 你现在可以测试CLI，但是首先先做一些网关URL的说明：\n如果你的网关不是部署在http://127.0.0.1:8080，那么你需要指定替代的地址。这里有几种方法：\n 设置OPENFAAS_URL的环境变量，faas-cli将会在当前shell会话中指向此endpoint。比如export OPENFAAS_URL=http://openfaas.endpoint.com:8080 使用-g或\u0026ndash;gateway指向正确的endpoint：faas deploy \u0026ndash;gateway http://openfaas.endpoint.com:8080 在部署的YAML文件中，改变provider下面的gateway的值。  列出已经部署的函数 这个命令将会列出已经部署的函数，副本数和调用数。\n$ faas-cli list 你应该可以看到markdown函数和figlet函数以及他们被调用的次数。\n现在试一下verbose参数\n$ faas-cli list --verbose 或\n$ faas-cli list -v 你现在可以看到函数的docker镜像。\n调用一个函数 选择一个faas-cli list命令列出的函数，比如markdown：\n$ faas-cli invoke markdown 现在输入一些文字，按ctrl+d退出。\n也可以使用其他命令，比如echo或者uname -a作为输入，和invoke命令形成管道：\n$ echo Hi | faas-cli invoke markdown $ uname -a | faas-cli invoke markdown 接下来你甚至可以将一个markdown文件转换为HTML文件：\n$ git clone https://github.com/openfaas/workshop \\ \u0026amp;\u0026amp; cd workshop $ cat lab2.md | faas-cli invoke markdown 监控仪表盘 OpenFaas使用Prometheus自动监控函数的指标。使用一些免费且开源的软件如 Grafana，将这些指标转换为可视化的仪表盘。\n部署OpenFaaS的Grafana：\n$ docker service create -d \\ --name=grafana \\ --publish=3000:3000 \\ --network=func_functions \\ stefanprodan/faas-grafana:4.6.3 服务创建之后，在浏览器中打开Grafana，使用admin/admin登录，然后进入预制的OpenFaaS的仪表盘：\nhttp://127.0.0.1:3000/dashboard/db/openfaas\n现在进入Lab 3\n未完待续\u0026hellip;\n","permalink":"https://zhenfeng-zhu.github.io/posts/openfaas-workshop-lab2/","summary":"Lab2 测试一下 在开始这个实验之前，首先创建一个新的文件夹：\n$ mkdir -p lab2 \\ \u0026amp;\u0026amp; cd lab2 使用UI界面 你现在可以打开http://127.0.0.1:8080测试OpenFaaS的UI。如果是在一台LInux虚拟机中部署，将127.0.0.1替换为该虚拟机的ip地址。\n 备注：我们使用的是127.0.0.1而不是localhost，因为在一些Linux发行版中IPv4/IPv6出现不兼容。\n 我们可以部署一些样例代码来进行测试：\nfaas-cli deploy -f https://raw.githubusercontent.com/openfaas/faas/master/stack.yml 你可以在UI里面选择Markdown函数进行测试，这个函数将markdown转换为html。\n在Request一栏中输入：\n## The **OpenFaaS** _workshop_ 点击INVOKE按钮，然后在屏幕下方观察响应。\n即：\n\u0026lt;h2\u0026gt;The \u0026lt;strong\u0026gt;OpenFaaS\u0026lt;/strong\u0026gt; \u0026lt;em\u0026gt;workshop\u0026lt;/em\u0026gt;\u0026lt;/h2\u0026gt; 你同时也会发现在其他栏中的值：\n Status - 函数是否可以运行的状态。如果在UI中的状态不可用，你将不能invoke函数。 Replicas - 运行在swarm集群中的函数副本。 Image - 发布在docker hub或其他docker仓库中的镜像名字。 Invocation count - 5秒更新一次，显示该函数已经被触发的次数。  点击INVOKE按钮几次，可以看到Invocation count在增加。\n通过函数商店进行部署 你也可以从OpenFaas商店中部署一个函数。这个商店是是社区整理的一些免费函数集合。\n 点击Deploy New Function 点击From Store 点击Figlet或者从搜索栏中进入figlet，然后点击Deploy  Figlet函数将会在左侧的函数列表中。稍等几分钟，等待函数从Docker Hub中下载。然后就像Markdown函数一样，输入一些文字，点击INVOKE。\n一个ASCII的logo将会生成：\n _ ___ ___ _ __ / |/ _ \\ / _ (_)/ / | | | | | | | |/ / | | |_| | |_| / /_ |_|\\___/ \\___/_/(_) 学习CLI 你现在可以测试CLI，但是首先先做一些网关URL的说明：","title":"openfaas-workshop-lab2"},{"content":"今天大多数公司在开发应用程序并将其部署在服务器上的时候，无论是选择公有云还是私有的数据中心，都需要提前了解究竟需要多少台服务器、多大容量的存储和数据库的功能等。并需要部署运行应用程序和依赖的软件到基础设施之上。假设我们不想在这些细节上花费精力，是否有一种简单的架构模型能够满足我们这种想法？这个答案已经存在，这就是今天软件架构世界中新鲜但是很热门的一个话题——Serverless（无服务器）架构。\n目前已经有一批优秀的serverless架构开源项目，OpenFaas就是其中的佼佼者。奈何其中的中文资料比较少，我也是边学边翻译，希望能够抛砖引玉，助力serverless的发展。\n这是一个自学研讨会，学习如何构建、部署和运行OpenFaas 函数。\nLab1 - OpenFaas的准备工作 OpenFaas可以在Docker Swarm和Kubernetes的过几个主要平台之上运行。在此教程里，我们将会在的您本地电脑使用Docker Swarm来入门。\n预备条件 Docker Mac\n Docker CE for Mac Edge Edition  Windows\n 仅针对windows10 专业版或企业版 安装Docker CE for Windows 安装Git Bash   备注：所有步骤中请使用Git Bash：不要尝试使用WSL或Bash for Windows。\n Linux - Ubuntu 或 Debian\n Docker CE for Linux   你可以从Docker Store中安装Docker CE\n 设置一个单节点的Docker Swarm OpenFaas在Docker Swarm和Kubernetes上工作。因为Docker Swarm很容易设置，所以在此Workshop中我们使用Docker Swarm。在文档中有他们两个的指南。\n在你的笔记本或虚拟机中设置一个单节点的Docker Swarm：\n$ docker swarm init  如果运行此命令出错，加上 \u0026ndash;advertise-addr 你的IP 参数。\n Docker Hub 注册一个Docker Hub账号。Docker Hub允许你在互联网中发布自己的Docker镜像来用于多节点集群或社区共享。在Workshop中我们使用Docker Hub发布函数。\n你可以在这里注册：Docker Hub\n 备注：Docker Hub也可以设置为自动构建镜像。\n 打开一个终端或者Git Bash窗口，然后使用上面注册的用户名登陆Docker Hub。\n$ docker login OpenFaas CLI 你可以在mac上使用brew或者在Linu和mac上使用一个集成脚本来安装OpenFaas CLI。\n在Mac或Linux上终端中输入：\n$ curl -sL cli.openfaas.com | sudo sh 对于windows平台，从releases page中下载最新的的faas-cli.exe。你可以把它放在一个local文件夹或者在C:\\Windows\\路径中，这样它就可以在命令行中使用。\n 如果你是一个高级Windows用户，把CLI放在你自定义的文件夹中，然后把此文件夹添加到环境变量。\n 我们将会使用faas-创建新函数的脚手架，build，deploy和invoke函数。你可以从faas-cli —help中找到这些命令。\n测试faas-cli\n打开一个终端或Git Bash窗口，然后输入：\n$ faas-cli help $ faas-cli version 部署OpenFaas 发布OpenFaas的说明文档修改了很多次，因为我们努力使他简单。接下来将会在60秒左右的时间使得OpenFaas部署起来。\n 首先clone项目  git clone https://github.com/openfaas/faas  然后使用git检出到最新版本  $ cd faas \u0026amp;\u0026amp; \\ git checkout master  备注：你也可以在project release page中找到最新导入release版本。\n  现在使用Docker Swarm部署stack  $ ./deploy_stack.sh 你现在应该已经把OpenFaas部署了。\n如果你现在在一个共享WIFI连接中，它将会需要几分钟时间拉取镜像并启动。\n在此屏幕上检查服务是否显示为1/1:\n$ docker service ls 如果你期间有遇到任何问题，请查阅Docker Swarm的 部署指南。\n现在进入Lab 2。\n未完待续 ","permalink":"https://zhenfeng-zhu.github.io/posts/workshop-lab1/","summary":"今天大多数公司在开发应用程序并将其部署在服务器上的时候，无论是选择公有云还是私有的数据中心，都需要提前了解究竟需要多少台服务器、多大容量的存储和数据库的功能等。并需要部署运行应用程序和依赖的软件到基础设施之上。假设我们不想在这些细节上花费精力，是否有一种简单的架构模型能够满足我们这种想法？这个答案已经存在，这就是今天软件架构世界中新鲜但是很热门的一个话题——Serverless（无服务器）架构。\n目前已经有一批优秀的serverless架构开源项目，OpenFaas就是其中的佼佼者。奈何其中的中文资料比较少，我也是边学边翻译，希望能够抛砖引玉，助力serverless的发展。\n这是一个自学研讨会，学习如何构建、部署和运行OpenFaas 函数。\nLab1 - OpenFaas的准备工作 OpenFaas可以在Docker Swarm和Kubernetes的过几个主要平台之上运行。在此教程里，我们将会在的您本地电脑使用Docker Swarm来入门。\n预备条件 Docker Mac\n Docker CE for Mac Edge Edition  Windows\n 仅针对windows10 专业版或企业版 安装Docker CE for Windows 安装Git Bash   备注：所有步骤中请使用Git Bash：不要尝试使用WSL或Bash for Windows。\n Linux - Ubuntu 或 Debian\n Docker CE for Linux   你可以从Docker Store中安装Docker CE\n 设置一个单节点的Docker Swarm OpenFaas在Docker Swarm和Kubernetes上工作。因为Docker Swarm很容易设置，所以在此Workshop中我们使用Docker Swarm。在文档中有他们两个的指南。\n在你的笔记本或虚拟机中设置一个单节点的Docker Swarm：\n$ docker swarm init  如果运行此命令出错，加上 \u0026ndash;advertise-addr 你的IP 参数。","title":"译：openfaas-workshop-Lab1"},{"content":"对于mac环境来讲，首先安装新版docker:\nbrew cask install docker 然后启动docker。\n命令行登陆docker hub\ndocker login 启动docker swarm\ndocker swarm init 安装faas-cli\nbrew install faas-cli clone下来代码：\ngit clone https://github.com/openfaas/faas 然后执行\n./deploy_stack.sh 部署一些示例\nfaas-cli deploy -f https://raw.githubusercontent.com/openfaas/faas/master/stack.yml 使用浏览器打开 http://127.0.0.1:8080 就可以看到ui界面了。\n安装grafana进行监控\ndocker service create -d \\ --name=grafana \\ --publish=3000:3000 \\ --network=func_functions \\ stefanprodan/faas-grafana:4.6.3 浏览器打开： http://127.0.0.1:3000 登陆admin admin 查看。\n常用命令：\n$ faas-cli new --list $ faas-cli build -f ./hello-openfaas.yml $ faas-cli push -f ./hello-openfaas.yml $ faas-cli deploy -f ./hello-openfaas.yml ","permalink":"https://zhenfeng-zhu.github.io/posts/openfaas/","summary":"对于mac环境来讲，首先安装新版docker:\nbrew cask install docker 然后启动docker。\n命令行登陆docker hub\ndocker login 启动docker swarm\ndocker swarm init 安装faas-cli\nbrew install faas-cli clone下来代码：\ngit clone https://github.com/openfaas/faas 然后执行\n./deploy_stack.sh 部署一些示例\nfaas-cli deploy -f https://raw.githubusercontent.com/openfaas/faas/master/stack.yml 使用浏览器打开 http://127.0.0.1:8080 就可以看到ui界面了。\n安装grafana进行监控\ndocker service create -d \\ --name=grafana \\ --publish=3000:3000 \\ --network=func_functions \\ stefanprodan/faas-grafana:4.6.3 浏览器打开： http://127.0.0.1:3000 登陆admin admin 查看。\n常用命令：\n$ faas-cli new --list $ faas-cli build -f ./hello-openfaas.yml $ faas-cli push -f ./hello-openfaas.yml $ faas-cli deploy -f .","title":"openfaas"},{"content":"Spring web mvc： 传统servlet web\nspring web flux： Reactive web\n 编程模式： non-blocking 非阻塞  nio：同步？异步？   并行模型  sync 同步 async 异步    Reactive 概念 Reactive programming： 响应式编程\nIn computing, reactive programming is a declarative programming paradigm concerned with data streams and the propagation of change. With this paradigm it is possible to express static (e.g. arrays) or dynamic (e.g. event emitters) data streams with ease, and also communicate that an inferred dependency within the associated execution model exists, which facilitates the automatic propagation of the changed data flow.\n实现框架   RxJava\nReactiveX is a library for composing asynchronous and event-based programs by using observable sequences.\n这种就是推的模式\nint[] a=[1, 2, 3] for(int i: a){ } package com.reactive.demo.reactivedemo; import java.util.Observable; /** * todo * * @author zhuzhenfeng * @date 2018/6/23 */ public class ObserverPatternDemo { public static void main(String[] args) { MyObservable observable = new MyObservable(); // 1 observable n个observer observable.addObserver((o, value) -\u0026gt; { System.out.println(\u0026quot;1 收到数据更新\u0026quot; + value); }); observable.addObserver((o, value) -\u0026gt; { System.out.println(\u0026quot;2 收到数据更新\u0026quot; + value); }); observable.setChanged(); observable.notifyObservers(\u0026quot;hello world\u0026quot;);// push data 发布数据 } public static class MyObservable extends Observable { @Override protected synchronized void setChanged() { super.setChanged(); } } } /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/bin/java \u0026quot;- 2 收到数据更新hello world 1 收到数据更新hello world Process finished with exit code 0 当时不阻塞，后续回调。非阻塞基本上采用callback的形式。\n对于java来讲，异步代表切换了线程。\n当前的实现： 同步+非阻塞\n如果是切换了线程，代表是异步 的非阻塞，一般是gui程序的。\n  Reactor\n  特性  异步 非阻塞 事件驱动 可能有背压 backpressure 防止回调地狱   Reactive 使用场景 Long Live 模式： netty的io连接（rpc） timeout\nshort live模式：不太适合Reactive web，因为这是等待。只是会快速返回，但是并不会给你真正的结果。短频快的连接，不太有用武之地。\n http http超时时间  Reactive 理解误区 web：快速响应\n200 Q-\u0026gt;200 T -\u0026gt; 50T\n1-50\nTomcat connector thread pool(200)-\u0026gt;reactive thread pool(50)\nio连接从Tomcat-\u0026gt;Reactive\n连接\nReactive thread pool（50）\n不太适合web请求。\nwebflux其实并不会提升性能。\n少量的线程，少量的内存来做更好的伸缩性，而并不是为了提升更好的性能。使用Reactive只会是使单位时间内接受请求的数量增加，单位时间内的处理请求的数量下降。\n","permalink":"https://zhenfeng-zhu.github.io/posts/java-reactive-web/","summary":"Spring web mvc： 传统servlet web\nspring web flux： Reactive web\n 编程模式： non-blocking 非阻塞  nio：同步？异步？   并行模型  sync 同步 async 异步    Reactive 概念 Reactive programming： 响应式编程\nIn computing, reactive programming is a declarative programming paradigm concerned with data streams and the propagation of change. With this paradigm it is possible to express static (e.g. arrays) or dynamic (e.g. event emitters) data streams with ease, and also communicate that an inferred dependency within the associated execution model exists, which facilitates the automatic propagation of the changed data flow.","title":"java-reactive-web"},{"content":"启动zookeeper\nbin/zookeeper-server-start.sh config/zookeeper.properties 启动kafka\nbin/kafka-server-start.sh config/server.properties 创建一个主题\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test bin/kafka-topics.sh --list --zookeeper localhost:2181 生产者\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 消费者\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning kafka connect\necho -e \u0026quot;zhisheng\\ntian\u0026quot; \u0026gt; test.txt ls\nzhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ echo -e \u0026quot;zhisheng\\ntian\u0026quot; \u0026gt; test.txt zhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ zhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ ls LICENSE NOTICE bin config libs logs site-docs test.txt zhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ 启动连接器\nbin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties 然后发现多了一个文件\n/Users/zhuzhenfeng/Documents/software/kafka_2.12-1.1.0 [kafka_2.12-1.1.0] ls 18:25:38 LICENSE NOTICE bin config libs logs site-docs test.sink.txt test.txt [kafka_2.12-1.1.0] 然后消费\n\u0026gt;\u0026gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning {\u0026quot;schema\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;,\u0026quot;optional\u0026quot;:false},\u0026quot;payload\u0026quot;:\u0026quot;zhisheng\u0026quot;} {\u0026quot;schema\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;,\u0026quot;optional\u0026quot;:false},\u0026quot;payload\u0026quot;:\u0026quot;tian\u0026quot;} {\u0026quot;schema\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;,\u0026quot;optional\u0026quot;:false},\u0026quot;payload\u0026quot;:\u0026quot;sd\u0026quot;} {\u0026quot;schema\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;,\u0026quot;optional\u0026quot;:false},\u0026quot;payload\u0026quot;:\u0026quot;sdafasdfasdfdsa\u0026quot;} ","permalink":"https://zhenfeng-zhu.github.io/posts/kafka/","summary":"启动zookeeper\nbin/zookeeper-server-start.sh config/zookeeper.properties 启动kafka\nbin/kafka-server-start.sh config/server.properties 创建一个主题\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test bin/kafka-topics.sh --list --zookeeper localhost:2181 生产者\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 消费者\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning kafka connect\necho -e \u0026quot;zhisheng\\ntian\u0026quot; \u0026gt; test.txt ls\nzhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ echo -e \u0026quot;zhisheng\\ntian\u0026quot; \u0026gt; test.txt zhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ zhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ ls LICENSE NOTICE bin config libs logs site-docs test.txt zhuzhenfengdeMacBook-Pro➜ kafka_2.12-1.1.0 ᐅ 启动连接器\nbin/connect-standalone.sh config/connect-standalone.","title":"kafka"},{"content":" 尝试翻译vertx的文档。尊重原文，部分使用自己的理解。\n Vert.x的kotlin协程提供了async/await或者和go类似的channel。这使得你能够以熟悉的顺序风格写垂直代码。\nvertx-lang-kotlin-coroutines集成了kotlin协程，用于执行异步操作和处理事件。这样就能够以同步代码的模型编写代码，而且不会阻塞内核线程。\n简介 vert.x与许多旧的应用平台相比的一个主要优势是它几乎完全是非阻塞的（内核线程）。这允许基于vert.x的程序使用极少数的内核线程处理大量的并发（例如：许多连接和消息），可以获得更好的伸缩性。\nvert.x的非阻塞特性形成了非阻塞API。非阻塞API可以采用多种形式来实现，包括回调函数，promise，fibers或者响应式扩展。vert.x的核心API使用回调函数的风格，但是它也支持其他模型，如RxJava 1和2。\n在某些情况下，使用异步的API编程可能比使用经典的顺序代码风格更具有挑战性，特别是需要按照顺序完成若干操作。另外，使用异步API时，错误的传播也更为复杂。\nvertx-lang-kotlin-coroutines使用协程。协程是非常轻量级的线程，而且不与底层的内核线程对应。所以当协程需要“阻塞”时，它会暂停并释放当前的内核线程，使得另一个协程可以处理事件。\nvertx-lang-kotlin-coroutines使用kotlinx.coroutines来实现协程。\n vertx-lang-kotlin-coroutines目前仅适用于kotlin，而且是kotlin1.1的一个实验特性。\n 从一个vertx.x的contex中启动协程 导入io.vertx.kotlin.coroutines.VertxCoroutine，launch（协程生成器）方法中允许运行一段代码作为可以暂停的协程：\nval vertx = Vertx.vertx() vertx.deployVerticle(ExampleVerticle()) launch(vertx.dispatcher()) { val timerId = awaitEvent\u0026lt;Long\u0026gt; { handler -\u0026gt; vertx.setTimer(1000, handler) } println(\u0026#34;Event fired from timer with id ${timerId}\u0026#34;) } vertx.dispatcher()返回一个使用vert.x的事件循环执行协程的disptacher。\nawaitEvent函数暂停协程的执行直到定时器触发为止，并使用赋给handler的值恢复协程。\n有关handlers，events和事件流的更多细节，将在下一节中给出。\n继承CoroutineVerticle 你可以将代码部署为io.vertx.kotlin.coroutines.CoroutineVerticle的实例，这是kotlin协程的专用类型。你应该重载verticle的start()方法，stop()方法的重载是可选的：\nclass MyVerticle : CoroutineVerticle() { suspend override fun start() { // ...  } } 获得一次性的异步结果 vert.x的许多异步操作都采用Handler\u0026lt;AsyncResult\u0026gt;作为最后一个参数。一个例子就是使用vert.x的mongo client执行对象检索，或者是发送一个事件总线消息之后等待回复。\n这是通过awaitResult方法来实现，它返回一个值或者抛出一个异常。\n协程会一直处于暂停的状态知道事件被处理，并且这时没有内核线程被阻塞。\nThe method is executed by specifying the asynchronous operation that needs to be executed in the form of a block that is passed to the handler at run-time.\n这个方法是通过指定一个异步操作来执行，这个异步操作需要以块的形式执行，而这个异步操作块在运行时会被传给handler。\n这里是一个例子：\nsuspend fun awaitResultExample() { val consumer = vertx.eventBus().localConsumer\u0026lt;String\u0026gt;(\u0026#34;a.b.c\u0026#34;) consumer.handler { message -\u0026gt; println(\u0026#34;Consumer received: ${message.body()}\u0026#34;) message.reply(\u0026#34;pong\u0026#34;) } // Send a message and wait for a reply  val reply = awaitResult\u0026lt;Message\u0026lt;String\u0026gt;\u0026gt; { h -\u0026gt; vertx.eventBus().send(\u0026#34;a.b.c\u0026#34;, \u0026#34;ping\u0026#34;, h) } println(\u0026#34;Reply received: ${reply.body()}\u0026#34;) } 当此块产生失败时，调用者可以使用try/catch结构处理异常。\nsuspend fun awaitResultFailureExample() { val consumer = vertx.eventBus().localConsumer\u0026lt;String\u0026gt;(\u0026#34;a.b.c\u0026#34;) consumer.handler { message -\u0026gt; // The consumer will get a failure  message.fail(0, \u0026#34;it failed!!!\u0026#34;) } // Send a message and wait for a reply  try { val reply: Message\u0026lt;String\u0026gt; = awaitResult\u0026lt;Message\u0026lt;String\u0026gt;\u0026gt; { h -\u0026gt; vertx.eventBus().send(\u0026#34;a.b.c\u0026#34;, \u0026#34;ping\u0026#34;, h) } } catch(e: ReplyException) { // Handle specific reply exception here  println(\u0026#34;Reply failure: ${e.message}\u0026#34;) } } 获取一次性事件 使用awaitEvent函数处理一次性事件（而不是下一次出现的事件）：\nsuspend fun awaitEventExample() { val id = awaitEvent\u0026lt;Long\u0026gt; { h -\u0026gt; vertx.setTimer(2000L, h) } println(\u0026#34;This should be fired in 2s by some time with id=$id\u0026#34;) } 获取一次性worker的结果 使用awaitBlocking函数处理阻塞计算：\nsuspend fun awaitBlockingExample() { val s = awaitBlocking\u0026lt;String\u0026gt; { Thread.sleep(1000) \u0026#34;some-string\u0026#34; } } 事件流 在vert.x API的很多地方，事件流都是通过handler来处理。这些例子包括事件消息总线的使用者和http服务器的请求。\nReceiveChannelHandler类允许通过(suspendable)receive方法接收事件：\nsuspend fun streamExample() { val adapter = vertx.receiveChannelHandler\u0026lt;Message\u0026lt;Int\u0026gt;\u0026gt;() vertx.eventBus().localConsumer\u0026lt;Int\u0026gt;(\u0026#34;a.b.c\u0026#34;).handler(adapter) // Send 15 messages  for (i in 0..15) vertx.eventBus().send(\u0026#34;a.b.c\u0026#34;, i) // Receive the first 10 messages  for (i in 0..10) { val message = adapter.receive() println(\u0026#34;Received: ${message.body()}\u0026#34;) } } 等待vert.x的future的完成 vert.x的future类实例的扩展方法await，可以暂停协程直到他们完成。在这种情况下，该方法返回相应的AsyncResult对象。\nsuspend fun awaitingFuture() { val httpServerFuture = Future.future\u0026lt;HttpServer\u0026gt;() vertx.createHttpServer() .requestHandler { req -\u0026gt; req.response().end(\u0026#34;Hello!\u0026#34;) } .listen(8000, httpServerFuture) val httpServer = httpServerFuture.await() println(\u0026#34;HTTP server port: ${httpServer.actualPort()}\u0026#34;) val result = CompositeFuture.all(httpServerFuture, httpServerFuture).await() if (result.succeeded()) { println(\u0026#34;The server is now running!\u0026#34;) } else { result.cause().printStackTrace() } } 通道 channel和java的BlockingQueue类似，只是Channel不会阻塞而是暂停协程。\n 将值发送到满了的Channel会暂停协程 从一个空Channel中接收值也会暂停协程  使用toChannel的扩展方法可以将vert.x的ReadStream 和 WriteStream适配成通道。\n这些适配器负责管理背压和流终端：\n ReadStream适配为 ReceiveChannel WriteStream适配为 SendChannel  接收数据 当你需要处理一系列相关值的时候，channel可能非常有用：\nsuspend fun handleTemperatureStream() { val stream = vertx.eventBus().consumer\u0026lt;Double\u0026gt;(\u0026#34;temperature\u0026#34;) val channel = stream.toChannel(vertx) var min = Double.MAX_VALUE var max = Double.MIN_VALUE // Iterate until the stream is closed  // Non-blocking  for (msg in channel) { val temperature = msg.body() min = Math.min(min, temperature) max = Math.max(max, temperature) } // The stream is now closed } 他也可以用于解析协议。我们将构建一个非阻塞的http请求解析器来展示通道的功能。\n我们将依靠RecordParser将以\\r \\n分割的buffer流进行分割。\n这是解析器的初始版本，它只处理http的请求行。\nval server = vertx.createNetServer().connectHandler { socket -\u0026gt; // The record parser provides a stream of buffers delimited by \\r\\n  val stream = RecordParser.newDelimited(\u0026#34;\\r\\n\u0026#34;, socket) // Convert the stream to a Kotlin channel  val channel = stream.toChannel(vertx) // Run the coroutine  launch(vertx.dispatcher()) { // Receive the request-line  // Non-blocking  val line = channel.receive().toString().split(\u0026#34; \u0026#34;) val method = line[0] val uri = line[1] println(\u0026#34;Received HTTP request ($method, $uri)\u0026#34;) // Still need to parse headers and body...  } } 解析请求行就像在channel上调用receive一样简单。\n下一步是通过接收块来解析http头，直到我们得到一个空块为止。\n// Receive HTTP headers val headers = HashMap\u0026lt;String, String\u0026gt;() while (true) { // Non-blocking  val header = channel.receive().toString() // Done with parsing headers  if (header.isEmpty()) { break } val pos = header.indexOf(\u0026#39;:\u0026#39;) headers[header.substring(0, pos).toLowerCase()] = header.substring(pos + 1).trim() } println(\u0026#34;Received HTTP request ($method, $uri) with headers ${headers.keys}\u0026#34;) 最后，我们通过处理可选的请求体来终止解析器。\n// Receive the request body val transferEncoding = headers[\u0026#34;transfer-encoding\u0026#34;] val contentLength = headers[\u0026#34;content-length\u0026#34;] val body : Buffer? if (transferEncoding == \u0026#34;chunked\u0026#34;) { // Handle chunked encoding, e.g  // 5\\r\\n  // HELLO\\r\\n  // 0\\r\\n  // \\r\\n  body = Buffer.buffer() while (true) { // Parse length chunk  // Non-blocking  val len = channel.receive().toString().toInt(16) if (len == 0) { break } // The stream is flipped to parse a chunk of the exact size  stream.fixedSizeMode(len + 2) // Receive the chunk and append it  // Non-blocking  val chunk = channel.receive() body.appendBuffer(chunk, 0, chunk.length() - 2) // The stream is flipped back to the \\r\\n delimiter to parse the next chunk  stream.delimitedMode(\u0026#34;\\r\\n\u0026#34;) } } else if (contentLength != null) { // The stream is flipped to parse a body of the exact size  stream.fixedSizeMode(contentLength.toInt()) // Non-blocking  body = channel.receive() } else { body = null } println(\u0026#34;Received HTTP request ($method, $uri) with headers ${headers.keys}and body with size ${body?.length() ?: 0}\u0026#34;) 发送数据 使用channel发送数据也非常直接：\nsuspend fun sendChannel() { val stream = vertx.eventBus().publisher\u0026lt;Double\u0026gt;(\u0026#34;temperature\u0026#34;) val channel = stream.toChannel(vertx) while (true) { val temperature = readTemperatureSensor() // Broadcast the temperature  // Non-blocking but could be suspended  channel.send(temperature) // Wait for one second  awaitEvent\u0026lt;Long\u0026gt; { vertx.setTimer(1000, it) } } } SendChannel#send 和 WriteStream#write都是非阻塞操作。不像当channel满的时候SendChannel#send可以停止执行，而等效WriteStream#writ的无channel操作可能像这样：\n// Check we can write in the stream if (stream.writeQueueFull()) { // We can\u0026#39;t write so we set a drain handler to be called when we can write again  stream.drainHandler { broadcastTemperature() } } else { // Read temperature  val temperature = readTemperatureSensor() // Write it to the stream  stream.write(temperature) // Wait for one second  vertx.setTimer(1000) { broadcastTemperature() } } 延迟，取消和超时 借助于vert.x的定时器，vert.x的调度器完全支持协程的delay函数：\nlaunch(vertx.dispatcher()) { // Set a one second Vertx timer  delay(1000) } 定时器也支持取消：\nval job = launch(vertx.dispatcher()) { // Set a one second Vertx timer  while (true) { delay(1000) // Do something periodically  } } // Sometimes later job.cancel() 取消是合作的。\n你也可以使用withTimeout函数安排超时。\nlaunch(vertx.dispatcher()) { try { val id = withTimeout\u0026lt;String\u0026gt;(1000) { return awaitEvent\u0026lt;String\u0026gt; { anAsyncMethod(it) } } } catch (e: TimeoutCancellationException) { // Cancelled  } } Vert.x支持所有的协程构建器：launch，async和runBlocking。runBlocking构建器不能再vert.x的时间循环线程中使用。\n协程的互操作性 vert.x集成协程被设计为完全可以和kotlin协程互操作。\nkotlinx.coroutines.experimental.sync.Mutex被执行在使用vert.x调度器的事件循环线程。\nRxJava的互操作性 虽然vertx-lang-kotlin-coroutines模块没有与RxJava特定集成，但是kotlin协程提供了RxJava的集成。RxJava可以和vertx-lang-kotlin-coroutines很好的协同工作。\n你可以阅读响应流和协程的指南。\n","permalink":"https://zhenfeng-zhu.github.io/posts/vertx-kotlin-coroutine/","summary":"尝试翻译vertx的文档。尊重原文，部分使用自己的理解。\n Vert.x的kotlin协程提供了async/await或者和go类似的channel。这使得你能够以熟悉的顺序风格写垂直代码。\nvertx-lang-kotlin-coroutines集成了kotlin协程，用于执行异步操作和处理事件。这样就能够以同步代码的模型编写代码，而且不会阻塞内核线程。\n简介 vert.x与许多旧的应用平台相比的一个主要优势是它几乎完全是非阻塞的（内核线程）。这允许基于vert.x的程序使用极少数的内核线程处理大量的并发（例如：许多连接和消息），可以获得更好的伸缩性。\nvert.x的非阻塞特性形成了非阻塞API。非阻塞API可以采用多种形式来实现，包括回调函数，promise，fibers或者响应式扩展。vert.x的核心API使用回调函数的风格，但是它也支持其他模型，如RxJava 1和2。\n在某些情况下，使用异步的API编程可能比使用经典的顺序代码风格更具有挑战性，特别是需要按照顺序完成若干操作。另外，使用异步API时，错误的传播也更为复杂。\nvertx-lang-kotlin-coroutines使用协程。协程是非常轻量级的线程，而且不与底层的内核线程对应。所以当协程需要“阻塞”时，它会暂停并释放当前的内核线程，使得另一个协程可以处理事件。\nvertx-lang-kotlin-coroutines使用kotlinx.coroutines来实现协程。\n vertx-lang-kotlin-coroutines目前仅适用于kotlin，而且是kotlin1.1的一个实验特性。\n 从一个vertx.x的contex中启动协程 导入io.vertx.kotlin.coroutines.VertxCoroutine，launch（协程生成器）方法中允许运行一段代码作为可以暂停的协程：\nval vertx = Vertx.vertx() vertx.deployVerticle(ExampleVerticle()) launch(vertx.dispatcher()) { val timerId = awaitEvent\u0026lt;Long\u0026gt; { handler -\u0026gt; vertx.setTimer(1000, handler) } println(\u0026#34;Event fired from timer with id ${timerId}\u0026#34;) } vertx.dispatcher()返回一个使用vert.x的事件循环执行协程的disptacher。\nawaitEvent函数暂停协程的执行直到定时器触发为止，并使用赋给handler的值恢复协程。\n有关handlers，events和事件流的更多细节，将在下一节中给出。\n继承CoroutineVerticle 你可以将代码部署为io.vertx.kotlin.coroutines.CoroutineVerticle的实例，这是kotlin协程的专用类型。你应该重载verticle的start()方法，stop()方法的重载是可选的：\nclass MyVerticle : CoroutineVerticle() { suspend override fun start() { // ...  } } 获得一次性的异步结果 vert.x的许多异步操作都采用Handler\u0026lt;AsyncResult\u0026gt;作为最后一个参数。一个例子就是使用vert.x的mongo client执行对象检索，或者是发送一个事件总线消息之后等待回复。\n这是通过awaitResult方法来实现，它返回一个值或者抛出一个异常。\n协程会一直处于暂停的状态知道事件被处理，并且这时没有内核线程被阻塞。\nThe method is executed by specifying the asynchronous operation that needs to be executed in the form of a block that is passed to the handler at run-time.","title":"译：vertx-kotlin-coroutine"},{"content":" Being happy doesn\u0026rsquo;t mean that everything is perfect. It means that you decided to look beyond the imperfections.\n 后端编程，涉及最多的就是并发。简单理解就是：\n 并发是同时管理多个任务去执行，并行是针对多核处理器，同时执行多个任务。可以理解为一个是manage，一个是run。\n 并发一般特指IO，IO是独立于CPU的设备，IO设备通常远远慢于CPU，所以我们引入了并发的概念，让CPU可以一次性发起多个IO操作而不用等待IO设备做完一个操作再做令一个。原理就是非阻塞操作+事件通知。\n硬件底层上我其实不关心，主要就是在写程序上，如何简单的去写并发的代码。在语法层面上对并发做的比较好的，很适合做服务端，比如go，比如node，又比如某些函数式语言。我最近最近主要使用的是node和kotlin。\n那么在写并发代码的时候，就会时不时的想这样一个问题：\n一个问题 当代码遇到一个“暂时不能完成”的流程时（例如建立一个tcp链接，可能需要5ms才能建立），他不想阻塞在这里睡眠，想暂时离开现场去干点别的事情（例如看看另外一个已经建立的链接是否可以收包了）。问题是：离开现场后，当你回来的时候，上下文还像你走的时候吗？\n跳转离开，在任何语言里都有2种最基本的方法：1）从当前函数返回； 2）调用一个新的函数。 前者会把上下文中的局部变量和函数参数全部摧毁，除非他返回前把这些变量找个别的地方保存起来；后者则能保护住整个上下文的内存（除了协程切换后会摧毁一些寄存器），而且跳转回来也是常规方法：函数返回。\n在写node的时候，基本上是无脑上async/await。每次看到回调函数的时候，强迫症就犯了，总是想方设法将那个方法转成promise，然后使用await获得结果。无脑尝试了bluebird和node的util，虽然有些是很好用的，但是有的还是无法达到我预期的。靠着无脑的async/await，实现了很多功能，代码写起来也是快的飞起，但是只顾着做业务而不深入思考的话，是一个不好的表现，所以我就停下来搜了很多async/await的东西，特别是从阮一峰老师那里收获了很多。\njs异步编程 因为js是单线程，所以异步编程对js特别重要。\n实现异步主要有如下几种：\n  回调函数\ncallback，英语直译就是重新调用。\n所谓的回调函数就是把任务的第二段单独写在一个函数里面，等到重新执行这个任务的时候，直接调用这个函数。\n回调本身没问题，但是就怕多重嵌套。\n  promise\npromise是一种新的写法，把回调函数的横向嵌套，用then的形式改成纵向的加载。\n  协程\n协程就是比线程更小的单位。\n执行过程大致如下：\n第一步，协程A开始执行。\n第二步，协程A执行到一半，进入暂停，执行权转移到协程B。\n第三步，（一段时间后）协程B交还执行权。\n第四步，协程A恢复执行。\n后面再展开说协程。\n  很明显，在go火起来之后，很多编程语言都在往协程上靠，因为协程很好的将异步的写法转化成了同步的写法，降低了心智负担。js当然也不落后。\njs的异步写法的演进\n  generator\nes6增加了generator函数，就是协程的一种实现，最大特点就是使用yield关键字就是用来交出函数的执行权。\nfunction* gen(x){ var y = yield x + 2; return y; } 不同于普通函数的地方在于调用generator函数的时候，不返回结果，而是会返回一个内部的指针。调用指针的next方法，会移动内部指针（即执行异步任务的第一段），遇到的yield语句就交出执行权，执行别的代码。下次再调用该函数指针的next方法，就继续执行到该函数的下一个yield语句。\n虽然 Generator 函数将异步操作表示得很简洁，但是流程管理却不方便（即何时执行第一阶段、何时执行第二阶段），这样看来其实generator函数就是一个异步操作的容器，需要有一个触发它自动执行的机制。\n  Thunk函数\n说到thunk函数，就得先了解一下参数的求值策略。\nlet m=1; function f(x){ return x*2 } f(m+5)   传值调用\n先计算出来m+5的值6，然后再将值传给函数f，即6*2\n  传名调用\n把m+5传入到f中，在用到的时候再计算，即(x+5)*2。\n  传值调用比较简单，但是对参数求值的时候，实际上还没用到这个参数，有可能造成性能损失。\n编译器的\u0026quot;传名调用\u0026quot;实现，往往是将参数放到一个临时函数之中，再将这个临时函数传入函数体。这个临时函数就叫做 Thunk 函数。\njs是传值调用。他的thunk函数是将多参数的函数，替换成了单参数的版本，而且只接受回调函数作为参数。\n这样就可以很方便的实现了基于thunk函数的generator自动执行器。\n具体的实现和如何使用，参考http://www.ruanyifeng.com/blog/2015/05/thunk.html。\n  co函数\nco函数是基于Promise的generator函数的自动执行器。\n源代码只有几十行，tj大神太强👍了。\nhttp://www.ruanyifeng.com/blog/2015/05/co.html\n  async/await\nasync函数就是generator函数的语法糖。\nasync函数自带执行器，无脑写async和await的时候就是，几乎所有的函数都写成了async函数，只要需要等待的方法，都用await去等待，这样就造成了很多无意义的等待。本来两个不相干的操作，如果每个都是用await等的话，就会很影响性能。\n多个请求并发执行的时候，尽量选用Promise.all方法。\n  理解了以上的演进过程，感觉自己终于摆脱了java思维的枪，对node终于入门了。然后，同步地写着kotlin项目，又陷入了泥潭中。\nFuture、RxJava、Actor和kotlin协程 我理解的也不是很深，求科普。\n以前写java的时候，自己都是无脑用线程池，开多线程去处理，一般这种情况下不需要线程的结果。\n  future\n因为不能直接从别的线程中得到函数的返回值，所以future就出场了。\nFutrue可以监视目标线程调用call的情况，当你调用Future的get()方法以获得结果时，当前线程就开始阻塞，直接call方法结束返回结果。 Future对象本身可以看作是一个显式的引用，一个对异步处理结果的引用。由于其异步性质，在创建之初，它所引用的对象可能还并不可用（比如尚在运算中，网络传输中或等待中）。这时，得到Future的程序流程如果并不急于使用Future所引用的对象，那么它可以做其它任何想做的事儿，当流程进行到需要Future背后引用的对象时，可能有两种情况：\n  希望能看到这个对象可用，并完成一些相关的后续流程。\n可以通过调用Future.isDone()判断引用的对象是否就绪，并采取不同的处理。\n  如果实在不可用，也可以进入其它分支流程。\n只需调用get()或 get(long timeout, TimeUnit unit)通过同步阻塞方式等待对象就绪。实际运行期是阻塞还是立即返回就取决于get()的调用时机和对象就绪的先后了。\n    rxjava\n  actor\n  coroutine\n  跪求科普，等理解了再接着完善。\n浅谈协程 说到协程，就要说线程。\n线程是操作系统的用户态概念，线程本身也依赖中断来进行调度。早期的用户态IO并发处理是用poll(select)模型去轮询IO状态，然后发起相应的IO操作，称之为事件响应式的异步模型，这种方式并不容易使用，所以又发展出了阻塞式IO操作，让逻辑挂起并等待IO完成，为了让阻塞式IO能够并发就必须依赖多线程或者多进程模型来实现。但是线程的开销是非常大的，当遇到大规模并发的时候多线程模型就无法胜任了。所以大规模并发时我们又退回去使用事件响应，epoll在本质上还是poll模型，只是在算法上优化了实现，此时我们只用单线程就可以处理上万的并发请求了。\n直到多核CPU的出现，我们发现只用一个线程是无法发挥多核CPU的威力的，所以再次引入线程池来分摊IO操作的CPU消耗，甚至CPU的中断响应也可以由多个核来分摊执行，此时的线程数量是大致等于CPU的核心数而远小于并发IO数的（这时CPU能处理百万级的并发），线程的引入完全是为了负载均衡而跟并发没有关系。所以不管是用select/epoll/iocp在逻辑层都绕不开基于事件响应的异步操作，面对异步逻辑本身的复杂性，我们才引入了async/await以及coroutine来降低复杂性。\ncoroutine是个很宽泛的概念，async/await也属于coroutine的一种。\n而协程在实现模式上又分为：stackful coroutine和stackless coroutine。\n所谓stackful是指每个coroutine有独立的运行栈，比如go语言的每个goroutine会分配一个4k的内存来做为运行栈，切换goroutine的时候运行栈也会切换。stackful的好处在于这种coroutine是完整的，coroutine可以嵌套、循环。\n与stackful对应的是stackless coroutine，比如js的generator函数，这类coroutine不需要分配单独的栈空间，coroutine状态保存在闭包里，但缺点是功能比较弱，不能被嵌套调用，也没办法和异步函数配合使用进行控制流的调度，所以基本上没办法跟stackful coroutine做比较。保存这些状态的时候，有的语言就引入了状态机的模型来实现线程。\nasync/await的出现，实现了基于stackless coroutine的完整coroutine。在特性上已经非常接近stackful coroutine了，不但可以嵌套使用也可以支持try catch。\n结语 发现自己挺无脑的，学会了一个东西，就无脑的去用，直到碰壁了才会去思考。\n","permalink":"https://zhenfeng-zhu.github.io/posts/async/","summary":"Being happy doesn\u0026rsquo;t mean that everything is perfect. It means that you decided to look beyond the imperfections.\n 后端编程，涉及最多的就是并发。简单理解就是：\n 并发是同时管理多个任务去执行，并行是针对多核处理器，同时执行多个任务。可以理解为一个是manage，一个是run。\n 并发一般特指IO，IO是独立于CPU的设备，IO设备通常远远慢于CPU，所以我们引入了并发的概念，让CPU可以一次性发起多个IO操作而不用等待IO设备做完一个操作再做令一个。原理就是非阻塞操作+事件通知。\n硬件底层上我其实不关心，主要就是在写程序上，如何简单的去写并发的代码。在语法层面上对并发做的比较好的，很适合做服务端，比如go，比如node，又比如某些函数式语言。我最近最近主要使用的是node和kotlin。\n那么在写并发代码的时候，就会时不时的想这样一个问题：\n一个问题 当代码遇到一个“暂时不能完成”的流程时（例如建立一个tcp链接，可能需要5ms才能建立），他不想阻塞在这里睡眠，想暂时离开现场去干点别的事情（例如看看另外一个已经建立的链接是否可以收包了）。问题是：离开现场后，当你回来的时候，上下文还像你走的时候吗？\n跳转离开，在任何语言里都有2种最基本的方法：1）从当前函数返回； 2）调用一个新的函数。 前者会把上下文中的局部变量和函数参数全部摧毁，除非他返回前把这些变量找个别的地方保存起来；后者则能保护住整个上下文的内存（除了协程切换后会摧毁一些寄存器），而且跳转回来也是常规方法：函数返回。\n在写node的时候，基本上是无脑上async/await。每次看到回调函数的时候，强迫症就犯了，总是想方设法将那个方法转成promise，然后使用await获得结果。无脑尝试了bluebird和node的util，虽然有些是很好用的，但是有的还是无法达到我预期的。靠着无脑的async/await，实现了很多功能，代码写起来也是快的飞起，但是只顾着做业务而不深入思考的话，是一个不好的表现，所以我就停下来搜了很多async/await的东西，特别是从阮一峰老师那里收获了很多。\njs异步编程 因为js是单线程，所以异步编程对js特别重要。\n实现异步主要有如下几种：\n  回调函数\ncallback，英语直译就是重新调用。\n所谓的回调函数就是把任务的第二段单独写在一个函数里面，等到重新执行这个任务的时候，直接调用这个函数。\n回调本身没问题，但是就怕多重嵌套。\n  promise\npromise是一种新的写法，把回调函数的横向嵌套，用then的形式改成纵向的加载。\n  协程\n协程就是比线程更小的单位。\n执行过程大致如下：\n第一步，协程A开始执行。\n第二步，协程A执行到一半，进入暂停，执行权转移到协程B。\n第三步，（一段时间后）协程B交还执行权。\n第四步，协程A恢复执行。\n后面再展开说协程。\n  很明显，在go火起来之后，很多编程语言都在往协程上靠，因为协程很好的将异步的写法转化成了同步的写法，降低了心智负担。js当然也不落后。\njs的异步写法的演进\n  generator\nes6增加了generator函数，就是协程的一种实现，最大特点就是使用yield关键字就是用来交出函数的执行权。\nfunction* gen(x){ var y = yield x + 2; return y; } 不同于普通函数的地方在于调用generator函数的时候，不返回结果，而是会返回一个内部的指针。调用指针的next方法，会移动内部指针（即执行异步任务的第一段），遇到的yield语句就交出执行权，执行别的代码。下次再调用该函数指针的next方法，就继续执行到该函数的下一个yield语句。","title":"小议async/await和coroutine"},{"content":"以前没有好好学的东西，现在在工作中慢慢的补回来了。\n基础概念  索引  es是将数据存储在一个或者多个索引（index）中。\n索引就像是数据库。\n 文档  文档是es的实体。由字段构成，每个字段包含字段名和一个或者多个字段值。\n文档就像数据库中的一条条记录。\n 类型  每个文档都有一个类型与之相对应。\n类型就像数据库中的表。\n 映射  所有文档在被写入到es中，都会被分析。由用户设置一些参数决定如何分割词条、哪些字应该被过滤掉等等。\n 节点  单个es服务实例就是一个节点。\n 集群  多个协同工作的es节点的集合就是集群。\n 分片  es将数据分散到多个物理的Lucene索引上，这些物理Lucene索引被称为分片。\n 副本  副本就是每个分片都做冗余处理，一个宕机之后，不影响服务。\n快速入门 安装 es的安装很简单，我这里使用的是mac，下载下来zip包，解压即可使用。\n[elasticsearch-6.2.4] pwd /Users/zhuzhenfeng/Documents/software/elasticsearch-6.2.4 [elasticsearch-6.2.4] ./bin/elasticsearch Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. [2018-05-20T17:18:37,619][INFO ][o.e.n.Node ] [] initializing ... [2018-05-20T17:18:37,766][INFO ][o.e.e.NodeEnvironment ] [M41310-] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [136gb], net total_space [233.4gb], types [apfs] [2018-05-20T17:18:37,767][INFO ][o.e.e.NodeEnvironment ] [M41310-] heap size [990.7mb], compressed ordinary object pointers [true] 这样就将es启动了，然后在chrome中，输入http://localhost:9200，即可查看有没有启动成功。\n{ \u0026quot;name\u0026quot;: \u0026quot;M41310-\u0026quot;, \u0026quot;cluster_name\u0026quot;: \u0026quot;elasticsearch\u0026quot;, \u0026quot;cluster_uuid\u0026quot;: \u0026quot;58U11tViTYuXpI2b5SiGrg\u0026quot;, \u0026quot;version\u0026quot;: { \u0026quot;number\u0026quot;: \u0026quot;6.2.4\u0026quot;, \u0026quot;build_hash\u0026quot;: \u0026quot;ccec39f\u0026quot;, \u0026quot;build_date\u0026quot;: \u0026quot;2018-04-12T20:37:28.497551Z\u0026quot;, \u0026quot;build_snapshot\u0026quot;: false, \u0026quot;lucene_version\u0026quot;: \u0026quot;7.2.1\u0026quot;, \u0026quot;minimum_wire_compatibility_version\u0026quot;: \u0026quot;5.6.0\u0026quot;, \u0026quot;minimum_index_compatibility_version\u0026quot;: \u0026quot;5.0.0\u0026quot; }, \u0026quot;tagline\u0026quot;: \u0026quot;You Know, for Search\u0026quot; } 常用的命令 使用postman来模拟发送请求。\n创建index PUT\nhttp://127.0.0.1:9200/myindex response\n{ \u0026quot;acknowledged\u0026quot;: true, \u0026quot;shards_acknowledged\u0026quot;: true, \u0026quot;index\u0026quot;: \u0026quot;myindex\u0026quot; } 创建了一个叫myindex的索引\n删除index DELETE\nhttp://127.0.0.1:9200/myindex response\n{ \u0026quot;acknowledged\u0026quot;: true } 使用delete方法就可以删除索引，而且可以发现es的response特别人性化。\n创建maping POST\nhttp://localhost:9200/myindex/fulltext/_mapping body\n{ \u0026quot;properties\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;text\u0026quot;, \u0026quot;analyzer\u0026quot;: \u0026quot;ik_max_word\u0026quot;, \u0026quot;search_analyzer\u0026quot;: \u0026quot;ik_max_word\u0026quot; } } } response\n{ \u0026quot;acknowledged\u0026quot;: true } 在这里在创建一个type是fulltext的同时，指定了这个fulltext类型的字段映射。在mapping中，一般是设置字段是什么类型的，比如bool，text等。analyzer是给文档建索引的分词方法，search_analyzer是搜索时对搜索的内容进行分词的方法。这里都是用了ik的分词器。\n新增doc POST\nhttp://localhost:9200/myindex/fulltext/1 body\n{ \u0026quot;content\u0026quot;: \u0026quot;中国崛起哦\u0026quot; } response\n{ \u0026quot;_index\u0026quot;: \u0026quot;myindex\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;fulltext\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_version\u0026quot;: 1, \u0026quot;result\u0026quot;: \u0026quot;created\u0026quot;, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 2, \u0026quot;successful\u0026quot;: 1, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;_seq_no\u0026quot;: 0, \u0026quot;_primary_term\u0026quot;: 1 } 如果没有指定id的话，每次新增的时候都会用es自动给的id。如果不注意的话，可能会出现重复新增，所以我们一般情况下会使用自己给的的id。\n搜索 POST\nhttp://127.0.0.1:9200/myindex/fulltext/_search body\n{ \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;中国\u0026quot; } } } } response\n{ \u0026quot;took\u0026quot;: 98, \u0026quot;timed_out\u0026quot;: false, \u0026quot;_shards\u0026quot;: { \u0026quot;total\u0026quot;: 5, \u0026quot;successful\u0026quot;: 5, \u0026quot;skipped\u0026quot;: 0, \u0026quot;failed\u0026quot;: 0 }, \u0026quot;hits\u0026quot;: { \u0026quot;total\u0026quot;: 1, \u0026quot;max_score\u0026quot;: 0.2876821, \u0026quot;hits\u0026quot;: [ { \u0026quot;_index\u0026quot;: \u0026quot;myindex\u0026quot;, \u0026quot;_type\u0026quot;: \u0026quot;fulltext\u0026quot;, \u0026quot;_id\u0026quot;: \u0026quot;1\u0026quot;, \u0026quot;_score\u0026quot;: 0.2876821, \u0026quot;_source\u0026quot;: { \u0026quot;content\u0026quot;: \u0026quot;中国崛起哦\u0026quot; } } ] } }。 前面都没啥，主要就是如何把数据灌到es中。es作为一个搜索引擎，肯定搜索才是最重要的。这里只是用的最简单的搜索。关于es的搜索，我在实际生产中主要使用的是，多字段的搜索，使用了bool操作符。\n原理 相关性得分 Elasticsearch 默认按照相关性得分排序，即每个文档跟查询的匹配程度。Elasticsearch中的 相关性 概念非常重要，也是完全区别于传统关系型数据库的一个概念，数据库中的一条记录要么匹配要么不匹配。\n搜索 轻量级搜索 _search\n_search?q=content:中国\n这种搜索方式比较简单，很轻量。\n查询表达式 即dsl形式的。使用的是POST方法，在body中，写搜索的dsl。\n简单的dsl { \u0026quot;query\u0026quot;: { \u0026quot;match\u0026quot;: { \u0026quot;content\u0026quot;: { \u0026quot;query\u0026quot;: \u0026quot;中国\u0026quot; } } } } bool操作符的DSL { \u0026quot;query\u0026quot; : { \u0026quot;bool\u0026quot;: { \u0026quot;must\u0026quot;: { \u0026quot;match\u0026quot; : { \u0026quot;last_name\u0026quot; : \u0026quot;smith\u0026quot; } }, \u0026quot;filter\u0026quot;: { \u0026quot;range\u0026quot; : { \u0026quot;age\u0026quot; : { \u0026quot;gt\u0026quot; : 30 } } } } } } 我们添加了一个 过滤器 用于执行一个范围查询，并复用之前的 match 查询。\n短语搜索 { \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;about\u0026quot; : \u0026quot;rock climbing\u0026quot; } } } 如果不用短语搜索的话，会将只含有rock或者climbing的返回。为了能让二者是短语形式，es中新增了短语搜索dsl。\n高亮 许多应用都倾向于在每个搜索结果中 高亮 部分文本片段，以便让用户知道为何该文档符合查询条件。在 Elasticsearch 中检索出高亮片段也很容易。\n{ \u0026quot;query\u0026quot; : { \u0026quot;match_phrase\u0026quot; : { \u0026quot;about\u0026quot; : \u0026quot;rock climbing\u0026quot; } }, \u0026quot;highlight\u0026quot;: { \u0026quot;fields\u0026quot; : { \u0026quot;about\u0026quot; : {} } } } 加上highlight关键字即可。\n聚合  Elasticsearch 有一个功能叫聚合（aggregations），允许我们基于数据生成一些精细的分析结果。聚合与 SQL 中的 GROUP BY 类似但更强大。\n{ \u0026quot;aggs\u0026quot;: { \u0026quot;all_interests\u0026quot;: { \u0026quot;terms\u0026quot;: { \u0026quot;field\u0026quot;: \u0026quot;interests\u0026quot; } } } } ","permalink":"https://zhenfeng-zhu.github.io/posts/elasticsearch/","summary":"以前没有好好学的东西，现在在工作中慢慢的补回来了。\n基础概念  索引  es是将数据存储在一个或者多个索引（index）中。\n索引就像是数据库。\n 文档  文档是es的实体。由字段构成，每个字段包含字段名和一个或者多个字段值。\n文档就像数据库中的一条条记录。\n 类型  每个文档都有一个类型与之相对应。\n类型就像数据库中的表。\n 映射  所有文档在被写入到es中，都会被分析。由用户设置一些参数决定如何分割词条、哪些字应该被过滤掉等等。\n 节点  单个es服务实例就是一个节点。\n 集群  多个协同工作的es节点的集合就是集群。\n 分片  es将数据分散到多个物理的Lucene索引上，这些物理Lucene索引被称为分片。\n 副本  副本就是每个分片都做冗余处理，一个宕机之后，不影响服务。\n快速入门 安装 es的安装很简单，我这里使用的是mac，下载下来zip包，解压即可使用。\n[elasticsearch-6.2.4] pwd /Users/zhuzhenfeng/Documents/software/elasticsearch-6.2.4 [elasticsearch-6.2.4] ./bin/elasticsearch Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. [2018-05-20T17:18:37,619][INFO ][o.e.n.Node ] [] initializing .","title":"elasticsearch"},{"content":"整理一下常用的git操作，不用再到处找了。\ngit放弃本地修改，强制更新 git fetch --all git reset --hard origin/master git修改远程仓库地址 git remote set-url origin url cherry-pick 当你通过一番挣扎终于搞定一个bug,顺手提交到 git 服务器,心里一阵暗爽. 这时发现你当前所在的分支是 master !!!\n这个分支不是开发者用来提交代码的,可惜现在剁手也晚了.\n 先切换到master  git checkout master git log  复制提交的commit id\n  切换到dev, cherry-pick\n  git checkout dev git cherry-pic ${commit_id} 常用开发流程 git checkout -b feature1\ngit commit之后，进行rebase\ngit pull \u0026ndash;rebase\ngca!\ngit rvm\n","permalink":"https://zhenfeng-zhu.github.io/posts/git/","summary":"整理一下常用的git操作，不用再到处找了。\ngit放弃本地修改，强制更新 git fetch --all git reset --hard origin/master git修改远程仓库地址 git remote set-url origin url cherry-pick 当你通过一番挣扎终于搞定一个bug,顺手提交到 git 服务器,心里一阵暗爽. 这时发现你当前所在的分支是 master !!!\n这个分支不是开发者用来提交代码的,可惜现在剁手也晚了.\n 先切换到master  git checkout master git log  复制提交的commit id\n  切换到dev, cherry-pick\n  git checkout dev git cherry-pic ${commit_id} 常用开发流程 git checkout -b feature1\ngit commit之后，进行rebase\ngit pull \u0026ndash;rebase\ngca!\ngit rvm","title":"git常用操作"},{"content":"我们知道js是运行单线程的，也就是说一个node进程只能运行在一个cpu上。那么如果用node来做web server的话，就无法享受到多核运算的好处。\n一个问题就是：\n如何榨干服务器资源，利用多核CPU的并发优势。 node官方提供的解决方案是cluster。\n1 cluster是什么 简单来说：\n 在服务器上同时启动多个进程。 每个进程都跑的是同一份源码。 这些进程可以同时监听一个端口。  其中：\n 负责启动其他进程的叫做master进程，不做具体工作，只负责启动其他进程。 其他被启动的叫worker进程。他们接收请求，对外提供服务。 worker进程的数量一般根据服务器的cpu核数来决定，这样就可以完美利用多核资源。  以下是官方文档的一个例子：\nconst cluster = require('cluster'); const http = require('http'); const numCPUs = require('os').cpus().length; if (cluster.isMaster) { console.log(`主进程 ${process.pid} 正在运行`); // 衍生工作进程。 for (let i = 0; i \u0026lt; numCPUs; i++) { cluster.fork(); } cluster.on('exit', (worker, code, signal) =\u0026gt; { console.log(`工作进程 ${worker.process.pid} 已退出`); }); } else { // 工作进程可以共享任何 TCP 连接。 // 在本例子中，共享的是一个 HTTP 服务器。 http.createServer((req, res) =\u0026gt; { res.writeHead(200); res.end('你好世界\\n'); }).listen(8000); console.log(`工作进程 ${process.pid} 已启动`); } % node cluster.js 主进程 16391 正在运行 工作进程 16394 已启动 工作进程 16393 已启动 工作进程 16395 已启动 工作进程 16392 已启动 2 多进程模型 官网的示例很简单，但是我们还要考虑很多东西。\n worker进程异常退出以后如何处理？ 多个worker进程之间如何共享资源？ 多个worker进程之间如何调度？ 。。。  进程守护 健壮性是我们做大型应用要考虑的一个问题。一般来说，node的进程退出可以分为两类：\n 未捕获异常 内存溢出（OOM）或者系统异常  代码跑出了异常却没有没捕捉时，进程将会退出，此时node提供了process.on(\u0026lsquo;uncaughtException\u0026rsquo;, handler)来捕获。但是当一个worker进程遇到未 捕获的异常时，他已经处于一个不确定的状态，此时我们应该让这个进程优雅的退出。\n优雅退出的流程是：\n 关闭异常worker进程和所有的tcp server(将已有的连接快速断开，且不再接收新的连接)，断开和master的ipc通道，不再接收新的用户请求。 master立即fork一个新的worker进程，保证在线的“工人”总数不变。 异常worker等待一段时间，处理完已经接受的请求后退出。  而当一个进程出现异常导致 crash 或者 OOM 被系统杀死时，不像未捕获异常发生时我们还有机会让进程继续执行，只能够让当前进程直接退出，Master 立刻 fork 一个新的 Worker。\nAgent机制 有些工作并不需要每个worker都去做，如果都做，一个是浪费资源，另一个更重要的是可能会导致多进程之间资源访问冲突。\n比如：生产环境中一般会按照日期进行归档，在单进程模型的时候比较简单：\n 每天凌晨，批量将日志文件重命名 创建新的日志文件继续写入。  如果这个任务由4个进程同时做，就乱套了。所以对于这一类的后台运行逻辑，应该放到一个单独的进程去执行，这个进程就是agent。agent好比是给其他worker请的一个秘书，它不对外提供服务，只给worker打工，处理一些公共事务。\n另外，关于 Agent Worker 还有几点需要注意的是：\n 由于 Worker 依赖于 Agent，所以必须等 Agent 初始化完成后才能 fork Worker Agent 虽然是 Worker 的『小秘』，但是业务相关的工作不应该放到 Agent 上去做。 由于 Agent 的特殊定位，我们应该保证它相对稳定。当它发生未捕获异常，不应该像 Worker 一样让他退出重启，而是记录异常日志、报警等待人工处理。  3 进程间通讯（IPC） 虽然每个 Worker 进程是相对独立的，但是它们之间始终还是需要通讯的，叫进程间通讯（IPC）。\nif (cluster.isMaster) { const worker = cluster.fork(); worker.send('hi there'); } else if (cluster.isWorker) { process.on('message', (msg) =\u0026gt; { process.send(msg); }); } 这个例子里面，工作进程将主进程发送的消息echo回去。\n4 长连接 一些中间件客户端需要和服务器建立长连接，理论上一台服务器最好只建立一个长连接，但多进程模型会导致 n 倍（n = Worker 进程数）连接被创建。\n为了尽可能的复用长连接（因为它们对于服务端来说是非常宝贵的资源），我们会把它放到 Agent 进程里维护，然后通过 messenger 将数据传递给各个 Worker。这种做法是可行的，但是往往需要写大量代码去封装接口和实现数据的传递，非常麻烦。\n另外，通过 messenger 传递数据效率是比较低的，因为它会通过 Master 来做中转；万一 IPC 通道出现问题还可能将 Master 进程搞挂。\n那么有没有更好的方法呢？答案是肯定的，我们提供一种新的模式来降低这类客户端封装的复杂度。通过建立 Agent 和 Worker 的 socket 直连跳过 Master 的中转。Agent 作为对外的门面维持多个 Worker 进程的共享连接。\n核心思想  受到 Leader/Follower 模式的启发 客户端会被区分为两种角色：  Leader: 负责和远程服务端维持连接，对于同一类的客户端只有一个 Leader Follower: 会将具体的操作委托给 Leader，常见的是订阅模型（让 Leader 和远程服务端交互，并等待其返回）。   如何确定谁是 Leader，谁是 Follower 呢？有两种模式：  自由竞争模式：客户端启动的时候通过本地端口的争夺来确定 Leader。例如：大家都尝试监听 7777 端口，最后只会有一个实例抢占到，那它就变成 Leader，其余的都是 Follower。 强制指定模式：框架指定某一个 Leader，其余的就是 Follower   启动的时候 Master 会随机选择一个可用的端口作为 Cluster Client 监听的通讯端口，并将它通过参数传递给 Agent 和 App Worker Leader 和 Follower 之间通过 socket 直连（通过通讯端口），不再需要 Master 中转   5 参考资料 多进程模型和进程间通讯\nNode.js v8.11.1 文档 cluster\n","permalink":"https://zhenfeng-zhu.github.io/posts/node%E7%9A%84cluster/","summary":"我们知道js是运行单线程的，也就是说一个node进程只能运行在一个cpu上。那么如果用node来做web server的话，就无法享受到多核运算的好处。\n一个问题就是：\n如何榨干服务器资源，利用多核CPU的并发优势。 node官方提供的解决方案是cluster。\n1 cluster是什么 简单来说：\n 在服务器上同时启动多个进程。 每个进程都跑的是同一份源码。 这些进程可以同时监听一个端口。  其中：\n 负责启动其他进程的叫做master进程，不做具体工作，只负责启动其他进程。 其他被启动的叫worker进程。他们接收请求，对外提供服务。 worker进程的数量一般根据服务器的cpu核数来决定，这样就可以完美利用多核资源。  以下是官方文档的一个例子：\nconst cluster = require('cluster'); const http = require('http'); const numCPUs = require('os').cpus().length; if (cluster.isMaster) { console.log(`主进程 ${process.pid} 正在运行`); // 衍生工作进程。 for (let i = 0; i \u0026lt; numCPUs; i++) { cluster.fork(); } cluster.on('exit', (worker, code, signal) =\u0026gt; { console.log(`工作进程 ${worker.process.pid} 已退出`); }); } else { // 工作进程可以共享任何 TCP 连接。 // 在本例子中，共享的是一个 HTTP 服务器。 http.","title":"node的cluster"},{"content":"module 首先第一个就是es6的module。\n看到别人写的\nimport { a } from \u0026quot;./module\u0026quot;; 所以自己也想要这么写，但是每次运行的时候都会报错。\n// demo2.js export const a = \u0026quot;hello\u0026quot;; //demo1.js import { a } from \u0026quot;./demo2\u0026quot;; function hello() { console.log(a); } zhuzhenfengdeMacBook-Pro :: node/node-example » node demo1.js /Users/zhuzhenfeng/Documents/github/node/node-example/demo1.js:1 (function (exports, require, module, __filename, __dirname) { import { a } from \u0026quot;./demo2\u0026quot;; ^ SyntaxError: Unexpected token { at new Script (vm.js:74:7) at createScript (vm.js:246:10) at Object.runInThisContext (vm.js:298:10) at Module._compile (internal/modules/cjs/loader.js:646:28) at Object.Module._extensions..js (internal/modules/cjs/loader.js:689:10) at Module.load (internal/modules/cjs/loader.js:589:32) at tryModuleLoad (internal/modules/cjs/loader.js:528:12) at Function.Module._load (internal/modules/cjs/loader.js:520:3) at Function.Module.runMain (internal/modules/cjs/loader.js:719:10) at startup (internal/bootstrap/node.js:228:19) zhuzhenfengdeMacBook-Pro :: node/node-example » 后来仔细查了资料之后，才发现，node现在还不能这么使用。\n如下是解决的办法。\n给每个js文件都以mjs命名。\n// module.js export const a = \u0026quot;hello\u0026quot;; // useModule.js import { a } from \u0026quot;./module.mjs\u0026quot;; function hello() { console.log(a); } hello(); zhuzhenfengdeMacBook-Pro :: node/node-example » node --experimental-modules useModule.mjs (node:15282) ExperimentalWarning: The ESM module loader is experimental. hello zhuzhenfengdeMacBook-Pro :: node/node-example » 这样才能够正常使用。\n","permalink":"https://zhenfeng-zhu.github.io/posts/node%E8%B8%A9%E5%9D%91/","summary":"module 首先第一个就是es6的module。\n看到别人写的\nimport { a } from \u0026quot;./module\u0026quot;; 所以自己也想要这么写，但是每次运行的时候都会报错。\n// demo2.js export const a = \u0026quot;hello\u0026quot;; //demo1.js import { a } from \u0026quot;./demo2\u0026quot;; function hello() { console.log(a); } zhuzhenfengdeMacBook-Pro :: node/node-example » node demo1.js /Users/zhuzhenfeng/Documents/github/node/node-example/demo1.js:1 (function (exports, require, module, __filename, __dirname) { import { a } from \u0026quot;./demo2\u0026quot;; ^ SyntaxError: Unexpected token { at new Script (vm.js:74:7) at createScript (vm.js:246:10) at Object.runInThisContext (vm.js:298:10) at Module._compile (internal/modules/cjs/loader.js:646:28) at Object.Module._extensions..js (internal/modules/cjs/loader.js:689:10) at Module.","title":"node踩坑"},{"content":"写node也有一段时间了，整理一下学习笔记，共同进步\n什么是node？ 首先看一下什么是node.js\n Node 是一个服务器端 JavaScript Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境 Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效 Node.js 的包管理器 npm，是全球最大的开源库生态系统  模块系统是node最基本也是最常用的。一般可以分为四类：\n 原生模块 文件模块 第三方模块 自定义模块  node社区崇尚DRY文化，即Don\u0026rsquo;t repeate yourself。这种文化使得node的生态异常繁荣，同样也由于某些包的质量低下引来了一些诟病。\n谈谈自定义模块 我们在写node程序的时候，一般都是在写自定义模块。\n  创建模块\n// b.js function FunA(){ return \u0026quot;hello world\u0026quot;; } // 暴露方法FunA module.exports = FunA;   加载模块\n// a.js const FunA=require('./b.js'); // 运行FunA const name=FunA(); console.log(name);   在做模块到处的时候有两种方式：\n  module.exports\nmodule.exports 就 Node.js 用于对外暴露，或者说对外开放指定访问权限的一个对象。\n一个模块中有且仅有一个 module.exports，如果有多个那后面的则会覆盖前面的。\n  exports\nexports 是 module 对象的一个属性，同时它也是一个对象。在很多时候一个 js 文件有多个需要暴露的方法或是对象，module.exports 又只能暴露一个，那这个时候就要用到 exports:\nfunction FunA(){ return 'Tom'; } function FunB(){ return 'Sam'; } exports.FunA = FunA; exports.FunB = FunB; //FunA = exports,exports 是一个对象 var FunA = require('./b.js'); var name1 = FunA.FunA();// 运行 FunA，name = 'Tom' var name2 = FunA.FunB();// 运行 FunB，name = 'Sam' console.log(name1); console.log(name2); 当然在引入的时候也可以这样写：\n//FunA = exports,exports 是一个对象 var {FunA, FunB} = require('./b.js'); var name1 = FunA();// 运行 FunA，name = 'Tom' var name2 = FunB();// 运行 FunB，name = 'Sam' console.log(name1); console.log(name2);   常用的原生模块 常用的原生模块有如下四个：\n http url queryString fs  http 所有后端的语言要想运行起来，都得有服务器。node通过原生的http模块来搭建服务器：\n 加载 http 模块 调用 http.createServer() 方法创建服务，方法接受一个回调函数，回调函数中有两个参数，第一个是请求体，第二个是响应体。 在回调函数中一定要使用 response.end() 方法，用于结束当前请求，不然当前请求会一直处在等待的状态。 调用 listen 监听一个端口。  //原生模块 var http = require('http'); http.createServer(function(reqeust, response){ response.end('Hello Node'); }).listen(8080); 处理参数\n  get\n当get请求的时候，服务器通过request.method来判断当前的请求方式并通过request.url来获取当前的请求参数：\nvar http = require('http'); var url = require('url'); http.createServer(function(req, res){ var params = url.parse(req.url, true).query; res.end(params); }).listen(3000);   post\npost请求则不能通过url来获取，这时候就得对请求体进行事件监听。\nvar http = require('http'); var util = require('util'); var querystring = require('querystring'); http.createServer(function(req, res){ // 定义了一个post变量，用于暂存请求体的信息 var post = ''; // 通过req的data事件监听函数，每当接受到请求体的数据，就累加到post变量中 req.on('data', function(chunk){ post += chunk; }); // 在end事件触发后，通过querystring.parse将post解析为真正的POST请求格式，然后向客户端返回。 req.on('end', function(){ post = querystring.parse(post); res.end(util.inspect(post)); }); }).listen(3000);   url url和http是配合使用的。一般情况下url都是字符串类型的，包含的信息也比较多，比如有：协议、主机名、端口、路径、参数、锚点等。如果是对字符串进行直接解析的话，相当麻烦，node提供的url模块便可轻松解决这一类的问题。\n字符串转对象  格式：url.parse(urlstring, boolean) 参数  urlstring：字符串格式的 url boolean：在 url 中有参数，默认参数为字符串，如果此参数为 true，则会自动将参数转转对象   常用属性  href： 解析前的完整原始 URL，协议名和主机名已转为小写 protocol： 请求协议，小写 host： url 主机名，包括端口信息，小写 hostname: 主机名，小写 port: 主机的端口号 pathname: URL中路径，下面例子的 /one search: 查询对象，即：queryString，包括之前的问号“?” path: pathname 和 search的合集 query: 查询字符串中的参数部分（问号后面部分字符串），或者使用 querystring.parse() 解析后返回的对象 hash: 锚点部分（即：“#”及其后的部分）    对象转字符串  格式：url.format(urlObj) 参数 urlObj 在格式化的时候会做如下处理  href: 会被忽略，不做处理 protocol：无论末尾是否有冒号都会处理，协议包括 http, https, ftp, gopher, file 后缀是 :// (冒号-斜杠-斜杠) hostname：如果 host 属性没被定义，则会使用此属性 port：如果 host 属性没被定义，则会使用此属性 host：优先使用，将会替代 hostname 和port pathname：将会同样处理无论结尾是否有/ (斜杠) search：将会替代 query属性，无论前面是否有 ? (问号)，都会同样的处理 query：(object类型; 详细请看 querystring) 如果没有 search,将会使用此属性. hash：无论前面是否有# (井号, 锚点)，都会同样处理    拼接 当有多个 url 需要拼接处理的时候，可以用到 url.resolve\nvar url = require('url'); url.resolve('http://dk-lan.com/', '/one')// 'http://dk-lan.com/one' querystring url是对url字符串的处理，而querystring就是仅针对参数的处理。\n字符串转对象 var str = 'firstname=dk\u0026amp;url=http%3A%2F%2Fdk-lan.com\u0026amp;lastname=tom\u0026amp;passowrd=123456'; var param = querystring.parse(param); //结果 //{firstname:\u0026quot;dk\u0026quot;, url:\u0026quot;http://dk-lan.com\u0026quot;, lastname: 'tom', passowrd: 123456}; 对象转字符串 var querystring = require('querystring'); var obj = {firstname:\u0026quot;dk\u0026quot;, url:\u0026quot;http://dk-lan.com\u0026quot;, lastname: 'tom', passowrd: 123456}; //将对象转换成字符串 var param = querystring.stringify(obj); //结果 //firstname=dk\u0026amp;url=http%3A%2F%2Fdk-lan.com\u0026amp;lastname=tom\u0026amp;passowrd=123456 fs 任何服务端语言都不能缺失文件的读写操作。\n读取文本 \u0026ndash; 异步读取 var fs = require('fs'); // 异步读取 // 参数1：文件路径， // 参数2：读取文件后的回调 fs.readFile('demoFile.txt', function (err, data) { if (err) { return console.error(err); } console.log(\u0026quot;异步读取: \u0026quot; + data.toString()); }); 读取文本 \u0026ndash; 同步读取 var fs = require('fs'); var data = fs.readFileSync('demoFile.txt'); console.log(\u0026quot;同步读取: \u0026quot; + data.toString()); 写入文本 \u0026ndash; 覆盖写入 var fs = require('fs'); //每次写入文本都会覆盖之前的文本内容 fs.writeFile('input.txt', '抵制一切不利于中国和世界和平的动机！', function(err) { if (err) { return console.error(err); } console.log(\u0026quot;数据写入成功！\u0026quot;); console.log(\u0026quot;--------我是分割线-------------\u0026quot;) console.log(\u0026quot;读取写入的数据！\u0026quot;); fs.readFile('input.txt', function (err, data) { if (err) { return console.error(err); } console.log(\u0026quot;异步读取文件数据: \u0026quot; + data.toString()); }); }); 写入文本 \u0026ndash; 追加写入 var fs = require('fs'); fs.appendFile('input.txt', '愿世界和平！', function (err) { if (err) { return console.error(err); } console.log(\u0026quot;数据写入成功！\u0026quot;); console.log(\u0026quot;--------我是分割线-------------\u0026quot;) console.log(\u0026quot;读取写入的数据！\u0026quot;); fs.readFile('input.txt', function (err, data) { if (err) { return console.error(err); } console.log(\u0026quot;异步读取文件数据: \u0026quot; + data.toString()); }); }); 图片读取 图片读取不同于文本，因为文本读出来可以直接用 console.log() 打印，但图片则需要在浏览器中显示，所以需要先搭建 web 服务，然后把以字节方式读取的图片在浏览器中渲染。\n 图片读取是以字节的方式 图片在浏览器的渲染因为没有 img 标签，所以需要设置响应头为 image  var http = require('http'); var fs = require('fs'); var content = fs.readFileSync('001.jpg', \u0026quot;binary\u0026quot;); http.createServer(function(request, response){ response.writeHead(200, {'Content-Type': 'image/jpeg'}); response.write(content, \u0026quot;binary\u0026quot;); response.end(); }).listen(8888); console.log('Server running at http://127.0.0.1:8888/'); stream流处理 对http 服务器发起请求的request 对象就是一个 Stream，还有stdout（标准输出）。往往用于打开大型的文本文件，创建一个读取操作的数据流。所谓大型文本文件，指的是文本文件的体积很大，读取操作的缓存装不下，只能分成几次发送，每次发送会触发一个data事件，发送结束会触发end事件。\n主要分为\n 读取流 写入流 管道流 链式流  这几种流都是fs的一部分。\n路由 在BS架构中，路由的概念都是一样的，可以理解为根据客户端请求的url映射到不同的方法实现。一般web框架中都会有相应的路由模块。但是在原生node中去处理的话只能是解析url来进行映射，实现起来不够简洁。\nfetch axios是一种对ajax的封装，fetch是一种浏览器原生实现的请求方式，跟ajax对等。\n在现在发起http请求里，都是通过fetch来发送请求，和ajax类似。\nconst fetch=require('isomorphic-fetch'); const options={ header:{}, body:JSON.strify({}), method: '' } try{ const res=await fetch('url', options); }catch(err){ } Async Node.js 是一个异步机制的服务端语言，在大量异步的场景下需要按顺序执行，那正常做法就是回调嵌套回调，回调嵌套太多的问题被称之回调地狱。\nNode.js 为解决这一问题推出了异步控制流 ———— Async\nAsync/Await\nAsync/Await 就 ES7 的方案，结合 ES6 的 Promise 对象，使用前请确定 Node.js 的版本是 7.6 以上。\nAsync/await的主要益处是可以避免回调地狱（callback hell），且以最接近同步代码的方式编写异步代码。\n基本规则\n async 表示这是一个async函数，await只能用在这个函数里面。 await 表示在这里等待promise返回结果了，再继续执行。 await 后面跟着的应该是一个promise对象  express框架 使用node，都绕不开express。\n简单使用 express的使用比较简单，由于我最早接触的是spring那套web框架，所以在使用到express的时候觉得node的web特别轻量简单。\n加载模块\nconst express=require('express'); const app=express(); 监听端口8080\napp.listen(3000, ()=\u0026gt;consloe.log('running')); 路由 express对路由的处理特别简单，配合中间件body parser，很方便的提供rest接口：\napp.get('/', (req, res)=\u0026gt;{ res.send('hello world'); }) response.send() 可理解为 response.end()，其中一个不同点在于 response.send() 参数可为对象。\nNode.js 默认是不能访问静态资源文件（.html、.js、.css、.jpg 等），如果要访问服务端的静态资源文件则要用到方法 sendFile\n__dirname 为 Node.js 的系统变量，指向文件的绝对路径。\napp.get('/index.html', function (req, res) { res.sendFile( __dirname + \u0026quot;/\u0026quot; + \u0026quot;index.html\u0026quot; ); }); Express \u0026ndash; GET 参数接收之路径方式\n访问地址：http://localhost:8080/getusers/admin/18，可通过 request.params 来获取参数\napp.get('/getUsers/:username/:age', function(request, response){ var params = { username: request.params.username, age: request.params.age } response.send(params); }) Express \u0026ndash; POST\n post 参数接收，可依赖第三方模块 body-parser 进行转换会更方便、更简单，该模块用于处理 JSON, Raw, Text 和 URL 编码的数据。 安装 body-parser npm install body-parser 参数接受和 GET 基本一样，不同的在于 GET 是 request.query 而 POST 的是 request.body  var bodyParser = require('body-parser'); // 创建 application/x-www-form-urlencoded 编码解析 var urlencodedParser = bodyParser.urlencoded({ extended: false }) app.post('/getUsers', urlencodedParser, function (request, response) { var params = { username: request.body.username, age: request.body.age } response.send(params); }); Express \u0026ndash; 跨域支持(放在最前面)\napp.all('*', function(req, res, next) { res.header(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;); res.header(\u0026quot;Access-Control-Allow-Headers\u0026quot;, \u0026quot;Content-Type,Content-Length, Authorization, Accept,X-Requested-With\u0026quot;); res.header(\u0026quot;Access-Control-Allow-Methods\u0026quot;,\u0026quot;PUT,POST,GET,DELETE,OPTIONS\u0026quot;); res.header(\u0026quot;X-Powered-By\u0026quot;,' 3.2.1') if(req.method==\u0026quot;OPTIONS\u0026quot;) { res.send(200);/*让options请求快速返回*/ } else{ next(); } }); 中间件 express的中间件编写——过滤器\n简单使用\nconst express = require('express') const app = express(); let filter = (req, res, next) =\u0026gt; { if(req.params.name == 'admin' \u0026amp;\u0026amp; req.params.pwd == 'admin'){ next() } else { next('用户名密码不正确') } } app.get('/:name/:pwd', filter, (req, res) =\u0026gt; { res.send('ok') }).listen(88) 这里写了一个filter方法，有一个next参数。在路由的时候，把filter作为一个参数，则就可以先执行filter函数，然后执行路由的逻辑。\n如果想要全局使用的话，就直接使用use方法即可。\napp.use(filter); 文件上传 前面说到的body-parser不支持文件上传，那么使用multer则可以实现。\n操作数据库 node一般会使用mongo和mysql，使用下面这个例子即可：\n操作 MongoDB 官方 api http://mongodb.github.io/node-mongodb-native/\nvar mongodb = require('mongodb'); var MongoClient = mongodb.MongoClient; var db; MongoClient.connect(\u0026quot;mongodb://localhost:27017/test1705candel\u0026quot;, function(err, database) { if(err) throw err; db = database; }); module.exports = { insert: function(_collection, _data, _callback){ var i = db.collection(_collection).insert(_data).then(function(result){ _callback(result); }); }, select: function(_collection, _condition, _callback){ var i = db.collection(_collection).find(_condition || {}).toArray(function(error, dataset){ _callback({status: true, data: dataset}); }) } } 操作 MySql var mysql = require('mysql'); //创建连接池 var pool = mysql.createPool({ host : 'localhost', user : 'root', password : 'root', port: 3306, database: '1000phone', multipleStatements: true }); module.exports = { select: function(tsql, callback){ pool.query(tsql, function(error, rows){ if(rows.length \u0026gt; 1){ callback({rowsCount: rows[1][0]['rowsCount'], data: rows[0]}); } else { callback(rows); } }) } } session Session 是一种记录客户状态的机制，不同的是 Cookie 保存在客户端浏览器中，而 Session 保存在服务器上的进程中。\n客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上，这就是 Session。客户端浏览器再次访问时只需要从该 Session 中查找该客户的状态就可以了。\n如果说 Cookie 机制是通过检查客户身上的“通行证”来确定客户身份的话，那么 Session 机制就是通过检查服务器上的“客户明细表”来确认客户身份。\nSession 相当于程序在服务器上建立的一份客户档案，客户来访的时候只需要查询客户档案表就可以了。\nSession 不能跨域。\nnode操作session和cookie也很简单，也是通过中间件的形式。\nconst express = require('express') const path = require('path') const app = express(); const bodyParser = require('body-parser'); const cp = require('cookie-parser'); const session = require('express-session'); app.use(cp()); app.use(session({ secret: '12345',//用来对session数据进行加密的字符串.这个属性值为必须指定的属性 name: 'testapp', //这里的name值得是cookie的name，默认cookie的name是：connect.sid cookie: {maxAge: 5000 }, //设置maxAge是5000ms，即5s后session和相应的cookie失效过期 resave: false, saveUninitialized: true, })) app.use(bodyParser.urlencoded({extended: false})); app.use(express.static(path.join(__dirname, '/'))); app.get('/setsession', (request, response) =\u0026gt; { request.session.user = {username: 'admin'}; response.send('set session success'); }) app.get('/getsession', (request, response) =\u0026gt; { response.send(request.session.user); }) app.get('/delsession', (request, response) =\u0026gt; { delete reqeust.session.user; response.send(request.session.user); }) app.listen(88) Token Token的特点\n 随机性 不可预测性 时效性 无状态、可扩展 跨域  基于Token的身份验证场景\n 客户端使用用户名和密码请求登录 服务端收到请求，验证登录是否成功 验证成功后，服务端会返回一个 Token 给客户端，反之，返回身份验证失败的信息 客户端收到 Token 后把 Token 用一种方式(cookie/localstorage/sessionstorage/其他)存储起来 客户端每次发起请求时都选哦将 Token 发给服务端 服务端收到请求后，验证Token的合法性，合法就返回客户端所需数据，反之，返回验证失败的信息  Token 身份验证实现 —— jsonwebtoken\n先安装第三方模块 jsonwebtoken npm install jsonwebtoken\nconst express = require('express') const path = require('path') const app = express(); const bodyParser = require('body-parser'); const jwt = require('jsonwebtoken'); app.use(bodyParser.urlencoded({extended: false})); app.use(express.static(path.join(__dirname, '/'))); app.all('*', function(req, res, next) { res.header(\u0026quot;Access-Control-Allow-Origin\u0026quot;, \u0026quot;*\u0026quot;); res.header(\u0026quot;Access-Control-Allow-Headers\u0026quot;, \u0026quot;Content-Type,Content-Length, Auth, Accept,X-Requested-With\u0026quot;); res.header(\u0026quot;Access-Control-Allow-Methods\u0026quot;,\u0026quot;PUT,POST,GET,DELETE,OPTIONS\u0026quot;); res.header(\u0026quot;X-Powered-By\u0026quot;,' 3.2.1') if(req.method==\u0026quot;OPTIONS\u0026quot;) { res.sendStatus(200);/*让options请求快速返回*/ } else{ next(); } }); app.get('/createtoken', (request, response) =\u0026gt; { //要生成 token 的主题信息 let user = { username: 'admin', } //这是加密的 key（密钥） let secret = 'dktoken'; //生成 Token let token = jwt.sign(user, secret, { 'expiresIn': 60*60*24 // 设置过期时间, 24 小时 }) response.send({status: true, token}); }) app.post('/verifytoken', (request, response) =\u0026gt; { //这是加密的 key（密钥），和生成 token 时的必须一样 let secret = 'dktoken'; let token = request.headers['auth']; if(!token){ response.send({status: false, message: 'token不能为空'}); } jwt.verify(token, secret, (error, result) =\u0026gt; { if(error){ response.send({status: false}); } else { response.send({status: true, data: result}); } }) }) app.listen(88) web socket HTTP 协议可以总结几个特点：\n 一次性的、无状态的短连接：客户端发起请求、服务端响应、结束。 被动性响应：只有当客户端请求时才被执行，给予响应，不能主动向客户端发起响应。 信息安全性：得在服务器添加 SSL 证书，访问时用 HTTPS。 跨域：服务器默认不支持跨域，可在服务端设置支持跨域的代码或对应的配置。  TCP 协议可以总结几个特点：\n 有状态的长连接：客户端发起连接请求，服务端响应并建立连接，连接会一直保持直到一方主动断开。 主动性：建立起与客户端的连接后，服务端可主动向客户端发起调用。 信息安全性：同样可以使用 SSL 证书进行信息加密，访问时用 WSS 。 跨域：默认支持跨域。  安装第三方模块 ws：npm install ws\n开启一个 WebSocket 的服务器，端口为 8080\nvar socketServer = require('ws').Server; var wss = new socketServer({ port: 8080 }); 也可以利用 Express 来开启 WebSocket 的服务器\nvar app = require('express')(); var server = require('http').Server(app); var socketServer = require('ws').Server; var wss = new socketServer({server: server, port: 8080});  用 on 来进行事件监听 connection：连接监听，当客户端连接到服务端时触发该事件 close：连接断开监听，当客户端断开与服务器的连接时触发 message：消息接受监听，当客户端向服务端发送信息时触发该事件 send: 向客户端推送信息  soket.io 可以理解为对 WebSocket 的一种封装。好比前端的 jQuery 对原生 javascript 的封装。 soket.io 依靠事件驱动的模式，灵活的使用了自定义事件和调用事件来完成更多的场景，不必依赖过多的原生事件。\n 安装第三方模块 npm install express socket.io 开户 Socket 服务器，端口为 88  var express = require('express'); var app = express(); var http = require('http').Server(app); var io = require('socket.io')(http); http.listen(88);  用 on 来进行事件监听和定义事件 connection：监听客户端连接,回调函数会传递本次连接的socket emit：触发用客户端的事件  io.on('connection', function(client){ //把当前登录的用户保存到对象 onlinePersons，并向所有在线的用户发起上线提示 //serverLogin 为自定义事件，供客户端调用 client.on('serverLogin', function(_person){ var _personObj = JSON.parse(_person); onlinePersons[_personObj.id] = _personObj; //向所有在线的用户发起上线提示 //触发客户端的 clientTips 事件 //clientTips 为客户端的自定义事件 io.emit('clientTips', JSON.stringify(onlinePersons)); }) //当监听到客户端有用户在移动，就向所有在线用户发起移动信息，触发客户端 clientMove 事件 //serverMove 为自定义事件，供客户端调用 client.on('serverMove', function(_person){ var _personObj = JSON.parse(_person); onlinePersons[_personObj.id] = _personObj; console.log('serverLogin', onlinePersons); //clientTips 为客户端的自定义事件 io.emit('clientMove', _person); }); }) kafka-node node也可以去读写kafka，而且很简单。只需要引入kafka的库即可。\nyarn add kafka-node 具体api可以看文档：https://github.com/SOHU-Co/kafka-node\n生产者\nvar kafka = require('..'); var Producer = kafka.Producer; var KeyedMessage = kafka.KeyedMessage; var Client = kafka.Client; var client = new Client('localhost:2181'); var argv = require('optimist').argv; var topic = argv.topic || 'topic1'; var p = argv.p || 0; var a = argv.a || 0; var producer = new Producer(client, { requireAcks: 1 }); producer.on('ready', function () { var message = 'a message'; var keyedMessage = new KeyedMessage('keyed', 'a keyed message'); producer.send([ { topic: topic, partition: p, messages: [message, keyedMessage], attributes: a } ], function (err, result) { console.log(err || result); process.exit(); }); }); producer.on('error', function (err) { console.log('error', err); }); 消费者\n'use strict'; var kafka = require('..'); var Consumer = kafka.Consumer; var Offset = kafka.Offset; var Client = kafka.Client; var argv = require('optimist').argv; var topic = argv.topic || 'topic1'; var client = new Client('localhost:2181'); var topics = [{ topic: topic, partition: 1 }, { topic: topic, partition: 0 }]; var options = { autoCommit: false, fetchMaxWaitMs: 1000, fetchMaxBytes: 1024 * 1024 }; var consumer = new Consumer(client, topics, options); var offset = new Offset(client); consumer.on('message', function (message) { console.log(message); }); consumer.on('error', function (err) { console.log('error', err); }); /* * If consumer get `offsetOutOfRange` event, fetch data from the smallest(oldest) offset */ consumer.on('offsetOutOfRange', function (topic) { topic.maxNum = 2; offset.fetch([topic], function (err, offsets) { if (err) { return console.error(err); } var min = Math.min.apply(null, offsets[topic.topic][topic.partition]); consumer.setOffset(topic.topic, topic.partition, min); }); }); Node单元测试 以function为最小单位，验证特定情况下的input和output是否正确。\n 防止改A坏B，避免不能跑的代码比能跑的还多。 明确指出问题所在、告知正确的行为是什么，减少debug的时间。  对于node来说，单元测试也很容易做。\n测试主要分为两种，TDD和BDD。\nTDD VS. BDD 比较TDD 与BDD 的差异。\n    TDD BDD     全名 测试驱动开发Test-Driven Development 行为驱动开发Behavior Driven Development   定义 在开发前先撰写测试程式，以确保程式码品质与符合验收规格。 TDD的进化版。除了实作前先写测试外，还要写一份「可以执行的规格」。   特性 从测试去思考程式如何实作。强调小步前进、快速且持续回馈、拥抱变化、重视沟通、满足需求。 从用户的需求出发，强调系统行为。使用自然语言描述测试案例，以减少使用者和工程师的沟通成本。测试后的输出结果可以直接做为文件阅读。    从代码层面来看：\nTDD\nsuite('Array', ()=\u0026gt;{ setup(()={ }); test('equal -1 when index beyond array length', ()=\u0026gt;{ assert.equal(-1, [1,2,3].indexOf(4)); }); }) BDD\ndescribe('Array', function() { before(function() { }); it('should return -1 when no such index', function() { [1,2,3].indexOf(4).should.equal(-1); }); }); 对比了这两种类型的语法之后，我选择了BDD。\n测试框架实践 在node社区，比较成熟的是mocha。mocha本身是不提供断言库的，一般来说断言库比较常用的是chai。mocha和chai，合起来就被戏称为抹茶。\nmocha一般需要全局安装，chai安装到项目目录下即可。\nyarn global add mocha yarn add chai mocha 语法说明  describe()：描述场景或圈出特定区块，例如：标明测试的功能或function。 it()：撰写测试案例（Test Case）。 before()：在所有测试开始前会执行的代码。 after()：在所有测试结束后会执行的代码。 beforeEach()：在每个Test Case 开始前执行的代码。 afterEach()：在每个Test Case 结束后执行的代码。  代码示例 describe('hooks', function() { before(function() { }); after(function() { }); beforeEach(function() { }); afterEach(function() { }); it('should ...', function() { }); }); chai assert assert(expression, message)：测试这个项目的expression是否为真，若为假则显示错误消息message。\nExpect / Should 预期3 等于（===）2。这是使用可串连的操作符 来完成断言。这些可串联的有to、is、have 等。它很像英文，用很口语的方式做判断。\n覆盖率 既然是给功能代码写单元测试，那就应该有个指标去衡量单元测试覆盖了哪些功能代码，这就是接下来要介绍的测试覆盖率。\n在 Node.js 中，我们使用 istanbul 作为覆盖率统计的工具，istanbul 可以帮助我们统计到代码的语句覆盖率、分支覆盖率、函数覆盖率以及行覆盖率。\n全局安装：\nyarn global add istanbul 只需要使用istanbul cover就可以得到覆盖率。\nistanbul cover simple.js 可以和mocha配合使用：\nisbuntal cover _mocha test/simple-test.js mocha 和 _mocha 是两个不同的命令，前者会新建一个进程执行测试，而后者是在当前进程（即 istanbul 所在的进程）执行测试，只有这样， istanbul 才会捕捉到覆盖率数据。其他测试框架也是如此，必须在同一个进程执行测试。\n引入typescript typescript其实就是加了类型的js。\n所谓类型，就是约定变量的内存布局。js作为一个动态弱类型的语言，在开发大型项目的时候，不免可能出现问题，所以有类型的语言可以在编译期就能检测到错误，减少debug的时间。\n安装 yarn global add typescript 新项目引入ts 现在新建文件server.ts：\nimport * as http from \u0026#39;http\u0026#39;; const server = http.createServer(function (req, res) { res.end(\u0026#39;Hello, world\u0026#39;); }); server.listen(3000, function () { console.log(\u0026#39;server is listening\u0026#39;); }); 为了能执行此文件，需要通过 tsc 命令来编译该 TypeScript 源码：\ntsc server.ts 如果没有什么意外的话，此时控制台会打印出以下的出错信息：\nserver.ts(1,23): error TS2307: Cannot find module 'http'. 这表示没有找到http这个模块定义（TyprScript 编译时是通过查找模块的 typings 声明文件来判断模块是否存在的，而不是根据真实的 js 文件，下文会详细解释），但是我们当前目录下还是生成了一个新的文件server.js，我们可以试着执行它：\nnode server.js 如果一切顺利，那么控制台将会打印出 server is listening 这样的信息，并且我们在浏览器中访问 http://127.0.0.1:3000时也能看到正确的结果：Hello, world\n现在再回过头来看看刚才的编译错误信息。由于这是一个 Node.js 项目，typescript 语言中并没有定义http这个模块，所以我们需要安装 Node.js 运行环境的声明文件：\nyarn global add @types/node 安装完毕之后，再重复上文的编译过程，此时 tsc 不再报错了。\n大多数时候，为了方便我们可以直接使用 ts-node 命令执行 TypeScript 源文件而不需要预先编译。首先执行以下命令安装 ts-node：\nyarn global add -g ts-node 然后使用 ts-node 命令执行即可：\nts-node --no-cache server.ts 说明：使用 ts-node 执行 TypeScript 程序时，为了提高编译速度，默认会缓存未修改过的 .ts 文件，但有时候会导致一些 Bug，所以建议启动时加上 --no-cache 参数。\ntsconfig.json 配置文件 每个 TypeScript 项目都需要一个 tsconfig.json 文件来指定相关的配置，比如告诉 TypeScript 编译器要将代码转换成 ES5 还是 ES6 代码等。\n可以使用tsc命令生成。\ntsc --init 使用第三方模块 一般情况下在 TypeScript 中是不能”直接“使用 npm 上的模块的，比如我们要使用 express 模块，先执行以下命令安装：\nyarn add express 然后新建文件 server.ts :\nimport * as express from \u0026#39;express\u0026#39;; const app = express(); app.get(\u0026#39;/\u0026#39;, function (req, res) { res.end(\u0026#39;hello, world\u0026#39;); }) app.listen(3000, function () { console.log(\u0026#39;server is listening\u0026#39;); }); 然后使用以下命令执行：\nts-node server.ts 如果不出意外，我们将会看到这样的报错信息：\nsrc/server.ts(1,26): error TS7016: Could not find a declaration file for module 'express'. 报错的信息表明没有找到express模块的声明文件。由于 TypeScript 项目最终会编译成 JavaScript 代码执行，当我们在 TypeScript 源码中引入这些被编译成 JavaScript 的模块时，它需要相应的声明文件（.d.ts文件）来知道该模块类型信息，这些声明文件可以通过设置tsconfig.json中的declaration: true来自动生成。而那些不是使用 TypeScript 编写的模块，也可以通过手动编写声明文件来兼容 TypeScript。\n当遇到缺少模块声明文件的情况，开发者可以尝试通过 yarn addl @types/xxx 来安装模块声明文件即可。\n现在我们尝试执行以下命令安装 express 模块的声明文件：\nyarn add @types/express 没有意外，果然能成功安装。现在再通过 ts-node 来执行的时候，发现已经没有报错了。\n单元测试 直接使用mocha和chai，进行ts的测试。\n旧项目迁移 通常来说这个过程包括了以下步骤：\n 添加 tsconfig.json 将你的源代码文件扩展名从 .js 改成 .ts。使用 any 来开始抑止错误。 使用 TypeScript 来编写新的代码并且尽可能少地使用 any。 返回到旧代码里并且开始加入类型标注和解决发现的 bugs。 为第三方 JavaScript 代码使用环境定义。  ","permalink":"https://zhenfeng-zhu.github.io/posts/node-learning/","summary":"写node也有一段时间了，整理一下学习笔记，共同进步\n什么是node？ 首先看一下什么是node.js\n Node 是一个服务器端 JavaScript Node.js 是一个基于 Chrome V8 引擎的 JavaScript 运行环境 Node.js 使用了一个事件驱动、非阻塞式 I/O 的模型，使其轻量又高效 Node.js 的包管理器 npm，是全球最大的开源库生态系统  模块系统是node最基本也是最常用的。一般可以分为四类：\n 原生模块 文件模块 第三方模块 自定义模块  node社区崇尚DRY文化，即Don\u0026rsquo;t repeate yourself。这种文化使得node的生态异常繁荣，同样也由于某些包的质量低下引来了一些诟病。\n谈谈自定义模块 我们在写node程序的时候，一般都是在写自定义模块。\n  创建模块\n// b.js function FunA(){ return \u0026quot;hello world\u0026quot;; } // 暴露方法FunA module.exports = FunA;   加载模块\n// a.js const FunA=require('./b.js'); // 运行FunA const name=FunA(); console.log(name);   在做模块到处的时候有两种方式：\n  module.exports\nmodule.exports 就 Node.","title":"node学习笔记"},{"content":"Node.js Redis客户端模块 为了追新，这里我使用的yarn，毕竟我是HDD（面向热点编程）编程实践者。\n模块安装\nyarn add redis 模块使用实例\nconst redis = require('redis') const client = redis.createClient('6379', '127.0.0.1') client.on(\u0026quot;error\u0026quot;, function (err) { console.log(\u0026quot;Error \u0026quot; + err); }); client.set(\u0026quot;string key\u0026quot;, \u0026quot;string val\u0026quot;, redis.print); client.hset(\u0026quot;hash key\u0026quot;, \u0026quot;hashtest 1\u0026quot;, \u0026quot;some value\u0026quot;, redis.print); client.hset([\u0026quot;hash key\u0026quot;, \u0026quot;hashtest 2\u0026quot;, \u0026quot;some other value\u0026quot;], redis.print); client.hkeys(\u0026quot;hash key\u0026quot;, function (err, replies) { console.log(replies.length + \u0026quot; replies:\u0026quot;); replies.forEach(function (reply, i) { console.log(\u0026quot; \u0026quot; + i + \u0026quot;: \u0026quot; + reply); }); client.quit(); }); 输出的结果如下所示：\n➜ node-example git:(master) ✗ node redis-demo.js Reply: OK Reply: 0 Reply: 0 2 replies: 0: hashtest 1 1: hashtest 2 Promises\n如果是使用node 8或者之上的话，使用node的util.promisify来将请求变成promise的。\nconst {promisify}=require('util') const redis = require('redis') const client = redis.createClient('6379', '127.0.0.1') const getAsync=promisify(client.get).bind(client) function getFoo(){ return getAsync('foo').then(res =\u0026gt; { console.log(res) }) } getFoo() 发送命令\n每个redis命令都会通过client对象的一个函数暴露，所有这些函数都会有一个args数组选项和一个callback回调函数。\n字符串操作\nset key value\nget key\n哈希操作\nhmset key field1 value1\nhget key field1 value1\n列表操作\nlpush key value1 value2\nlrange key 0 n\n集合操作\nsadd key member1 member2\nsmembers key\n有序集合操作\nzadd key index value\nzrange key 0 n\n","permalink":"https://zhenfeng-zhu.github.io/posts/node%E7%9A%84redis%E5%AE%9E%E6%88%98/","summary":"Node.js Redis客户端模块 为了追新，这里我使用的yarn，毕竟我是HDD（面向热点编程）编程实践者。\n模块安装\nyarn add redis 模块使用实例\nconst redis = require('redis') const client = redis.createClient('6379', '127.0.0.1') client.on(\u0026quot;error\u0026quot;, function (err) { console.log(\u0026quot;Error \u0026quot; + err); }); client.set(\u0026quot;string key\u0026quot;, \u0026quot;string val\u0026quot;, redis.print); client.hset(\u0026quot;hash key\u0026quot;, \u0026quot;hashtest 1\u0026quot;, \u0026quot;some value\u0026quot;, redis.print); client.hset([\u0026quot;hash key\u0026quot;, \u0026quot;hashtest 2\u0026quot;, \u0026quot;some other value\u0026quot;], redis.print); client.hkeys(\u0026quot;hash key\u0026quot;, function (err, replies) { console.log(replies.length + \u0026quot; replies:\u0026quot;); replies.forEach(function (reply, i) { console.log(\u0026quot; \u0026quot; + i + \u0026quot;: \u0026quot; + reply); }); client.","title":"node的redis实战"},{"content":"这篇文章打的标签比较多，也基本涵盖了我所了解的一些知识，归纳总结一下自己对web框架的理解。自己了解的也不是很多，也请多多指教。\n写程序免不了要做web相关的，现在由于前后端的分离，后端一般只提供rest接口，前端一般使用node来做渲染。在之前使用jsp那一套的时候，基本上都要写html+js的前端的一套，也要写后端java的CRUD。\n我理解的web框架中，大致是分为这么两类：\n router框架 mvc框架  mvc类框架 mvc，初级程序员面试笔试的时候必考的一个知识点。model-view-controller，即模型-视图-控制器。\n m，模型主要用于封装与应用程序相关的数据以及对数据的处理方法。 v，在 View 中一般没有程序上的逻辑。为了实现 View 上的刷新功能，View 需要访问它监视的数据模型（Model），因此应该事先在被它监视的数据那里注册。 c，用于控制应用程序的流程。  我了解比较多的mvc框架是spring mvc。spring、spring mvc和spring boot等，他们并不是一个概念，也不是仅仅用于web开发。但是在这里我就不分那么细，统一用spring来代替。这里所说的spring都是指狭义上的web开发方面。\n在做web开发的时候，项目目录一般是这样的：\n $ tree [16:23:43] . ├── mvnw ├── mvnw.cmd ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── example │ │ └── demo │ │ └── DemoApplication.java │ └── resources │ ├── application.properties │ ├── static │ └── templates └── test └── java └── com └── example └── demo └── DemoApplicationTests.java 14 directories, 6 files 要渲染页面的时候，会把相关的类写在controller包下面，然后使用@Controlle注解表示这是一个controller。\n如果是实体类，一般会放在entity包或者domain包中。\n对数据库进行操作的类，一般会放在repository或者dao中。\ncontroller一般不直接使用dao，而是会单独写一个service负责去做一些其他的事情。\n每个包分工明确。\n而url的路由拦截处理是在controller了中去实现的。\nrouter框架 我理解的router框架主要是以express为代表的框架。现在的轻量级的web框架会有路由这么一个重要的概念。\n路由用于确定应用程序如何响应对特定端点的客户机请求，包含一个 URI（或路径）和一个特定的 HTTP 请求方法（GET、POST 等）。\n每个路由可以具有一个或多个处理程序函数，这些函数在路由匹配时执行。\n路由一般采用如下的结构：\nrouter.METHOD(PATH, HANDLER) 其中：\n router是路由实例。 METHOD是http的请求方法，如GET，POST等。 PATH是URL请求路径。 HANDLER是一个回调函数，在路由匹配成功时执行的。  可以发现，在router类框架中，handler是一个很常用的，这是一种编程的模式——行为参数化。\njava的vertx和go的gin框架也是这样一种思路。\n下面这个示例是gin的：\npackage main import ( \u0026quot;github.com/gin-gonic/gin\u0026quot; \u0026quot;net/http\u0026quot; ) func main() { router:=gin.Default() router.GET(\u0026quot;/hello\u0026quot;, greeting) // 也可以写成匿名函数 router.GET(\u0026quot;/\u0026quot;, func(context *gin.Context) { context.String(http.StatusOK, \u0026quot;I am Lucas\u0026quot;) }) router.Run() } // 可以单独写一个handler函数 func greeting(context *gin.Context) { context.String(http.StatusOK, \u0026quot;hello world\u0026quot;) } 可以看到router类的框架特别轻量级，而且很适合写rest api接口。\nrouter的原理 一般情况下，router使用的数据结构是radix tree，压缩字典树。\n字典树是一个比较常用的数据结构，下图是一个典型的字典树结构：\n字典树一般用来进行字符串的检索。对于目标字符串，只要从根节点开始深度优先搜索，即可判断出该字符串是否曾经出现过，时间复杂度为 O(n)，n 可以认为是目标字符串的长度。为什么要这样做？字符串本身不像数值类型可以进行数值比较，两个字符串对比的时间复杂度取决于字符串长度。如果不用字典树来完成上述功能，要对历史字符串进行排序，再利用二分查找之类的算法去搜索，时间复杂度只高不低。可认为字典树是一种空间换时间的典型做法。\n普通的字典树有一个比较明显的缺点，就是每个字母都需要建立一个孩子节点，这样会导致字典树的层树比较深，压缩字典树相对好地平衡了字典树的优点和缺点。下图是典型的压缩字典树结构：\n每个节点上不只存储一个字母了，这也是压缩字典树中“压缩”的主要含义。使用压缩字典树可以减少树的层数，同时因为每个节点上数据存储也比通常的字典树要多，所以程序的局部性较好(一个节点的 path 加载到 cache 即可进行多个字符的对比)，从而对 CPU 缓存友好。\n中间件 对于大多数的场景来讲，非业务的需求都是在 http 请求处理前做一些事情，或者/并且在响应完成之后做一些事情。我们有没有办法使用一些重构思路把这些公共的非业务功能代码剥离出去呢？\n这个时候就是就引入了中间件的概念。\n中间件函数能够访问请求对象 (req)、响应对象 (res) 以及应用程序的请求/响应循环中的下一个中间件函数。\n中间件函数可以执行以下任务：\n 执行任何代码。 对请求和响应对象进行更改。 结束请求/响应循环。 调用堆栈中的下一个中间件。  这些中间件其实就是一些可插拔的函数组件，对请求和响应的对象进行封装处理。\n哪些事情适合在中间件中做 以较流行的开源 go 框架 chi 为例：\ncompress.go =\u0026gt; 对 http 的 response body 进行压缩处理 heartbeat.go =\u0026gt; 设置一个特殊的路由，例如 /ping，/healthcheck，用来给 load balancer 一类的前置服务进行探活 logger.go =\u0026gt; 打印 request 处理日志，例如请求处理时间，请求路由 profiler.go =\u0026gt; 挂载 pprof 需要的路由，如 /pprof、/pprof/trace 到系统中 realip.go =\u0026gt; 从请求头中读取 X-Forwarded-For 和 X-Real-IP，将 http.Request 中的 RemoteAddr 修改为得到的 RealIP requestid.go =\u0026gt; 为本次请求生成单独的 requestid，可一路透传，用来生成分布式调用链路，也可用于在日志中串连单次请求的所有逻辑 timeout.go =\u0026gt; 用 context.Timeout 设置超时时间，并将其通过 http.Request 一路透传下去 throttler.go =\u0026gt; 通过定长大小的 channel 存储 token，并通过这些 token 对接口进行限流 我们可以发现，一些通用的非业务场景的都可以用中间件来包裹。\nspring有AOP这个大杀器，它采用动态代理的方式也可以实现中间件的行为。\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E8%B0%88%E8%B0%88web%E6%A1%86%E6%9E%B6/","summary":"这篇文章打的标签比较多，也基本涵盖了我所了解的一些知识，归纳总结一下自己对web框架的理解。自己了解的也不是很多，也请多多指教。\n写程序免不了要做web相关的，现在由于前后端的分离，后端一般只提供rest接口，前端一般使用node来做渲染。在之前使用jsp那一套的时候，基本上都要写html+js的前端的一套，也要写后端java的CRUD。\n我理解的web框架中，大致是分为这么两类：\n router框架 mvc框架  mvc类框架 mvc，初级程序员面试笔试的时候必考的一个知识点。model-view-controller，即模型-视图-控制器。\n m，模型主要用于封装与应用程序相关的数据以及对数据的处理方法。 v，在 View 中一般没有程序上的逻辑。为了实现 View 上的刷新功能，View 需要访问它监视的数据模型（Model），因此应该事先在被它监视的数据那里注册。 c，用于控制应用程序的流程。  我了解比较多的mvc框架是spring mvc。spring、spring mvc和spring boot等，他们并不是一个概念，也不是仅仅用于web开发。但是在这里我就不分那么细，统一用spring来代替。这里所说的spring都是指狭义上的web开发方面。\n在做web开发的时候，项目目录一般是这样的：\n $ tree [16:23:43] . ├── mvnw ├── mvnw.cmd ├── pom.xml └── src ├── main │ ├── java │ │ └── com │ │ └── example │ │ └── demo │ │ └── DemoApplication.java │ └── resources │ ├── application.properties │ ├── static │ └── templates └── test └── java └── com └── example └── demo └── DemoApplicationTests.","title":"谈谈web框架"},{"content":"创业公司真的比较锻炼人，接触了很多的东西，视野开阔了，但是在某些时候自己疲于奔命，每个东西都是接触了一点点就被赶鸭子上架开始开发了。\n技术栈  Docker  docker是一个容器，以前就看过docker相关的东西，但是没有仔细研究，docker的命令会用一些，在工作中使用了，看了一本docker的书，能够编写docker的compose文件。\n rancher  rancher是一个做容器管理的。我们把主机添加到rancher中，他就可以自动做到LB，服务的发现编排。我们部署的时候只需要编写catalog，他就可以自动发现docker应用，然后拉取镜像，部署到相关的机器上，很是方便。\n aws  近期主要是对aws的进行公司服务的部署，搭建一套rancher的环境。aws的服务特别多，ec2是实例主机，就和虚拟机一样，VPC就像机房，ec2依托于VPC而存在，在这基础上又了解了子网、DHCP弹性IP等等。\n kotlin  之前自己用kotlin开发过一个博客，对kotlin的感觉是有些东西写的很爽，但是还是觉得java好用一些，对kotlin的态度是用不用都无所谓。\n guice  这个我之前都读错了，我读成了盖斯，其实是和果汁的英文发音很像，ju斯。只是一个依赖注入框架，只是单纯的去做DI，比spring更轻量级一些。\n需要我们编写AppModule.java去手动配置哪个类注入哪个类。\n rxjava  rxjava我都没有找到一个系统的教程，不知道该从哪里学习。\n vertx  vertx+Reactive编程的方式相当考验心智，自己脑子中的编程方式还没转过来。\n ES6  node代码中都是用es6来写的，async和await现在也会用了。\n express  想到自己大学的时候看过node的书，里面讲的就是express，只是自己当时没想明白，现在看的回调多了，算是熟悉了他这种的编程模式，所以觉得express特别简单易上手。\n typescript  还没它去写东西，可能下周会用它来写个机器人。\n貌似自己已经完全抛弃了spring+java的那一套东西，没机会用到。\n知识面扩展  监控  grafana+Prometheus+graylog去做可视化和日志的监控。\n对于业务的数据，需要在代码层面进行埋点，把要监控的数据传给普罗米修斯。\n FSM状态机  机器人的框架使用的是FSM状态机来管理，以前做游戏的时候接触过。\n 网关  网关现在已经是微服务架构中的标配了，用它来做一些限流，LB和日志收集等等。\n我们使用的是kong，在这里kong加上一些插件，相当好用。\n 规则引擎  在规则引擎中，都是一个个规则。\n DSL  领域特定语言，在规则引擎和机器人的时候，就用了DSL。我现在的理解就是DSL就是用编程语言实现的一些函数。\n Key Transparency  谷歌的一个公钥管理库，保证了无法被篡改。\n未来可能要接触  go  ","permalink":"https://zhenfeng-zhu.github.io/posts/%E6%8A%80%E6%9C%AF%E6%A0%88/","summary":"创业公司真的比较锻炼人，接触了很多的东西，视野开阔了，但是在某些时候自己疲于奔命，每个东西都是接触了一点点就被赶鸭子上架开始开发了。\n技术栈  Docker  docker是一个容器，以前就看过docker相关的东西，但是没有仔细研究，docker的命令会用一些，在工作中使用了，看了一本docker的书，能够编写docker的compose文件。\n rancher  rancher是一个做容器管理的。我们把主机添加到rancher中，他就可以自动做到LB，服务的发现编排。我们部署的时候只需要编写catalog，他就可以自动发现docker应用，然后拉取镜像，部署到相关的机器上，很是方便。\n aws  近期主要是对aws的进行公司服务的部署，搭建一套rancher的环境。aws的服务特别多，ec2是实例主机，就和虚拟机一样，VPC就像机房，ec2依托于VPC而存在，在这基础上又了解了子网、DHCP弹性IP等等。\n kotlin  之前自己用kotlin开发过一个博客，对kotlin的感觉是有些东西写的很爽，但是还是觉得java好用一些，对kotlin的态度是用不用都无所谓。\n guice  这个我之前都读错了，我读成了盖斯，其实是和果汁的英文发音很像，ju斯。只是一个依赖注入框架，只是单纯的去做DI，比spring更轻量级一些。\n需要我们编写AppModule.java去手动配置哪个类注入哪个类。\n rxjava  rxjava我都没有找到一个系统的教程，不知道该从哪里学习。\n vertx  vertx+Reactive编程的方式相当考验心智，自己脑子中的编程方式还没转过来。\n ES6  node代码中都是用es6来写的，async和await现在也会用了。\n express  想到自己大学的时候看过node的书，里面讲的就是express，只是自己当时没想明白，现在看的回调多了，算是熟悉了他这种的编程模式，所以觉得express特别简单易上手。\n typescript  还没它去写东西，可能下周会用它来写个机器人。\n貌似自己已经完全抛弃了spring+java的那一套东西，没机会用到。\n知识面扩展  监控  grafana+Prometheus+graylog去做可视化和日志的监控。\n对于业务的数据，需要在代码层面进行埋点，把要监控的数据传给普罗米修斯。\n FSM状态机  机器人的框架使用的是FSM状态机来管理，以前做游戏的时候接触过。\n 网关  网关现在已经是微服务架构中的标配了，用它来做一些限流，LB和日志收集等等。\n我们使用的是kong，在这里kong加上一些插件，相当好用。\n 规则引擎  在规则引擎中，都是一个个规则。\n DSL  领域特定语言，在规则引擎和机器人的时候，就用了DSL。我现在的理解就是DSL就是用编程语言实现的一些函数。\n Key Transparency  谷歌的一个公钥管理库，保证了无法被篡改。","title":"技术栈"},{"content":"具体方法 Configuring a remote for a fork  给 fork 配置一个 remote 主要使用 git remote -v 查看远程状态。  git remote -v # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)  添加一个将被同步给 fork 远程的上游仓库  git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git  再次查看状态确认是否配置成功。  git remote -v # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Syncing a fork  从上游仓库 fetch 分支和提交点，传送到本地，并会被存储在一个本地分支 upstream/master git fetch upstream  git fetch upstream # remote: Counting objects: 75, done. # remote: Compressing objects: 100% (53/53), done. # remote: Total 62 (delta 27), reused 44 (delta 9) # Unpacking objects: 100% (62/62), done. # From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY # * [new branch] master -\u0026gt; upstream/master  切换到本地主分支(如果不在的话) git checkout master  git checkout master # Switched to branch 'master'  把 upstream/master 分支合并到本地 master 上，这样就完成了同步，并且不会丢掉本地修改的内容。 git merge upstream/master  git merge upstream/master # Updating a422352..5fdff0f # Fast-forward # README | 9 ------- # README.md | 7 ++++++ # 2 files changed, 7 insertions(+), 9 deletions(-) # delete mode 100644 README # create mode 100644 README.md  如果想更新到 GitHub 的 fork 上，直接 git push origin master 就好了。  ","permalink":"https://zhenfeng-zhu.github.io/posts/%E5%90%8C%E6%AD%A5%E4%B8%80%E4%B8%AA-fork/","summary":"具体方法 Configuring a remote for a fork  给 fork 配置一个 remote 主要使用 git remote -v 查看远程状态。  git remote -v # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)  添加一个将被同步给 fork 远程的上游仓库  git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git  再次查看状态确认是否配置成功。  git remote -v # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch) # origin https://github.com/YOUR_USERNAME/YOUR_FORK.git (push) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Syncing a fork  从上游仓库 fetch 分支和提交点，传送到本地，并会被存储在一个本地分支 upstream/master git fetch upstream  git fetch upstream # remote: Counting objects: 75, done.","title":"同步一个 fork"},{"content":"最近公司要统一技术栈，在kotlin和go之间选。我心里是比较倾向go的，主要有如下几点体会。\n 语言简单，上手快。 gorotuine 易发布 垃圾回收 约定大于配置  我最早听说协程，是在大三找实习的时候，那个时候面试会问线程和进程的关系，问的深一些就是协程和线程的区别。游戏公司基本都用lua，看了lua的资料后，对协程有了一些自己的了解，随后就是在做Unity相关的开发，在unity中使用了很多的协程，但是在unity中使用的协程好像跟主流的不太一样，在看了go之后，豁然开朗。\ngoroutine使用的内存比线程更少，go在运行的时候会自动在配置的一组逻辑处理器上调度执行。比如：\nfunc log(msg string){ ... } go log(\u0026quot;\u0026quot;) 使用关键字go，即可让log函数在一个goroutine里执行了。\n并发最难的部分是要确保其他并发运行的进程、线程或者goroutine不会以外的修改数据。go使用了Channel的方式来解决这个问题。对于通道模式，保证同一时刻只会有一个goroutine修改数据。\n说起go的语言简单，其实主要是他的类型比较简单。go使用的是组合模式，只需要将一个类型嵌入到另外一个类型就可以复用所有的功能。而且go还具有独特的接口实现机制，允许用户对行为进行建模，在go中不需要声明某个类型实现了某个接口，编译器会自动判断一个实例是使用什么接口。\n对于java来说，所有的设计都是围绕着接口展开，于是在设计模式中，就是面向接口编程：\ninterface User{ void login(); void logout(); } 在java中，继承的类必须显式声明继承了此接口。而在go中接口只是描述一个动作，如果说是实现这个接口，只需要让某个实例实现了这个接口中的所有方法就行了。\ntype Reader interface{ Read(p []byte))(n int, err error) } 这其实和传统的oop语言的接口有着本质的区别，go中的接口一般只定义一个单一的动作，实际使用的过程中，这更有利于使用组合来复用代码。\n约定大于配置这点，go在这方面上做的感觉有点儿吹毛求疵了，但是这样也使得程序可读性更强，没有很多垃圾代码。比如go的文件结构必须是src pkg 和bin 三个包，而且go也不允许你声明一个变量却不使用，导入了一个包却不使用，而且程序的代码也有约定，init方法比main方法更早执行。\ngo的并发 说到并发，就会想到另外一个概念，并行。可以简单这样的理解：\n并发是同时管理多个事情，而并行是同时做很多事情。也就是并发是manage，并行是run。 对于单核处理器来讲，同一时刻只能有一个任务在执行，那么并发就是同时管理多个任务，让他们交替执行。并行是针对于多核处理器的，同一时刻可以把多个任务放在不同的处理器上执行，这样就可以同时执行。\n在go里面主要是采用协程来实现并发的，也就是goroutine。与其他语言不同的是，go是在语法层面做到的，即go func();\n语法 go f(x, y) go是关键字，后面跟函数。\n例子 package main import ( \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; ) func doSomething(id int) { log.Printf(\u0026quot;before do job:(%d) \\n\u0026quot;, id) time.Sleep(3 * time.Second) log.Printf(\u0026quot;after do job:(%d) \\n\u0026quot;, id) } func main() { doSomething(1) doSomething(2) doSomething(3) } 这个例子的输出是：\n2018/04/15 17:06:05 before do job:(1) 2018/04/15 17:06:08 after do job:(1) 2018/04/15 17:06:08 before do job:(2) 2018/04/15 17:06:11 after do job:(2) 2018/04/15 17:06:11 before do job:(3) 2018/04/15 17:06:14 after do job:(3) 可以看到是用了9秒的时间才完成，如果是采用goroutine的话，就很快。很简单，就是在执行doSomething之前，加上go关键字。\nimport ( \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; ) func doSomething(id int) { log.Printf(\u0026quot;before do job:(%d) \\n\u0026quot;, id) time.Sleep(3 * time.Second) log.Printf(\u0026quot;after do job:(%d) \\n\u0026quot;, id) } func main() { go doSomething(1) go doSomething(2) go doSomething(3) } 但是这样的话，什么结果也没有，是因为main函数本身也是一个goroutine，main执行完之后，其他的还没开始，所以什么也看不到。最简单的办法就是让main函数等待一段时间再结束，但是这样不够优雅。\n我们应该采用sync.WaitGroup来等待所有的goroutine结束。\npackage main import ( \u0026quot;log\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; ) func doSomething(id int, wg *sync.WaitGroup) { defer wg.Done() log.Printf(\u0026quot;before do job:(%d) \\n\u0026quot;, id) time.Sleep(3 * time.Second) log.Printf(\u0026quot;after do job:(%d) \\n\u0026quot;, id) } func main() { var wg sync.WaitGroup wg.Add(3) // 因为我们要修改wg的状态，所以要传指针过去 go doSomething(1, \u0026amp;wg) go doSomething(2, \u0026amp;wg) go doSomething(3, \u0026amp;wg) wg.Wait() log.Printf(\u0026quot;finish all jobs\\n\u0026quot;) } 执行结果是：\n2018/04/15 17:13:14 before do job:(3) 2018/04/15 17:13:14 before do job:(2) 2018/04/15 17:13:14 before do job:(1) 2018/04/15 17:13:17 after do job:(2) 2018/04/15 17:13:17 after do job:(1) 2018/04/15 17:13:17 after do job:(3) 可以看到这次只用了3秒左右就执行完了，而且他们的执行顺序也不确定，竞争执行。\nchannel 每个协程之间要进行通信，那么在通信的时候采用的是Channel的形式，即一个goroutine将数据传递给Channel，另一个goroutine从Channel中读取数据。\n创建Channel有两种方式：\n使用内建函数 make 可以创建 channel，举例如下：\nch := make(chan int) // 注意： channel 必须定义其传递的数据类型 也可以用 var 声明 channel, 如下：\nvar ch chan int 以上声明的 channel 都是双向的，意味着可以该 channel 可以发送数据，也可以接收数据。\n","permalink":"https://zhenfeng-zhu.github.io/posts/go%E8%AF%AD%E8%A8%80%E4%BD%93%E4%BC%9A/","summary":"最近公司要统一技术栈，在kotlin和go之间选。我心里是比较倾向go的，主要有如下几点体会。\n 语言简单，上手快。 gorotuine 易发布 垃圾回收 约定大于配置  我最早听说协程，是在大三找实习的时候，那个时候面试会问线程和进程的关系，问的深一些就是协程和线程的区别。游戏公司基本都用lua，看了lua的资料后，对协程有了一些自己的了解，随后就是在做Unity相关的开发，在unity中使用了很多的协程，但是在unity中使用的协程好像跟主流的不太一样，在看了go之后，豁然开朗。\ngoroutine使用的内存比线程更少，go在运行的时候会自动在配置的一组逻辑处理器上调度执行。比如：\nfunc log(msg string){ ... } go log(\u0026quot;\u0026quot;) 使用关键字go，即可让log函数在一个goroutine里执行了。\n并发最难的部分是要确保其他并发运行的进程、线程或者goroutine不会以外的修改数据。go使用了Channel的方式来解决这个问题。对于通道模式，保证同一时刻只会有一个goroutine修改数据。\n说起go的语言简单，其实主要是他的类型比较简单。go使用的是组合模式，只需要将一个类型嵌入到另外一个类型就可以复用所有的功能。而且go还具有独特的接口实现机制，允许用户对行为进行建模，在go中不需要声明某个类型实现了某个接口，编译器会自动判断一个实例是使用什么接口。\n对于java来说，所有的设计都是围绕着接口展开，于是在设计模式中，就是面向接口编程：\ninterface User{ void login(); void logout(); } 在java中，继承的类必须显式声明继承了此接口。而在go中接口只是描述一个动作，如果说是实现这个接口，只需要让某个实例实现了这个接口中的所有方法就行了。\ntype Reader interface{ Read(p []byte))(n int, err error) } 这其实和传统的oop语言的接口有着本质的区别，go中的接口一般只定义一个单一的动作，实际使用的过程中，这更有利于使用组合来复用代码。\n约定大于配置这点，go在这方面上做的感觉有点儿吹毛求疵了，但是这样也使得程序可读性更强，没有很多垃圾代码。比如go的文件结构必须是src pkg 和bin 三个包，而且go也不允许你声明一个变量却不使用，导入了一个包却不使用，而且程序的代码也有约定，init方法比main方法更早执行。\ngo的并发 说到并发，就会想到另外一个概念，并行。可以简单这样的理解：\n并发是同时管理多个事情，而并行是同时做很多事情。也就是并发是manage，并行是run。 对于单核处理器来讲，同一时刻只能有一个任务在执行，那么并发就是同时管理多个任务，让他们交替执行。并行是针对于多核处理器的，同一时刻可以把多个任务放在不同的处理器上执行，这样就可以同时执行。\n在go里面主要是采用协程来实现并发的，也就是goroutine。与其他语言不同的是，go是在语法层面做到的，即go func();\n语法 go f(x, y) go是关键字，后面跟函数。\n例子 package main import ( \u0026quot;log\u0026quot; \u0026quot;time\u0026quot; ) func doSomething(id int) { log.Printf(\u0026quot;before do job:(%d) \\n\u0026quot;, id) time.","title":"Go语言体会"},{"content":"主动管理时间，敢于说不。\n有目标向前看，没目标向钱看。\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E5%85%B3%E4%BA%8E%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/","summary":"主动管理时间，敢于说不。\n有目标向前看，没目标向钱看。","title":"关于时间管理"},{"content":"当我们在做数据库分库分表或者做分布式缓存的时候，不可避免的都会遇到一个问题：\n如何将数据均匀的分散到各个节点中，并且尽量的在加减节点的时能使受影响的数据最少。\n1 hash取模 随机放置就不多说了。通常最容易想到的方案是哈希取模了。\n可以将传入的key按照 $$ index=hash(key) % N $$ 这样来计算出需要存放的节点。\n这样可以满足数据的均匀分配，但是这个算法的容错性和扩展性比较差。比如增加或者删除一个节点的时候，所有的key都要重新计算，显然这样的成本比较高，为此需要一个算法来满足均匀的同时也要有良好的容错性和扩展性。\n2 一致性hash算法 一致性hash算法是将所有的哈希值构成了一个环，其范围是0~2^32-1。如图：\n之后将各个服务器节点散列到这个环上，可以用节点的IP，hostname这样唯一性的字段作为key进行hash。散列之后如下：\n之后需要将数据定位到对应的节点上，使用同样的hash函数将key也映射到这个环上。\n这样就按照顺时针方向就可以将k1定位到N1节点，k2定位到N3节点，k3定位到N2节点。\n2.1 容错性 假设N1宕机了：\n依然根据顺时针方向，k2和k3保持不变，只有k1被重新映射到了N3。这样就很好的保证了容错性，当一个节点宕机时只会影响到少部分数据。\n2.2 扩展性 当新增一个节点时：\n在N2和N3之间新增了一个节点N4，这时受影响的数据只有k3，其余的数据也是保持不变。\n2.3 虚拟节点 到目前为止，该算法也有一些问题：\n当节点较少的时候可能出现数据不均匀的情况：\n这样会导致大部分数据都在N1节点，只有少量的数据在N2节点。\n为了解决这个问题，一致性哈希算法引入了虚拟节点。\n将每一个节点进行多次哈希，生成的节点放置在环上成为虚拟节点。\n计算时可以在 IP 后加上编号来生成哈希值。\n这样只需要在原有的基础上多一步由虚拟节点映射到实际节点的步骤即可让少量节点也能满足均匀性。\n3 参考 https://crossoverjie.top/2018/01/08/Consistent-Hash/#more\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/","summary":"当我们在做数据库分库分表或者做分布式缓存的时候，不可避免的都会遇到一个问题：\n如何将数据均匀的分散到各个节点中，并且尽量的在加减节点的时能使受影响的数据最少。\n1 hash取模 随机放置就不多说了。通常最容易想到的方案是哈希取模了。\n可以将传入的key按照 $$ index=hash(key) % N $$ 这样来计算出需要存放的节点。\n这样可以满足数据的均匀分配，但是这个算法的容错性和扩展性比较差。比如增加或者删除一个节点的时候，所有的key都要重新计算，显然这样的成本比较高，为此需要一个算法来满足均匀的同时也要有良好的容错性和扩展性。\n2 一致性hash算法 一致性hash算法是将所有的哈希值构成了一个环，其范围是0~2^32-1。如图：\n之后将各个服务器节点散列到这个环上，可以用节点的IP，hostname这样唯一性的字段作为key进行hash。散列之后如下：\n之后需要将数据定位到对应的节点上，使用同样的hash函数将key也映射到这个环上。\n这样就按照顺时针方向就可以将k1定位到N1节点，k2定位到N3节点，k3定位到N2节点。\n2.1 容错性 假设N1宕机了：\n依然根据顺时针方向，k2和k3保持不变，只有k1被重新映射到了N3。这样就很好的保证了容错性，当一个节点宕机时只会影响到少部分数据。\n2.2 扩展性 当新增一个节点时：\n在N2和N3之间新增了一个节点N4，这时受影响的数据只有k3，其余的数据也是保持不变。\n2.3 虚拟节点 到目前为止，该算法也有一些问题：\n当节点较少的时候可能出现数据不均匀的情况：\n这样会导致大部分数据都在N1节点，只有少量的数据在N2节点。\n为了解决这个问题，一致性哈希算法引入了虚拟节点。\n将每一个节点进行多次哈希，生成的节点放置在环上成为虚拟节点。\n计算时可以在 IP 后加上编号来生成哈希值。\n这样只需要在原有的基础上多一步由虚拟节点映射到实际节点的步骤即可让少量节点也能满足均匀性。\n3 参考 https://crossoverjie.top/2018/01/08/Consistent-Hash/#more","title":"一致性哈希算法"},{"content":"Spring Boot启动原理分析 我们在开发spring boot应用的时候，一般会遇到如下的启动类：\n@SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 从这段代码可以看出，注解@SpringBootApplication和SpringApplication.run()是比较重要的两个东西。\n1 @SpringApplication注解 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ... } 在这段代码里，比较重要的只有三个注解：\n @Configuration（@SpringBootConfiguration点开查看发现里面还是应用了@Configuration） @EnableAutoConfiguration @ComponentScan  其实，我们使用这三个注解来修饰springboot的启动类也可以正常运行,如下所示：\n@ComponentScan @EnableAutoConfiguration @Configuration public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 每次写这三个注解的话，比较繁琐，所以就spring团队就封装了一个@SpringBootApplication。\n1.1 @Configuration @Configuration就是JavaConfig形式的Spring Ioc容器的配置类使用的那个@Configuration，SpringBoot社区推荐使用基于JavaConfig的配置形式，所以，这里的启动类标注了@Configuration之后，本身其实也是一个IoC容器的配置类。\nXML跟config配置方式的区别可以从如下几个方面来说：\n  表达形式层面 基于xml的配置方式是这样的：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd\u0026#34; default-lazy-init=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;!--bean定义--\u0026gt; \u0026lt;/beans\u0026gt; 基于java config配置方式是这样的：\n@Configuration public class MockConfiguration{ //bean定义 }   注册bean定义层面 基于XML的配置形式是这样：\n\u0026lt;bean id=\u0026#34;mockService\u0026#34; class=\u0026#34;..MockServiceImpl\u0026#34;\u0026gt; ... \u0026lt;/bean\u0026gt; 而基于Java config的配置形式是这样的：\n@Configuration public class MockConfiguration{ @Bean public MockService mockService(){ return new MockServiceImpl(); } } 任何一个标注了@Bean的方法，其返回值将作为一个bean定义注册到Spring的IoC容器，方法名将默认成该bean定义的id。\n  表达依赖注入关系层面 为了表达bean与bean之间的依赖关系，在XML形式中一般是这样：\n\u0026lt;bean id=\u0026#34;mockService\u0026#34; class=\u0026#34;..MockServiceImpl\u0026#34;\u0026gt; \u0026lt;propery name =\u0026#34;dependencyService\u0026#34; ref=\u0026#34;dependencyService\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;dependencyService\u0026#34; class=\u0026#34;DependencyServiceImpl\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; 而基于Java config的配置形式是这样的：\n@Configuration public class MockConfiguration{ @Bean public MockService mockService(){ return new MockServiceImpl(dependencyService()); } @Bean public DependencyService dependencyService(){ return new DependencyServiceImpl(); } } 如果一个bean的定义依赖其他bean,则直接调用对应的JavaConfig类中依赖bean的创建方法就可以了。\n  1.2 @ComponentScan @ComponentScan的功能其实就是自动扫描并加载符合条件的组件（比如@Component和@Repository等）或者bean定义，最终将这些bean定义加载到IoC容器中。\n我们可以通过basePackages等属性来细粒度的定制@ComponentScan自动扫描的范围，如果不指定，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。\n所以SpringBoot的启动类最好是放在root package下，因为默认不指定basePackages。\n1.3 @EnableAutoConfiguration Spring框架提供了各种名字为@Enable开头的Annotation定义，比如@EnableScheduling、@EnableCaching、@EnableMBeanExport等。@EnableAutoConfiguration的理念和做事方式其实一脉相承，简单概括一下就是，借助@Import的支持，收集和注册特定场景相关的bean定义。\n@EnableAutoConfiguration也是借助@Import的帮助，将所有符合自动配置条件的bean定义加载到IoC容器。\n@SuppressWarnings(\u0026quot;deprecation\u0026quot;) @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage @Import(EnableAutoConfigurationImportSelector.class) public @interface EnableAutoConfiguration { ... } @EnableAutoConfiguration作为一个复合Annotation，\n其中，最关键的要属@Import(EnableAutoConfigurationImportSelector.class)，借助EnableAutoConfigurationImportSelector，@EnableAutoConfiguration借助于SpringFactoriesLoader的支持可以帮助SpringBoot应用将所有符合条件的@Configuration配置都加载到当前SpringBoot创建并使用的IoC容器。SpringFactoriesLoader的支持。\n1.4 SpringFactoriesLoader SpringFactoriesLoader属于Spring框架私有的一种扩展方案，其主要功能就是从指定的配置文件META-INF/spring.factories加载配置。\npublic abstract class SpringFactoriesLoader { public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; loadFactories(Class\u0026lt;T\u0026gt; factoryClass, ClassLoader classLoader) { ... } public static List\u0026lt;String\u0026gt; loadFactoryNames(Class\u0026lt;?\u0026gt; factoryClass, ClassLoader classLoader){ .... } } 配合@EnableAutoConfiguration使用的话，它更多是提供一种配置查找的功能支持，即根据@EnableAutoConfiguration的完整类名org.springframework.boot.autoconfigure.EnableAutoConfiguration作为查找的Key,获取对应的一组@Configuration类。\n@EnableAutoConfiguration自动配置流程就是：\n 从classpath中搜寻所有的META-INF/spring.factories配置文件； 并将其中org.springframework.boot.autoconfigure.EnableutoConfiguration对应的配置项通过反射（Java Refletion）实例化为对应的标注了@Configuration的JavaConfig形式的IoC容器配置类； 然后汇总为一个并加载到IoC容器。  2 SpringApplication SpringApplication的run该方法的主要流程大体可以归纳如下：\n1） 如果我们使用的是SpringApplication的静态run方法，那么，这个方法里面首先要创建一个SpringApplication对象实例，然后调用这个创建好的SpringApplication的实例方法。在SpringApplication实例初始化的时候，它会提前做几件事情：\n 根据classpath里面是否存在某个特征类（org.springframework.web.context.ConfigurableWebApplicationContext）来决定是否应该创建一个为Web应用使用的ApplicationContext类型。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationContextInitializer。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationListener。 推断并设置main方法的定义类。  2） SpringApplication实例初始化完成并且完成设置后，就开始执行run方法的逻辑了，方法执行伊始，首先遍历执行所有通过SpringFactoriesLoader可以查找到并加载的SpringApplicationRunListener。调用它们的started()方法，告诉这些SpringApplicationRunListener，“嘿，SpringBoot应用要开始执行咯！”。\n3） 创建并配置当前Spring Boot应用将要使用的Environment（包括配置要使用的PropertySource以及Profile）。\n4） 遍历调用所有SpringApplicationRunListener的environmentPrepared()的方法，告诉他们：“当前SpringBoot应用使用的Environment准备好了咯！”。\n5） 如果SpringApplication的showBanner属性被设置为true，则打印banner。\n6） 根据用户是否明确设置了applicationContextClass类型以及初始化阶段的推断结果，决定该为当前SpringBoot应用创建什么类型的ApplicationContext并创建完成，然后根据条件决定是否添加ShutdownHook，决定是否使用自定义的BeanNameGenerator，决定是否使用自定义的ResourceLoader，当然，最重要的，将之前准备好的Environment设置给创建好的ApplicationContext使用。\n7） ApplicationContext创建好之后，SpringApplication会再次借助Spring-FactoriesLoader，查找并加载classpath中所有可用的ApplicationContext-Initializer，然后遍历调用这些ApplicationContextInitializer的initialize（applicationContext）方法来对已经创建好的ApplicationContext进行进一步的处理。\n8） 遍历调用所有SpringApplicationRunListener的contextPrepared()方法。\n9） 最核心的一步，将之前通过@EnableAutoConfiguration获取的所有配置以及其他形式的IoC容器配置加载到已经准备完毕的ApplicationContext。\n10） 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。\n11） 调用ApplicationContext的refresh()方法，完成IoC容器可用的最后一道工序。\n12） 查找当前ApplicationContext中是否注册有CommandLineRunner，如果有，则遍历执行它们。\n13） 正常情况下，遍历执行SpringApplicationRunListener的finished()方法、（如果整个过程出现异常，则依然调用所有SpringApplicationRunListener的finished()方法，只不过这种情况下会将异常信息一并传入处理）\n去除事件通知点后，整个流程如下图所示：\n3 参考资料 Spring Boot干货系列：（三）启动原理解析 SpringBoot揭秘快速构建为服务体系\n","permalink":"https://zhenfeng-zhu.github.io/posts/spring-boot%E5%90%AF%E5%8A%A8%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/","summary":"Spring Boot启动原理分析 我们在开发spring boot应用的时候，一般会遇到如下的启动类：\n@SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 从这段代码可以看出，注解@SpringBootApplication和SpringApplication.run()是比较重要的两个东西。\n1 @SpringApplication注解 @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ... } 在这段代码里，比较重要的只有三个注解：\n @Configuration（@SpringBootConfiguration点开查看发现里面还是应用了@Configuration） @EnableAutoConfiguration @ComponentScan  其实，我们使用这三个注解来修饰springboot的启动类也可以正常运行,如下所示：\n@ComponentScan @EnableAutoConfiguration @Configuration public class DemoApplication { public static void main(String[] args) { SpringApplication.","title":"Spring Boot启动原理分析"},{"content":"为了解决抽象各个Java实体基本的“增删改查”操作，我们通常会以泛型的方式封装一个模板Dao来进行抽象简化，但是这样依然不是很方便，我们需要针对每个实体编写一个继承自泛型模板Dao的接口，再编写该接口的实现。虽然一些基础的数据访问已经可以得到很好的复用，但是在代码结构上针对每个实体都会有一堆Dao的接口和实现。\n由于模板Dao的实现，使得这些具体实体的Dao层已经变的非常“薄”，有一些具体实体的Dao实现可能完全就是对模板Dao的简单代理，并且往往这样的实现类可能会出现在很多实体上。Spring-data-jpa的出现正可以让这样一个已经很“薄”的数据访问层变成只是一层接口的编写方式。\n1 工程配置 1.1 pom \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jpa-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;jpa-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.9.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 1.2 application.properties spring.datasource.url=jdbc:mysql://localhost:3306/test spring.datasource.username=root spring.datasource.password=root spring.datasource.driver-class-name=com.mysql.jdbc.Driver spring.jpa.properties.hibernate.hbm2ddl.auto=create-drop spring.jpa.properties.hibernate.hbm2ddl.auto是hibernate的配置属性，其主要作用是：自动创建、更新、验证数据库表结构。该参数的几种配置如下：\n create：每次加载hibernate时都会删除上一次的生成的表，然后根据你的model类再重新来生成新表，哪怕两次没有任何改变也要这样执行，这就是导致数据库表数据丢失的一个重要原因。 create-drop：每次加载hibernate时根据model类生成表，但是sessionFactory一关闭,表就自动删除。 update：最常用的属性，第一次加载hibernate时根据model类会自动建立起表的结构（前提是先建立好数据库），以后加载hibernate时根据model类自动更新表结构，即使表结构改变了但表中的行仍然存在不会删除以前的行。要注意的是当部署到服务器后，表结构是不会被马上建立起来的，是要等应用第一次运行起来后才会。 validate：每次加载hibernate时，验证创建数据库表结构，只会和数据库中的表进行比较，不会创建新表，但是会插入新值。  2 实体类 创建一个User实体，包含id（主键）、name（姓名）、age（年龄）属性，通过ORM框架其会被映射到数据库表中，由于配置了hibernate.hbm2ddl.auto，在应用启动的时候框架会自动去数据库中创建对应的表。\n@Data @NoArgsConstructor @Entity public class Users { @Id @GeneratedValue private Long id; @Column(nullable = false) private String name; @Column(nullable = false) private Integer age; public Users(String name, Integer age) { this.name = name; this.age = age; } } 3 repository 针对User实体创建对应的Repository接口实现对该实体的数据访问：\n@Repository public interface UsersRepository extends JpaRepository\u0026lt;Users, Long\u0026gt; { Users findByName(String name); Users findByNameAndAge(String name, Integer age); @Query(\u0026quot;from Users u where u.name=:name\u0026quot;) Users findUser(@Param(\u0026quot;name\u0026quot;) String name); } 在Spring-data-jpa中，只需要编写类似上面这样的接口就可实现数据访问。不再像我们以往编写了接口时候还需要自己编写接口实现类，直接减少了我们的文件清单。\n下面对上面的UserRepository做一些解释，该接口继承自JpaRepository，通过查看JpaRepository接口的API文档，可以看到该接口本身已经实现了创建（save）、更新（save）、删除（delete）、查询（findAll、findOne）等基本操作的函数，因此对于这些基础操作的数据访问就不需要开发者再自己定义。\n在上例中，我们可以看到下面两个函数：\n User findByName(String name) User findByNameAndAge(String name, Integer age)  它们分别实现了按name查询User实体和按name和age查询User实体，可以看到我们这里没有任何类SQL语句就完成了两个条件查询方法。这就是Spring-data-jpa的一大特性：通过解析方法名创建查询。\n除了通过解析方法名来创建查询外，它也提供通过使用@Query 注解来创建查询，您只需要编写JPQL语句，并通过类似“:name”来映射@Param指定的参数，就像例子中的第三个findUser函数一样。\n4 单元测试 @RunWith(SpringRunner.class) @SpringBootTest public class JpaDemoApplicationTests { @Autowired private UsersRepository usersRepository; @Test public void contextLoads() { } @Test public void testJPA() { // 创建10条记录 usersRepository.save(new Users(\u0026quot;AAA\u0026quot;, 10)); usersRepository.save(new Users(\u0026quot;BBB\u0026quot;, 20)); usersRepository.save(new Users(\u0026quot;CCC\u0026quot;, 30)); usersRepository.save(new Users(\u0026quot;DDD\u0026quot;, 40)); usersRepository.save(new Users(\u0026quot;EEE\u0026quot;, 50)); usersRepository.save(new Users(\u0026quot;FFF\u0026quot;, 60)); usersRepository.save(new Users(\u0026quot;GGG\u0026quot;, 70)); usersRepository.save(new Users(\u0026quot;HHH\u0026quot;, 80)); usersRepository.save(new Users(\u0026quot;III\u0026quot;, 90)); usersRepository.save(new Users(\u0026quot;JJJ\u0026quot;, 100)); // 测试findAll, 查询所有记录 Assert.assertEquals(10, usersRepository.findAll().size()); // 测试findByName, 查询姓名为FFF的User Assert.assertEquals(60, usersRepository.findByName(\u0026quot;FFF\u0026quot;).getAge().longValue()); // 测试findUser, 查询姓名为FFF的User Assert.assertEquals(60, usersRepository.findUser(\u0026quot;FFF\u0026quot;).getAge().longValue()); // 测试findByNameAndAge, 查询姓名为FFF并且年龄为60的User Assert.assertEquals(\u0026quot;FFF\u0026quot;, usersRepository.findByNameAndAge(\u0026quot;FFF\u0026quot;, 60).getName()); // 测试删除姓名为AAA的User usersRepository.delete(usersRepository.findByName(\u0026quot;AAA\u0026quot;)); // 测试findAll, 查询所有记录, 验证上面的删除是否成功 Assert.assertEquals(9, usersRepository.findAll().size()); } } 5 参考资料 Spring Boot 使用Spring-data-jpa简化数据访问层\n","permalink":"https://zhenfeng-zhu.github.io/posts/spring-data-jpa%E5%AE%9E%E6%88%98/","summary":"为了解决抽象各个Java实体基本的“增删改查”操作，我们通常会以泛型的方式封装一个模板Dao来进行抽象简化，但是这样依然不是很方便，我们需要针对每个实体编写一个继承自泛型模板Dao的接口，再编写该接口的实现。虽然一些基础的数据访问已经可以得到很好的复用，但是在代码结构上针对每个实体都会有一堆Dao的接口和实现。\n由于模板Dao的实现，使得这些具体实体的Dao层已经变的非常“薄”，有一些具体实体的Dao实现可能完全就是对模板Dao的简单代理，并且往往这样的实现类可能会出现在很多实体上。Spring-data-jpa的出现正可以让这样一个已经很“薄”的数据访问层变成只是一层接口的编写方式。\n1 工程配置 1.1 pom \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jpa-demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;jpa-demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.5.9.RELEASE\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;project.reporting.outputEncoding\u0026gt;UTF-8\u0026lt;/project.reporting.outputEncoding\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.","title":"Spring Data Jpa实战"},{"content":"spring boot多数据源配置 在单数据源的情况下，Spring Boot的配置非常简单，只需要在application.properties文件中配置连接参数即可。但是往往随着业务量发展，我们通常会进行数据库拆分或是引入其他数据库，从而我们需要配置多个数据源。\n1 准备 1.1 禁止DataSourceAutoConfiguration 首先要将spring boot自带的DataSourceAutoConfiguration禁掉，因为它会读取application.properties文件的spring.datasource.*属性并自动配置单数据源。在@SpringBootApplication注解中添加exclude属性即可：\n@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class}) public class DemoApplication { public static void main(String[] args) { SpringApplication.run(JpaDemoApplication.class, args); } } 1.2 配置数据库连接 然后在application.properties中配置多数据源连接信息：\nspring.datasource.primary.url=jdbc:mysql://localhost:3306/test spring.datasource.primary.username=root spring.datasource.primary.password=root spring.datasource.primary.driver-class-name=com.mysql.jdbc.Driver spring.datasource.secondary.url=jdbc:mysql://localhost:3306/test1 spring.datasource.secondary.username=root spring.datasource.secondary.password=root spring.datasource.secondary.driver-class-name=com.mysql.jdbc.Driver 1.3 手段创建数据源 由于我们禁掉了自动数据源配置，因些下一步就需要手动将这些数据源创建出来：\n@Configuration public class DataSourceConfig { @Bean(name = \u0026quot;primaryDataSource\u0026quot;) // @Qualifier(value = \u0026quot;primaryDataSource\u0026quot;) @ConfigurationProperties(prefix = \u0026quot;spring.datasource.primary\u0026quot;) public DataSource primaryDataSource(){ return DataSourceBuilder.create().build(); } @Bean(name = \u0026quot;secondaryDataSource\u0026quot;) // @Qualifier(value = \u0026quot;secondaryDataSource\u0026quot;) @ConfigurationProperties(prefix = \u0026quot;spring.datasource.secondary\u0026quot;) public DataSource secondaryDataSource() { return DataSourceBuilder.create().build(); } } 2 jdbcTemplate多数据源 2.1 jdbcTemplate的数据源配置 新建jdbcTemplate的数据源配置：\n@Configuration public class JdbcTemplateConfig { @Bean(name = \u0026quot;primaryJdbcTemplate\u0026quot;) public JdbcTemplate primaryJdbcTemplate( @Qualifier(\u0026quot;primaryDataSource\u0026quot;) DataSource dataSource) { return new JdbcTemplate(dataSource); } @Bean(name = \u0026quot;secondaryJdbcTemplate\u0026quot;) public JdbcTemplate secondaryJdbcTemplate( @Qualifier(\u0026quot;secondaryDataSource\u0026quot;) DataSource dataSource) { return new JdbcTemplate(dataSource); } } 2.2 单元测试 然后编写单元测试用例：\n@RunWith(SpringRunner.class) @SpringBootTest public class JpaDemoApplicationTests { @Autowired @Qualifier(\u0026quot;primaryJdbcTemplate\u0026quot;) protected JdbcTemplate jdbcTemplate1; @Autowired @Qualifier(\u0026quot;secondaryJdbcTemplate\u0026quot;) protected JdbcTemplate jdbcTemplate2; @Test public void testJdbc() { // 往第一个数据源中插入两条数据 jdbcTemplate1.update(\u0026quot;insert into users(id,name,age) values(?, ?, ?)\u0026quot;, 1, \u0026quot;aaa\u0026quot;, 20); jdbcTemplate1.update(\u0026quot;insert into users(id,name,age) values(?, ?, ?)\u0026quot;, 2, \u0026quot;bbb\u0026quot;, 30); // 往第二个数据源中插入一条数据，若插入的是第一个数据源，则会主键冲突报错 jdbcTemplate2.update(\u0026quot;insert into users(id,name,age) values(?, ?, ?)\u0026quot;, 1, \u0026quot;aaa\u0026quot;, 20); // 查一下第一个数据源中是否有两条数据，验证插入是否成功 Assert.assertEquals(\u0026quot;2\u0026quot;, jdbcTemplate1.queryForObject(\u0026quot;select count(1) from users\u0026quot;, String.class)); // 查一下第一个数据源中是否有两条数据，验证插入是否成功 Assert.assertEquals(\u0026quot;1\u0026quot;, jdbcTemplate2.queryForObject(\u0026quot;select count(1) from users\u0026quot;, String.class)); } @Test public void contextLoads() { } } 3 mybatis多数据源配置 3.1 自定义SqlSessionFactory 新建两个mybatis的SqlSessionFactory配置：\n@Configuration @MapperScan(basePackages = {\u0026quot;com.example.jpademo.primary.mapper\u0026quot;}, sqlSessionFactoryRef = \u0026quot;sqlSessionFactory1\u0026quot;) public class MybatisPrimaryConfig { @Autowired @Qualifier(\u0026quot;primaryDataSource\u0026quot;) private DataSource primaryDataSource; @Bean public SqlSessionFactory sqlSessionFactory1() throws Exception { SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); // 使用primaryDataSource数据源 factoryBean.setDataSource(primaryDataSource); return factoryBean.getObject(); } @Bean public SqlSessionTemplate sqlSessionTemplate1() throws Exception { // 使用上面配置的Factory SqlSessionTemplate template = new SqlSessionTemplate(sqlSessionFactory1()); return template; } } 这样，com.example.jpademo.primary.mapper包下的所有mapper就会用sqlSessionFactory1。同理可以创建\nsqlSessionFactory2：\n@Configuration @MapperScan(basePackages = {\u0026quot;com.example.jpademo.secondary.mapper\u0026quot;}, sqlSessionFactoryRef = \u0026quot;sqlSessionFactory2\u0026quot;) public class MybatisSecondaryConfig { @Autowired @Qualifier(\u0026quot;secondaryDataSource\u0026quot;) private DataSource secondaryDataSource; @Bean public SqlSessionFactory sqlSessionFactory2() throws Exception { SqlSessionFactoryBean factoryBean = new SqlSessionFactoryBean(); // 使用primaryDataSource数据源 factoryBean.setDataSource(secondaryDataSource); return factoryBean.getObject(); } @Bean public SqlSessionTemplate sqlSessionTemplate1() throws Exception { // 使用上面配置的Factory SqlSessionTemplate template = new SqlSessionTemplate(sqlSessionFactory2()); return template; } } 3.2 mapper和实体类 然后编写mapper和实体类：\n@Data public class User { private Integer id; private String name; private Integer age; } package com.example.jpademo.primary.mapper; import com.example.jpademo.domain.User; import org.apache.ibatis.annotations.Mapper; import org.apache.ibatis.annotations.Param; import org.apache.ibatis.annotations.Select; import org.springframework.beans.factory.annotation.Qualifier; @Mapper @Qualifier(\u0026quot;userMapper1\u0026quot;) public interface UserMapper1 { @Select(\u0026quot;select * from users where id=#{id}\u0026quot;) User findById(@Param(\u0026quot;id\u0026quot;) Integer id); } package com.example.jpademo.secondary.mapper; import com.example.jpademo.domain.User; import org.apache.ibatis.annotations.Mapper; import org.apache.ibatis.annotations.Select; import org.springframework.beans.factory.annotation.Qualifier; @Mapper @Qualifier(\u0026quot;userMapper2\u0026quot;) public interface UserMapper2 { @Select(\u0026quot;select * from users where id=#{id}\u0026quot;) User findById(Integer id); } 3.3 单元测试 编写单元测试用例：\n@RunWith(SpringRunner.class) @SpringBootTest public class JpaDemoApplicationTests { @Autowired private UserMapper1 userMapper1; @Autowired private UserMapper2 userMapper2; @Test public void testMybatis() { User user1 = userMapper1.findById(1); User user2 = userMapper2.findById(1); Assert.assertEquals(\u0026quot;aaa\u0026quot;, user1.getName()); Assert.assertEquals(\u0026quot;ccc\u0026quot;, user2.getName()); } } 4 参考资料 Spring Boot + Mybatis多数据源和动态数据源配置\nSpring Boot 两种多数据源配置：JdbcTemplate、Spring-data-jpa\n","permalink":"https://zhenfeng-zhu.github.io/posts/spring-boot%E5%A4%9A%E6%95%B0%E6%8D%AE%E6%BA%90%E9%85%8D%E7%BD%AE/","summary":"spring boot多数据源配置 在单数据源的情况下，Spring Boot的配置非常简单，只需要在application.properties文件中配置连接参数即可。但是往往随着业务量发展，我们通常会进行数据库拆分或是引入其他数据库，从而我们需要配置多个数据源。\n1 准备 1.1 禁止DataSourceAutoConfiguration 首先要将spring boot自带的DataSourceAutoConfiguration禁掉，因为它会读取application.properties文件的spring.datasource.*属性并自动配置单数据源。在@SpringBootApplication注解中添加exclude属性即可：\n@SpringBootApplication(exclude = {DataSourceAutoConfiguration.class}) public class DemoApplication { public static void main(String[] args) { SpringApplication.run(JpaDemoApplication.class, args); } } 1.2 配置数据库连接 然后在application.properties中配置多数据源连接信息：\nspring.datasource.primary.url=jdbc:mysql://localhost:3306/test spring.datasource.primary.username=root spring.datasource.primary.password=root spring.datasource.primary.driver-class-name=com.mysql.jdbc.Driver spring.datasource.secondary.url=jdbc:mysql://localhost:3306/test1 spring.datasource.secondary.username=root spring.datasource.secondary.password=root spring.datasource.secondary.driver-class-name=com.mysql.jdbc.Driver 1.3 手段创建数据源 由于我们禁掉了自动数据源配置，因些下一步就需要手动将这些数据源创建出来：\n@Configuration public class DataSourceConfig { @Bean(name = \u0026quot;primaryDataSource\u0026quot;) // @Qualifier(value = \u0026quot;primaryDataSource\u0026quot;) @ConfigurationProperties(prefix = \u0026quot;spring.datasource.primary\u0026quot;) public DataSource primaryDataSource(){ return DataSourceBuilder.create().build(); } @Bean(name = \u0026quot;secondaryDataSource\u0026quot;) // @Qualifier(value = \u0026quot;secondaryDataSource\u0026quot;) @ConfigurationProperties(prefix = \u0026quot;spring.","title":"spring boot多数据源配置"},{"content":"Spring-data-redis为spring-data模块中对redis的支持部分，简称为“SDR”，提供了基于jedis客户端API的高度封装以及与spring容器的整合，\njedis客户端在编程实施方面存在如下不足：\n connection管理缺乏自动化，connection-pool的设计缺少必要的容器支持。 数据操作需要关注“序列化”/“反序列化”，因为jedis的客户端API接受的数据类型为string和byte，对结构化数据(json,xml,pojo等)操作需要额外的支持。 事务操作纯粹为硬编码 pub/sub功能，缺乏必要的设计模式支持，对于开发者而言需要关注的太多。  1 spring-data-redis特性  连接池自动管理，提供了一个高度封装的“RedisTemplate”类 针对jedis客户端中大量api进行了归类封装,将同一类型操作封装为operation接口  ValueOperations：简单K-V操作 SetOperations：set类型数据操作 ZSetOperations：zset类型数据操作 HashOperations：针对map类型的数据操作 ListOperations：针对list类型的数据操作   提供了对key的“bound”(绑定)便捷化操作API，可以通过bound封装指定的key，然后进行一系列的操作而无须“显式”的再次指定Key，即BoundKeyOperations：  BoundValueOperations BoundSetOperations BoundListOperations BoundSetOperations BoundHashOperations   将事务操作封装，有容器控制。 针对数据的“序列化/反序列化”，提供了多种可选择策略(RedisSerializer)  JdkSerializationRedisSerializer：POJO对象的存取场景，使用JDK本身序列化机制，将pojo类通过ObjectInputStream/ObjectOutputStream进行序列化操作，最终redis-server中将存储字节序列。是目前最常用的序列化策略。 StringRedisSerializer：Key或者value为字符串的场景，根据指定的charset对数据的字节序列编码成string，是“new String(bytes, charset)”和“string.getBytes(charset)”的直接封装。是最轻量级和高效的策略。 JacksonJsonRedisSerializer：jackson-json工具提供了javabean与json之间的转换能力，可以将pojo实例序列化成json格式存储在redis中，也可以将json格式的数据转换成pojo实例。因为jackson工具在序列化和反序列化时，需要明确指定Class类型，因此此策略封装起来稍微复杂。 OxmSerializer：提供了将javabean与xml之间的转换能力，目前可用的三方支持包括jaxb，apache-xmlbeans；redis存储的数据将是xml工具。不过使用此策略，编程将会有些难度，而且效率最低；不建议使用。   基于设计模式，和JMS开发思路，将pub/sub的API设计进行了封装，使开发更加便捷。 spring-data-redis中，并没有对sharding提供良好的封装，如果你的架构是基于sharding，那么你需要自己去实现，这也是sdr和jedis相比，唯一缺少的特性。  2 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 3 配置 # REDIS (RedisProperties) # Redis数据库索引（默认为0） spring.redis.database=0 # Redis服务器地址 spring.redis.host=localhost # Redis服务器连接端口 spring.redis.port=6379 # Redis服务器连接密码（默认为空） spring.redis.password=root # 连接池最大连接数（使用负值表示没有限制） spring.redis.pool.max-active=8 # 连接池最大阻塞等待时间（使用负值表示没有限制） spring.redis.pool.max-wait=-1 # 连接池中的最大空闲连接 spring.redis.pool.max-idle=8 # 连接池中的最小空闲连接 spring.redis.pool.min-idle=0 # 连接超时时间（毫秒） spring.redis.timeout=0 其中spring.redis.database的配置通常使用0即可，Redis在配置的时候可以设置数据库数量，默认为16，可以理解为数据库的schema\n3.1 StringRedisTemplate @RunWith(SpringRunner.class) @SpringBootTest public class DemoApplicationTests { @Autowired private StringRedisTemplate stringRedisTemplate; @Test public void testRedis(){ stringRedisTemplate.opsForValue().set(\u0026quot;myKey\u0026quot;, \u0026quot;hello redis\u0026quot;); Assert.assertEquals(\u0026quot;hello redis\u0026quot;, stringRedisTemplate.opsForValue().get(\u0026quot;myKey\u0026quot;)); } } 通过上面这段极为简单的测试案例演示了如何通过自动配置的StringRedisTemplate对象进行Redis的读写操作，该对象从命名中就可注意到支持的是String类型。如果有使用过spring-data-redis的开发者一定熟悉RedisTemplate\u0026lt;K, V\u0026gt;接口，StringRedisTemplate就相当于RedisTemplate\u0026lt;String, String\u0026gt;的实现。\n除了String类型，我们还经常会在Redis中存储对象。\n3.2 RedisTemplate\u0026lt;Object, Object\u0026gt; 3.2.1 新建User类 @Data @AllArgsConstructor public class User implements Serializable{ private static final long serialVersionUID = 1L; private Integer id; private String username; private Integer age; } 3.2.2 创建UserRepository @Repository public class UserRepository { @Autowired private RedisTemplate\u0026lt;Object, Object\u0026gt; redisTemplate; // @Resource(name = \u0026quot;redisTemplate\u0026quot;) ValueOperations\u0026lt;Object, Object\u0026gt; valOps; /** * 保存 * @param user */ public void save(User user) { int id = user.getId(); valOps.set(id, user); } /** * 获取 * @param id * @return */ public User getUserById(int id) { return (User) valOps.get(id); } } @Resource注解和@Autowired一样，也可以标注在字段或属性的setter方法上，但它默认按名称装配。名称可以通过@Resource的name属性指定，如果没有指定name属性，当注解标注在字段上，即默认取字段的名称作为bean名称寻找依赖对象，当注解标注在属性的setter方法上，即默认取属性名作为bean名称寻找依赖对象。\n3.2.3 单元测试 @RunWith(SpringRunner.class) @SpringBootTest public class DemoApplicationTests { @Autowired private UserRepository userRepository; @Test public void testRedis(){ User user=new User(1, \u0026quot;hello\u0026quot;, 12); userRepository.save(user); Assert.assertEquals(\u0026quot;hello\u0026quot;, userRepository.getUserById(1).getUsername()); } } 4 参考资料 SpringBoot之Redis的支持\nSpring-data-redis特性与实例\n","permalink":"https://zhenfeng-zhu.github.io/posts/spring-boot%E8%BF%9E%E6%8E%A5redis/","summary":"Spring-data-redis为spring-data模块中对redis的支持部分，简称为“SDR”，提供了基于jedis客户端API的高度封装以及与spring容器的整合，\njedis客户端在编程实施方面存在如下不足：\n connection管理缺乏自动化，connection-pool的设计缺少必要的容器支持。 数据操作需要关注“序列化”/“反序列化”，因为jedis的客户端API接受的数据类型为string和byte，对结构化数据(json,xml,pojo等)操作需要额外的支持。 事务操作纯粹为硬编码 pub/sub功能，缺乏必要的设计模式支持，对于开发者而言需要关注的太多。  1 spring-data-redis特性  连接池自动管理，提供了一个高度封装的“RedisTemplate”类 针对jedis客户端中大量api进行了归类封装,将同一类型操作封装为operation接口  ValueOperations：简单K-V操作 SetOperations：set类型数据操作 ZSetOperations：zset类型数据操作 HashOperations：针对map类型的数据操作 ListOperations：针对list类型的数据操作   提供了对key的“bound”(绑定)便捷化操作API，可以通过bound封装指定的key，然后进行一系列的操作而无须“显式”的再次指定Key，即BoundKeyOperations：  BoundValueOperations BoundSetOperations BoundListOperations BoundSetOperations BoundHashOperations   将事务操作封装，有容器控制。 针对数据的“序列化/反序列化”，提供了多种可选择策略(RedisSerializer)  JdkSerializationRedisSerializer：POJO对象的存取场景，使用JDK本身序列化机制，将pojo类通过ObjectInputStream/ObjectOutputStream进行序列化操作，最终redis-server中将存储字节序列。是目前最常用的序列化策略。 StringRedisSerializer：Key或者value为字符串的场景，根据指定的charset对数据的字节序列编码成string，是“new String(bytes, charset)”和“string.getBytes(charset)”的直接封装。是最轻量级和高效的策略。 JacksonJsonRedisSerializer：jackson-json工具提供了javabean与json之间的转换能力，可以将pojo实例序列化成json格式存储在redis中，也可以将json格式的数据转换成pojo实例。因为jackson工具在序列化和反序列化时，需要明确指定Class类型，因此此策略封装起来稍微复杂。 OxmSerializer：提供了将javabean与xml之间的转换能力，目前可用的三方支持包括jaxb，apache-xmlbeans；redis存储的数据将是xml工具。不过使用此策略，编程将会有些难度，而且效率最低；不建议使用。   基于设计模式，和JMS开发思路，将pub/sub的API设计进行了封装，使开发更加便捷。 spring-data-redis中，并没有对sharding提供良好的封装，如果你的架构是基于sharding，那么你需要自己去实现，这也是sdr和jedis相比，唯一缺少的特性。  2 引入依赖 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 3 配置 # REDIS (RedisProperties) # Redis数据库索引（默认为0） spring.redis.database=0 # Redis服务器地址 spring.redis.host=localhost # Redis服务器连接端口 spring.redis.port=6379 # Redis服务器连接密码（默认为空） spring.redis.password=root # 连接池最大连接数（使用负值表示没有限制） spring.redis.pool.max-active=8 # 连接池最大阻塞等待时间（使用负值表示没有限制） spring.","title":"spring boot连接redis"},{"content":"最终一致性的实现手段 实现最终一致性有三种手段：可靠事件模式、业务补偿模式和TCC模式\n1 可靠事件模式 可靠事件模式属于事件驱动架构，当某件重要的事情发生时，比如更新一个业务实体，微服务会向消息代理发布一个事件。消息代理会将订阅事件的微服务推送事件。\n要实现这种模式需要消息队列实现事件的持久化和at least once的可靠事件投递模式。\n1.1 本地事件表 本地事件表方法是将事件和业务数据保存在同一个数据库中，使用一个额外的事件恢复服务来恢复事件，由本地事物保证更新业务和发布事件的原子性。\n但是业务系统和事件系统耦合比较紧密，额外的事件数据库操作也会给数据库带来额外的压力，可能成为瓶颈。\n1.2 外部事件 此方法是将事件持久化到外部的事件系统，事件系统需要提供实时事件服务以接受微服务发布的事件，同时事件系统还需要提供事件恢复服务来确认恢复事件。\n1.3 不足 此过程可能出现重复消费的情况。\n2 补偿模式 一般来讲，异常一般是由以下两种情况造成的：\n业务异常：业务逻辑产生的错误，比如余额不足、库存不足等。\n技术异常：非业务逻辑产生的异常，比如网络连接异常、超时等。\n补偿模式就是使用一个额外的协调服务来协调各个需要保证一致性的其他服务。协调服务按顺序调用每一个服务，如果某个服务调用异常就取消之前所有已经调用成功的服务。\n建议仅用于技术异常的情况。对于业务异常来讲，应该尽可能的去优化业务模式，以避免要求补偿事务。\n2.1 常用手段 在实现补偿模式时应该做到两点：\n 首先要确定失败的步骤和状态，从而确定要补偿的范围。 其次要能提供补偿操作使用的业务数据。  可以通过记录完整的业务流水的方法来实现上面两点要求。但是对于一个通用的补偿框架来说，预先知道微服务需要记录的业务要素是不可能的，那么就需要一种办法来保证业务流水的可扩展性，实践中主要有两种方法：大表和关联表。\n 大表，顾明思议就是设计时除了必须的字段外，还需要预留大量的备用字段，框架可以提供辅助工具来将业务数据映射到备用字段中。大表对于框架层实现起来比较简单，但是也有一些难点，比如预留多少个字段合适，每个字段又需要预留多长。还有一个难点是如果仅从数据层面来查询数据，很难一眼看出备用字段的业务含义，维护过程不友好。 关联表，分为技术表和业务表。技术表中保存为实现补偿操作所需要的技术数据，业务表中保存业务数据。通过在技术表中增加业务表名和业务表主键来建立和业务数据的关联。关联表更灵活，能支持不同业务类型记录不同的业务要素。但是在框架的实现上难度较高，每次查询都需要复杂的关联动作，性能会受到影响。  2.2 重试 补偿过程作为一个服务，在调用的时候也会出现不成功的情况，这时就要通过重试机制来保证补偿的成功率。因此要求补偿操作具有幂等性。\n但是也不是盲目的重试，我们需要根据服务执行失败的原因来选择不同的策略：\n 因业务因素导致失败，需要停止重试。 罕见的异常，如网络中断，传输过程中数据丢失，应该立即重试。 如果是因为系统繁忙，此时需要等待一段时间再重试。  2.3 不足 在补偿模式中有一个明显的缺陷是隔离性，从第一个服务开始一直到补偿完成，不一致性是对其他服务可见的。另外补偿模式过分依赖协调服务的健壮性，如果协调服务异常，则没办法达到一致性。\n3 TCC模式 TCC，是Try，Confirm和Cancel的缩写。一个完整的TCC业务一般是由一个主业务和若干个从业务组成。\n Try  完成所有业务检查 预留必须的业务资源   Confirm  真正执行业务 不做任何业务检查 只使用Try阶段预留的业务资源 满足幂等性   Cancel  释放Try阶段预留的业务资源 满足幂等性    3.1 实现过程 整个TCC业务分成两个阶段完成：\n第一阶段：主业务服务分别调用所有从业务的try操作，并在活动管理器中登记所有从业务服务。当所有从业务服务的try操作都调用成功或者某个从业务服务的try操作失败，进入第二阶段。\n第二阶段：活动管理器根据第一阶段的执行结果来执行Confirm或Cancel操作。如果第一阶段所有的try操作是成功的，则调用所有从业务的Confirm操作，否则都调用Cancel操作。\nTCC模式不再需要记录详细的业务流水，在一定程度上弥补了补偿模式的缺陷，在TCC模式中，直到明确的Confirm动作，所有的业务操作都是隔离的。而且还可以通过指定try的超时时间，主动的Cancel预留的资源，从而实现了自治。\n3.2 不足 TCC模式不能百分百保证一致性，如果某服务提交了Confirm成功，但是由于网络故障，导致主服务收到的失败，那么就会出现不一致性，这被称为heuristic exception。因此为保证成功率，都需要支持重试。\nheuristic exception是不可杜绝的，但是通过设置合理的超时时间、重试频率以及监控，可以使此异常的可能性降到很低，另外如果出现了此异常，还可通过人工手段补救。\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7%E7%9A%84%E5%AE%9E%E7%8E%B0%E6%89%8B%E6%AE%B5/","summary":"最终一致性的实现手段 实现最终一致性有三种手段：可靠事件模式、业务补偿模式和TCC模式\n1 可靠事件模式 可靠事件模式属于事件驱动架构，当某件重要的事情发生时，比如更新一个业务实体，微服务会向消息代理发布一个事件。消息代理会将订阅事件的微服务推送事件。\n要实现这种模式需要消息队列实现事件的持久化和at least once的可靠事件投递模式。\n1.1 本地事件表 本地事件表方法是将事件和业务数据保存在同一个数据库中，使用一个额外的事件恢复服务来恢复事件，由本地事物保证更新业务和发布事件的原子性。\n但是业务系统和事件系统耦合比较紧密，额外的事件数据库操作也会给数据库带来额外的压力，可能成为瓶颈。\n1.2 外部事件 此方法是将事件持久化到外部的事件系统，事件系统需要提供实时事件服务以接受微服务发布的事件，同时事件系统还需要提供事件恢复服务来确认恢复事件。\n1.3 不足 此过程可能出现重复消费的情况。\n2 补偿模式 一般来讲，异常一般是由以下两种情况造成的：\n业务异常：业务逻辑产生的错误，比如余额不足、库存不足等。\n技术异常：非业务逻辑产生的异常，比如网络连接异常、超时等。\n补偿模式就是使用一个额外的协调服务来协调各个需要保证一致性的其他服务。协调服务按顺序调用每一个服务，如果某个服务调用异常就取消之前所有已经调用成功的服务。\n建议仅用于技术异常的情况。对于业务异常来讲，应该尽可能的去优化业务模式，以避免要求补偿事务。\n2.1 常用手段 在实现补偿模式时应该做到两点：\n 首先要确定失败的步骤和状态，从而确定要补偿的范围。 其次要能提供补偿操作使用的业务数据。  可以通过记录完整的业务流水的方法来实现上面两点要求。但是对于一个通用的补偿框架来说，预先知道微服务需要记录的业务要素是不可能的，那么就需要一种办法来保证业务流水的可扩展性，实践中主要有两种方法：大表和关联表。\n 大表，顾明思议就是设计时除了必须的字段外，还需要预留大量的备用字段，框架可以提供辅助工具来将业务数据映射到备用字段中。大表对于框架层实现起来比较简单，但是也有一些难点，比如预留多少个字段合适，每个字段又需要预留多长。还有一个难点是如果仅从数据层面来查询数据，很难一眼看出备用字段的业务含义，维护过程不友好。 关联表，分为技术表和业务表。技术表中保存为实现补偿操作所需要的技术数据，业务表中保存业务数据。通过在技术表中增加业务表名和业务表主键来建立和业务数据的关联。关联表更灵活，能支持不同业务类型记录不同的业务要素。但是在框架的实现上难度较高，每次查询都需要复杂的关联动作，性能会受到影响。  2.2 重试 补偿过程作为一个服务，在调用的时候也会出现不成功的情况，这时就要通过重试机制来保证补偿的成功率。因此要求补偿操作具有幂等性。\n但是也不是盲目的重试，我们需要根据服务执行失败的原因来选择不同的策略：\n 因业务因素导致失败，需要停止重试。 罕见的异常，如网络中断，传输过程中数据丢失，应该立即重试。 如果是因为系统繁忙，此时需要等待一段时间再重试。  2.3 不足 在补偿模式中有一个明显的缺陷是隔离性，从第一个服务开始一直到补偿完成，不一致性是对其他服务可见的。另外补偿模式过分依赖协调服务的健壮性，如果协调服务异常，则没办法达到一致性。\n3 TCC模式 TCC，是Try，Confirm和Cancel的缩写。一个完整的TCC业务一般是由一个主业务和若干个从业务组成。\n Try  完成所有业务检查 预留必须的业务资源   Confirm  真正执行业务 不做任何业务检查 只使用Try阶段预留的业务资源 满足幂等性   Cancel  释放Try阶段预留的业务资源 满足幂等性    3.","title":"最终一致性的实现手段"},{"content":"Reactive微服务 分布式系统构建起来很困难，因为它们容易出问题，运行缓慢，并且被CAP和FLP理论所限制。换句话说，它们的构建和运维都特别复杂。为了解决这个问题，reactive便出现了。\nReactive编程：一种开发模型，其专注于数据流向、对变化的反馈，以及传播他们。\n在reactive编程中，刺激信号是数据的转移，叫做streams。其实很像生产者——消费者模式，消费者对值进行订阅并响应。\nReactive系统：一种架构风格，其基于异步消息来构建响应式的分布式系统。\nreactive系统使用了消息驱动的方法。所有的构建通过异步消息的发送和接收来交互。消息投递的逻辑由底层的实现决定。发送者不会阻塞着等待回复，它们可能会稍后才接收到回复。\nreactive系统会有两个重要的特征：\n  伸缩性——可以横向伸缩\n伸缩性来自消息传递的解耦。消息被发送到一个地址之后，可以被一组消费者按照一种负载均衡方法消费。当reactive系统遇到负载高峰时，它可以创造出新的消费者，并在此之后销毁它们。\n  恢复性——可以处理错误并且恢复\n首先，这种消息交互模式允许组件在其本地处理错误，组件不需要等待消息，因此当一个组件发生错误时，其他组件仍然会正常工作。其次，当一个处理消息的组件发生错误后，消息可以可以传递给在相同地址注册的其他组件。\n  reactive微服务系统是由reactive微服务组成的。这些微服务有下面四个特征：\n 自治性 异步性 恢复性 伸缩性  Reactive微服务是可自治的。他们可以根据周围的服务是否可用来调整自己的行为。自治性往往伴随着孤立性；Reactive微服务可以在本地处理错误、独立地完成任务，并在必要时和其他服务合作。它们使用异步消息传递的机制和其他服务沟通；它们也会接收消息并且对其作出回应。\n得益于异步消息机制，reactive微服务可以处理错误并根据情况调整自己的行为。错误不会被扩散，而是在靠近错误源头的地方被处理掉。当一个微服务挂掉之后，它的消费者微服务要能够处理错误并避免扩散。这一孤立原则是避免错误逐层上浮而毁掉整个系统的关键。可恢复性不只是关于处理错误，它还涉及到自愈性；一个reactive微服务应该能够从错误中恢复并且对错误进行补救。\n最后，reactive微服务必须是可伸缩的，这样系统才可以根据负载情况来调整节点数量。这一特性意味着将会有一系列的限制，比如不能有在内存中的状态，要能够在必要时同步状态信息，或者要能够将消息路由到状态信息相同的节点。\nVert.x Vert.x是一个用于构建reactive和分布式系统的工具箱，其使用了异步非阻塞编程模型。当使用Vert.x构建微服务的时候，微服务会自然地带上一个核心特征：所有事情都是异步的。\n传统编程模式\nint res = compute(1, 2); 在这段代码中，是在等待compute函数计算出来结果之后再进行剩下的操作。而在异步非阻塞的编程模式中，将会创建一个handler：\ncompute(1, 2, res -\u0026gt; { // called with the result }); 在上述代码中，compute函数不再返回一个结果，而是传一个handler，当结果准备好时调用就可以了。得益于这种开发模型，可以使用很少的线程去处理高并发工作。在vert.x中，到处都可以看到这种形式的代码，比如创建http服务器时：\nvertx.createHttpServer() .requestHandler(request -\u0026gt; { request.response().end(\u0026quot;hello vert.x\u0026quot;); }) .listen(8080); 这个例子中，我们让一个requestHandler接收HTTP请求(事件)并且返回\u0026quot;hello vert.x\u0026quot;。Handler是一个函数，当事件发生时，它会被调用。在我们的例子中，handler代码会在每次请求进来时被调用执行。要注意的是，Handler并不会返回一个结果，但是它可以提供一个结果；这个结果是怎样被提供的，这个要看是哪种交互行为。在上面的代码段中，它只是向一个HTTP response写入了结果。这个Handler后面跟了一个方法令其监听8080端口。调用这个HTTP服务它会返回一个简单的response。\nevent loop 绝大多数情况，Vert.x会用一个叫做event loop的线程来调用所有的handler。\n基于消息循环的线程模型有一个很大的优点：它简化了并发。因为只有一个线程存在，因此永远都只被一个线程调用而不存在并发的情况。但是同样也有一个限制：\n 不要阻塞消息循环\n 因为没有阻塞，一个消息循环线程可以短时间内分发巨量的事件，这个模式就叫做reactor模式。\nverticles Verticles是被Vert.x部署和运行的代码块。一个微服务的应用，是由运行在同一个Vert.x实例上的若干verticle组成的。一个verticle通常会创建服务器或客户端、注册一组Handler，以及封装一部分系统的业务处理逻辑。\n标准的verticle 标准的verticle会在Vert.x的消息循环中被执行，并且永远不会阻塞。Vert.x保证了每一个verticle都会只被同一个线程执行而不会有并发发生，从而避免同步工作。\nWorker Verticle 和标准的verticle不同，worker verticle不是在消息循环中执行的，这就意味着他们可以执行阻塞代码。但是，这会限制你的可扩展性。\nVerticle可以访问vertx成员变量(是由AbstractVerticle类提供的)来创建服务器和客户端，以及和其他的verticle交互。Verticle还可以部署其他的verticle，对它们进行配置，并设置创建实例的数量。这些实例会和不同的消息循环线程绑定，Vert.x通过这些实例来均衡负载。\n从Callbacks到Observables 我们可以发现，Vert.x开发模式使用回调方法。在组织管理多个异步动作时，这种基于回调的开发模式容易产生复杂的代码，陷入callback hell。\nVert.x提供了解决这个开发难题的答案——RxJava API。\n","permalink":"https://zhenfeng-zhu.github.io/posts/reactive%E5%BE%AE%E6%9C%8D%E5%8A%A1/","summary":"Reactive微服务 分布式系统构建起来很困难，因为它们容易出问题，运行缓慢，并且被CAP和FLP理论所限制。换句话说，它们的构建和运维都特别复杂。为了解决这个问题，reactive便出现了。\nReactive编程：一种开发模型，其专注于数据流向、对变化的反馈，以及传播他们。\n在reactive编程中，刺激信号是数据的转移，叫做streams。其实很像生产者——消费者模式，消费者对值进行订阅并响应。\nReactive系统：一种架构风格，其基于异步消息来构建响应式的分布式系统。\nreactive系统使用了消息驱动的方法。所有的构建通过异步消息的发送和接收来交互。消息投递的逻辑由底层的实现决定。发送者不会阻塞着等待回复，它们可能会稍后才接收到回复。\nreactive系统会有两个重要的特征：\n  伸缩性——可以横向伸缩\n伸缩性来自消息传递的解耦。消息被发送到一个地址之后，可以被一组消费者按照一种负载均衡方法消费。当reactive系统遇到负载高峰时，它可以创造出新的消费者，并在此之后销毁它们。\n  恢复性——可以处理错误并且恢复\n首先，这种消息交互模式允许组件在其本地处理错误，组件不需要等待消息，因此当一个组件发生错误时，其他组件仍然会正常工作。其次，当一个处理消息的组件发生错误后，消息可以可以传递给在相同地址注册的其他组件。\n  reactive微服务系统是由reactive微服务组成的。这些微服务有下面四个特征：\n 自治性 异步性 恢复性 伸缩性  Reactive微服务是可自治的。他们可以根据周围的服务是否可用来调整自己的行为。自治性往往伴随着孤立性；Reactive微服务可以在本地处理错误、独立地完成任务，并在必要时和其他服务合作。它们使用异步消息传递的机制和其他服务沟通；它们也会接收消息并且对其作出回应。\n得益于异步消息机制，reactive微服务可以处理错误并根据情况调整自己的行为。错误不会被扩散，而是在靠近错误源头的地方被处理掉。当一个微服务挂掉之后，它的消费者微服务要能够处理错误并避免扩散。这一孤立原则是避免错误逐层上浮而毁掉整个系统的关键。可恢复性不只是关于处理错误，它还涉及到自愈性；一个reactive微服务应该能够从错误中恢复并且对错误进行补救。\n最后，reactive微服务必须是可伸缩的，这样系统才可以根据负载情况来调整节点数量。这一特性意味着将会有一系列的限制，比如不能有在内存中的状态，要能够在必要时同步状态信息，或者要能够将消息路由到状态信息相同的节点。\nVert.x Vert.x是一个用于构建reactive和分布式系统的工具箱，其使用了异步非阻塞编程模型。当使用Vert.x构建微服务的时候，微服务会自然地带上一个核心特征：所有事情都是异步的。\n传统编程模式\nint res = compute(1, 2); 在这段代码中，是在等待compute函数计算出来结果之后再进行剩下的操作。而在异步非阻塞的编程模式中，将会创建一个handler：\ncompute(1, 2, res -\u0026gt; { // called with the result }); 在上述代码中，compute函数不再返回一个结果，而是传一个handler，当结果准备好时调用就可以了。得益于这种开发模型，可以使用很少的线程去处理高并发工作。在vert.x中，到处都可以看到这种形式的代码，比如创建http服务器时：\nvertx.createHttpServer() .requestHandler(request -\u0026gt; { request.response().end(\u0026quot;hello vert.x\u0026quot;); }) .listen(8080); 这个例子中，我们让一个requestHandler接收HTTP请求(事件)并且返回\u0026quot;hello vert.x\u0026quot;。Handler是一个函数，当事件发生时，它会被调用。在我们的例子中，handler代码会在每次请求进来时被调用执行。要注意的是，Handler并不会返回一个结果，但是它可以提供一个结果；这个结果是怎样被提供的，这个要看是哪种交互行为。在上面的代码段中，它只是向一个HTTP response写入了结果。这个Handler后面跟了一个方法令其监听8080端口。调用这个HTTP服务它会返回一个简单的response。\nevent loop 绝大多数情况，Vert.x会用一个叫做event loop的线程来调用所有的handler。\n基于消息循环的线程模型有一个很大的优点：它简化了并发。因为只有一个线程存在，因此永远都只被一个线程调用而不存在并发的情况。但是同样也有一个限制：\n 不要阻塞消息循环\n 因为没有阻塞，一个消息循环线程可以短时间内分发巨量的事件，这个模式就叫做reactor模式。\nverticles Verticles是被Vert.x部署和运行的代码块。一个微服务的应用，是由运行在同一个Vert.x实例上的若干verticle组成的。一个verticle通常会创建服务器或客户端、注册一组Handler，以及封装一部分系统的业务处理逻辑。","title":"Reactive微服务"},{"content":"Guice快速入门 接手的新项目主要是使用kotlin+vert.x来写的，使用gradle构建，依赖注入框架使用了guice。这段时间都是在熟悉代码的过程，恶补一些知识。\nguice是谷歌推出的一个轻量级的依赖注入框架，当然spring也可以实现依赖注入，只是spring太庞大了。\n1 基本使用 引入依赖 使用gradle或者maven，引入guice。\nmaven:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.inject\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guice\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradle:\ncompile \u0026quot;com.google.inject:guice:4.1.0\u0026quot; 项目骨架 首先需要一个业务接口，包含一个方法来执行业务逻辑，它的实现非常简单：\npackage com.learning.guice; public interface UserService { void process(); } package com.learning.guice; public class UserServiceImpl implements UserService { @Override public void process() { System.out.println(\u0026quot;我需要做一些业务逻辑\u0026quot;); } } 然后写一个日志的接口：\npackage com.learning.guice; public interface LogService { void log(String msg); } package com.learning.guice; public class LogServiceImpl implements LogService { @Override public void log(String msg) { System.out.println(\u0026quot;------LOG: \u0026quot; + msg); } } 最后是一个系统接口和相应的实现，在实现中使用了业务接口和日志接口处理业务逻辑和打印日志信息：\npackage com.learning.guice; public interface Application { void work(); } package com.learning.guice; import com.google.inject.Inject; public class MyApp implements Application { private UserService userService; private LogService logService; @Inject public MyApp(UserService userService, LogService logService) { this.userService = userService; this.logService = logService; } @Override public void work() { userService.process(); logService.log(\u0026quot;程序正常运行\u0026quot;); } } 配置依赖注入 guice是使用java代码来配置依赖。继承AbstractModule类，并重写其中的config方法。在config方法中，调用AbstractModule类中提供的方法来配置依赖关系。最常用的是bind(接口).to(实现类)。\npackage com.learning.guice; import com.google.inject.AbstractModule; public class MyAppModule extends AbstractModule { @Override protected void configure() { bind(LogService.class).to(LogServiceImpl.class); bind(UserService.class).to(UserServiceImpl.class); bind(Application.class).to(MyApp.class); } } 单元测试 guice配置完之后，我们需要调用Guice.createInjector方法传入配置类来创建一个注入器，然后使用注入器中的getInstance方法获取目标类。\npackage com.learning.guice; import com.google.inject.Guice; import com.google.inject.Injector; import org.junit.BeforeClass; import org.junit.Test; public class MyAppTest { private static Injector injector; @BeforeClass public static void init(){ injector= Guice.createInjector(new MyAppModule()); } @Test public void testMyApp(){ Application application=injector.getInstance(Application.class); application.work(); } } 程序执行结果是：\n/Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/bin/java -ea -... 我需要做一些业务逻辑 ------LOG: 程序正常运行 Process finished with exit code 0 2 基本概念 2.1 Bingdings 绑定   链式绑定\n在绑定依赖的时候不仅可以将父类和子类绑定，还可以将子类和子类的子类进行绑定。\npublic class BillingModule extends AbstractModule { @Override protected void configure() { bind(TransactionLog.class).to(DatabaseTransactionLog.class); bind(DatabaseTransactionLog.class).to(MySqlDatabaseTransactionLog.class); } } 在这种情况下，injector 会把所有 TransactionLog 替换为 MySqlDatabaseTransactionLog。\n  注解绑定\n当我们需要将多个同一类型的对象注入不同对象的时候，就需要使用注解区分这些依赖了。最简单的办法就是使用@Named注解进行区分。\n首先需要在要注入的地方添加@Named注解。\npublic class RealBillingService implements BillingService { @Inject public RealBillingService(@Named(\u0026quot;Checkout\u0026quot;) CreditCardProcessor processor, TransactionLog transactionLog) { ... } 然后在绑定中添加annotatedWith方法指定@Named中指定的名称。由于编译器无法检查字符串，所以Guice官方建议我们保守地使用这种方式。\nbind(CreditCardProcessor.class) .annotatedWith(Names.named(\u0026quot;Checkout\u0026quot;)) .to(CheckoutCreditCardProcessor.class);   实例绑定\n有时候需要直接注入一个对象的实例，而不是从依赖关系中解析。如果我们要注入基本类型的话只能这么做。\nbind(String.class) .annotatedWith(Names.named(\u0026quot;JDBC URL\u0026quot;)) .toInstance(\u0026quot;jdbc:mysql://localhost/pizza\u0026quot;); bind(Integer.class) .annotatedWith(Names.named(\u0026quot;login timeout seconds\u0026quot;)) .toInstance(10);   @Privides方法\n当一个对象很复杂，无法使用简单的构造器来生成的时候，我们可以使用@Provides方法，也就是在配置类中生成一个注解了@Provides的方法。在该方法中我们可以编写任意代码来构造对象。\n@Provides方法也可以应用@Named和自定义注解，还可以注入其他依赖，Guice会在调用方法之前注入需要的对象。\npublic class BillingModule extends AbstractModule { @Override protected void configure() { ... } @Provides TransactionLog provideTransactionLog() { DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setJdbcUrl(\u0026quot;jdbc:mysql://localhost/pizza\u0026quot;); transactionLog.setThreadPoolSize(30); return transactionLog; } }   Provider绑定\n如果项目中存在多个比较复杂的对象需要构建，使用@Provide方法会让配置类变得比较乱。我们可以使用Guice提供的Provider接口将复杂的代码放到单独的类中。办法很简单，实现Provider接口的get方法即可。在Provider类中，我们可以使用@Inject任意注入对象。\npublic class DatabaseTransactionLogProvider implements Provider\u0026lt;TransactionLog\u0026gt; { private final Connection connection; @Inject public DatabaseTransactionLogProvider(Connection connection) { this.connection = connection; } public TransactionLog get() { DatabaseTransactionLog transactionLog = new DatabaseTransactionLog(); transactionLog.setConnection(connection); return transactionLog; } } 然后在config方法中，调用.toProvider方法：\npublic class BillingModule extends AbstractModule { @Override protected void configure() { bind(TransactionLog.class) .toProvider(DatabaseTransactionLogProvider.class); } }   无目标绑定\n无目标绑定没有to子句\n  构造器绑定\n某些场景下，你能需要把某个类型绑定到任意一个构造函数上。以下情况会有这种需求：1、 @Inject 注解无法被应用到目标构造函数；2、目标类是一个第三方类；3、目标类有多个构造函数参与DI。\n为了解决这个问题，guice 提供了 toConstructor()绑定 ，它需要你指定要使用的确切的某个目标构造函数，并处理 \u0026ldquo;constructor annot be found\u0026rdquo; 异常：\npublic class BillingModule extends AbstractModule { @Override protected void configure() { try { bind(TransactionLog.class).toConstructor( DatabaseTransactionLog.class.getConstructor(DatabaseConnection.class)); } catch (NoSuchMethodException e) { addError(e); } } }   内置绑定\n除了显示绑定和即时绑定 just-in-time bindings，剩下的绑定都属于injector的内置绑定。这些绑定只能由injector自己创建，不允许外部调用。\n  即时绑定\n当 injector 需要某一个类型的实例的时候，它需要获取一个绑定。在Module类中的绑定叫做显式绑定，只要他们可用，injector 就会在任何时候使用它们。如果需要某一类型的实例，但是又没有显式绑定，那么injector将会试图创建一个即时绑定（Just-in-time Bindings），也被称为JIT绑定 或 隐式绑定。\n  2.2 作用域 默认情况下Guice会在每次注入的时候创建一个新对象。如果希望创建一个单例依赖的话，可以在实现类上应用@Singleton注解。\n@Singleton public class InMemoryTransactionLog implements TransactionLog { /* everything here should be threadsafe! */ } 或者也可以在配置类中指定。\nbind(TransactionLog.class).to(InMemoryTransactionLog.class).in(Singleton.class); 在@Provides方法中也可以指定单例。\n@Provides @Singleton TransactionLog provideTransactionLog() { ... } 如果一个类型上存在多个冲突的作用域，Guice会使用bind()方法中指定的作用域。如果不想使用注解的作用域，可以在bind()方法中将对象绑定为Scopes.NO_SCOPE。\nGuice和它的扩展提供了很多作用域，和spring一样，有单例Singleton，Session作用域SessionScoped，Request请求作用域RequestScoped等等。我们可以根据需要选择合适的作用域。\n2.3 注入 guice的注入和spring类似，而且还做了一些扩展。\n  构造器注入\n使用 @Inject 注解标记类的构造方法，这个构造方法需要接受类依赖作为参数。大多数构造子将会把接收到的参数分派给内部成员变量。\n  方法注入\nGuice 可以向标注了 @Inject 的方法中注入依赖。依赖项以参数的形式传给方法，Guice 会在调用注入方法前完成依赖项的构建。注入方法可以有任意数量的参数，并且方法名对注入操作不会有任何影响。\n  字段注入\n使用 @Inject 注解标记字段。这是最简洁的注入方式。\n注意：不能给final字段加@Inject注解。\n  可选注入\n有的时候，可能需要一个依赖项存在则进行注入，不存在则不注入。此时可以使用方法注入或字段注入来做这件事，当依赖项不可用的时候Guice 就会忽略这些注入。如果你需要配置可选注入的话，使用 @Inject(optional = true) 注解就可以了。\n  按需注入\n方法注入和字段注入可以可以用来初始化现有实例，你可以使用 Injector.injectMembers。\n这个不常用。\n  静态注入\n不建议使用静态注入。\n  自动注入\nGuice 会对以下情形做自动注入：\n 在绑定语句里，通过 toInstance() 注入实例。 在绑定语句里，通过 toProvider() 注入 Provider 实例。这些对象会在注入器创建的时候被创建并注入容器。如果它们需要满足其他启动注入，Guice 会在它们被使用前将他们注入进去。    2.4 AOP guice的aop功能较弱，时间原因还没研究透，后续继续写。\n","permalink":"https://zhenfeng-zhu.github.io/posts/guice%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","summary":"Guice快速入门 接手的新项目主要是使用kotlin+vert.x来写的，使用gradle构建，依赖注入框架使用了guice。这段时间都是在熟悉代码的过程，恶补一些知识。\nguice是谷歌推出的一个轻量级的依赖注入框架，当然spring也可以实现依赖注入，只是spring太庞大了。\n1 基本使用 引入依赖 使用gradle或者maven，引入guice。\nmaven:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.inject\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guice\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.1.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradle:\ncompile \u0026quot;com.google.inject:guice:4.1.0\u0026quot; 项目骨架 首先需要一个业务接口，包含一个方法来执行业务逻辑，它的实现非常简单：\npackage com.learning.guice; public interface UserService { void process(); } package com.learning.guice; public class UserServiceImpl implements UserService { @Override public void process() { System.out.println(\u0026quot;我需要做一些业务逻辑\u0026quot;); } } 然后写一个日志的接口：\npackage com.learning.guice; public interface LogService { void log(String msg); } package com.learning.guice; public class LogServiceImpl implements LogService { @Override public void log(String msg) { System.out.println(\u0026quot;------LOG: \u0026quot; + msg); } } 最后是一个系统接口和相应的实现，在实现中使用了业务接口和日志接口处理业务逻辑和打印日志信息：","title":"Guice快速入门"},{"content":"快速浏览一下 Kotlin 的语法。\n基本语法 包定义和引用 在源文件头部：\npackage my.demo import java.util.* 方法定义  带有方法体，并且返回确定类型数据的定义方式，例如接受 Int 类型的参数并返回 Int 类型的值：  fun sum(a: Int, b: Int): Int { return a + b }  带有方法体，返回推断类型数据的定义方式，例如：  fun sum(a: Int, b: Int) = a + b  返回无意义类型的定义方式：  fun printSum(a: Int, b: Int): Unit { println(\u0026#34;sum of $aand $bis ${a + b}\u0026#34;) } 或者省略 Unit：\nfun printSum(a: Int, b: Int) { println(\u0026#34;sum of $aand $bis ${a + b}\u0026#34;) } 变量定义  只赋值一次（只读）本地变量，val：  val a:Int = 1 // 指定初始值 val b = 2 // 类型自推断为 `Int` val c:Int // 当不指定初始值时需要指定类型 c = 3 // 延迟赋值   可变变量， var：  var x = 5 // 类型自推断为 `Int` x += 1  顶层变量  val PI = 3.14 var x = 0 fun incrementX() { x += 1 } 注释 与 Java 和 JavaScript 一样，Kotlin 支持行尾注释和块注释：\n// 行尾注释 /* 多行 块注释 */ 与 Java 不同，Kotlin 中的块注释可以嵌套。\nstring 模板 var a = 1 val s1 = \u0026#34;a is $a\u0026#34; a = 2 val s2 = \u0026#34;${s1.replace(\u0026#34;is\u0026#34;, \u0026#34;was\u0026#34;)}, but now is $a}\u0026#34; 条件表达式 fun maxOf(a:Int, b:Int): Int { if (a \u0026gt; b) { return a } else { return b } } 使用 if 做为表达式：\nfun maxOf(a:Int, b:Int) = if (a \u0026gt; b) a else b 可能为 null 的值，检查是否为 null 如果值可能为 null 时，必须显示的指出。 例如：\nfun parseInt(str: String): Int? { // ... } 使用上面定义的方法：\nfun printProduct(arg1: String, arg2: String) { val x = parseInt(arg1) val y = parseInt(arg2) if (x != null \u0026amp;\u0026amp; y != null) { println(x * y) } else { println(either \u0026#39;$arg1\u0026#39; or \u0026#39;$arg2\u0026#39; is not a number) } 或者：\nif (x == null) { println(\u0026#34;Wrong number format in arg1: \u0026#39;$arg1\u0026#39;\u0026#34;) return } if (y == null) { println(\u0026#34;Wrong number format in arg2: \u0026#39;$arg2\u0026#39;\u0026#34;) return } println(x * y) 类型检查和自动转换 is 操作符用于检查某个实例是否为某种类型。如果一个不可变本地变量或属性已经做过类型检查，那么可以不必显示的进行类型转换就可以使用对应类型的属性或方法。\nfun getStringLength(obj: Any): Int? { if (obj is String) { return obj.length // 在这个类型检查分支中，`obj` 自动转换为 `String`  } return null // 在类型检查分支外，`obj` 仍然为 `Any` } 或者：\nfun getStringLength(obj: Any): Int? { if (obj !is String) return null return obj.length // 在这个分支中，`obj` 自动转换为 `String` } 再或者：\nfun getStringLength(obj: Any): Int? { if (obj is String \u0026amp;\u0026amp; obj.length \u0026gt; 0) { // 在 `\u0026amp;\u0026amp;` 操作符的右侧，`obj` 自动转换为 `String`  return obj.length } return null } for 循环 val items = listOf(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;kiwi\u0026#34;) for (item in items) { println(item) } 或者：\nval items = listOf(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;kiwi\u0026#34;) for (index in items.indices) { println(\u0026#34;item at $indexis ${items[index]}\u0026#34;) } while 循环 val items = listOf(\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;kiwi\u0026#34;) var index = 0 while (index \u0026lt; items.size) { println(\u0026#34;item at $indexis ${items[index]}\u0026#34;) index ++ } when 表达式 fun describe(obj: Any): String = when (obj) { 1 -\u0026gt; \u0026#34;one\u0026#34; \u0026#34;hello\u0026#34; -\u0026gt; \u0026#34;Greeting\u0026#34; is Long -\u0026gt; \u0026#34;Long\u0026#34; !is String -\u0026gt; \u0026#34;Not a String\u0026#34; else -\u0026gt; \u0026#34;Unknown\u0026#34; } 区间  使用 in 操作符检查数字是否在区间内：  val x = 10 val y = 9 if (x in 1..y+1) { println(\u0026#34;fits in range\u0026#34;) }  检查数字是否在范围外：  val list = listOf(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) if (-1 !in 0..list.lastIndex) { println(\u0026#34;-1 is out of range\u0026#34;) } if (list.size !in list.indices) { println(\u0026#34;list size is out of valid list indices range too\u0026#34;) }  区间遍历  for (x in 1..5) { print(x) } 集合  遍历集合：  for (item in items) { println(item) }  使用 in 操作符判断集合中是否包含某个对象：  when { \u0026#34;orange\u0026#34; in items -\u0026gt; println(\u0026#34;juicy\u0026#34;) \u0026#34;apple\u0026#34; in items -\u0026gt; println(\u0026#34;apple is fine too\u0026#34;) }  使用 lambda 表达式过滤和 map 集合：  val fruits = listOf(\u0026#34;banana\u0026#34;, \u0026#34;avocado\u0026#34;, \u0026#34;apple\u0026#34;, \u0026#34;kiwi\u0026#34;) fruits .filter {it.startWith(\u0026#34;a\u0026#34;)} .sortedBy {it} .map {it.upperCase()} .forEach {println(it)} 创建基本类和实例 fun main(args: Array\u0026lt;String\u0026gt;) { val rectangle = Rectangle(5.0, 2.0) // 不需要使用 \u0026#39;new\u0026#39; 关键词  val triangle = Triangle(3.0, 4.0, 5.0) println(\u0026#34;Area of rectangle is ${rectangle.calculateArea()}, its perimeter is ${rectangle.perimeter}\u0026#34;) println(\u0026#34;Area of triangle is ${triangle.calculateArea()}, its perimeter is ${triangle.perimeter}\u0026#34;) } abstract class Shape(val sides: List\u0026lt;Double\u0026gt;) { val perimeter: Double get() = sides.sum() abstract fun calculateArea(): Double } interface RectangleProperties { val isSquare: Boolean } class Rectangle( var height: Double, var length: Double ) : Shape(listOf(height, length, height, length)), RectangleProperties { override val isSquare: Boolean get() = height == length override fun calculateArea(): Double = height * length } class Triangle( var sideA: Double, var sideB: Double, var sideC: Double ) : Shape(listOf(sideA, sideB, sideC)) { override fyb calculateArre(): Double { val s = perimeter / 2 return Math.sqrt(s * (s - sideA) * (s - sideB) * (s - sideC)) } }  以上引自：\n http://kotlinlang.org/docs/reference/basic-syntax.html\n ","permalink":"https://zhenfeng-zhu.github.io/posts/kotlin%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","summary":"快速浏览一下 Kotlin 的语法。\n基本语法 包定义和引用 在源文件头部：\npackage my.demo import java.util.* 方法定义  带有方法体，并且返回确定类型数据的定义方式，例如接受 Int 类型的参数并返回 Int 类型的值：  fun sum(a: Int, b: Int): Int { return a + b }  带有方法体，返回推断类型数据的定义方式，例如：  fun sum(a: Int, b: Int) = a + b  返回无意义类型的定义方式：  fun printSum(a: Int, b: Int): Unit { println(\u0026#34;sum of $aand $bis ${a + b}\u0026#34;) } 或者省略 Unit：\nfun printSum(a: Int, b: Int) { println(\u0026#34;sum of $aand $bis ${a + b}\u0026#34;) } 变量定义  只赋值一次（只读）本地变量，val：  val a:Int = 1 // 指定初始值 val b = 2 // 类型自推断为 `Int` val c:Int // 当不指定初始值时需要指定类型 c = 3 // 延迟赋值   可变变量， var：  var x = 5 // 类型自推断为 `Int` x += 1  顶层变量  val PI = 3.","title":"kotlin快速入门"},{"content":"RxJava2快速入门 引入依赖 compile 'io.reactivex.rxjava2:rxjava:2.0.1' 写法 简单版本 \tprivate static void helloSimple() { Consumer\u0026lt;String\u0026gt; consumer = new Consumer\u0026lt;String\u0026gt;() { @Override public void accept(String s) throws Exception { System.out.println(\u0026quot;consumer accept is \u0026quot; + s); } }; Observable.just(\u0026quot;hello world\u0026quot;).subscribe(consumer); } 复杂版本 \tprivate static void helloComplex() { Observer\u0026lt;String\u0026gt; observer = new Observer\u0026lt;String\u0026gt;() { @Override public void onSubscribe(Disposable d) { System.out.println(\u0026quot;onSubscribe: \u0026quot; + d); } @Override public void onNext(String s) { System.out.println(\u0026quot;onNext: \u0026quot; + s); } @Override public void onError(Throwable e) { System.out.println(\u0026quot;onError: \u0026quot; + e); } @Override public void onComplete() { System.out.println(\u0026quot;onComplete: \u0026quot;); } }; Observable.just(\u0026quot;Hello world\u0026quot;).subscribe(observer); } 变态版本 \tprivate static void helloPlus() { Observer\u0026lt;String\u0026gt; observer = new Observer\u0026lt;String\u0026gt;() { @Override public void onSubscribe(Disposable d) { System.out.println(\u0026quot;onSubscribe: \u0026quot; + d); } @Override public void onNext(String s) { System.out.println(\u0026quot;onNext: \u0026quot; + s); } @Override public void onError(Throwable e) { System.out.println(\u0026quot;onError: \u0026quot; + e); } @Override public void onComplete() { System.out.println(\u0026quot;onComplete: \u0026quot;); } }; Observable\u0026lt;String\u0026gt; observable = Observable.create(new ObservableOnSubscribe\u0026lt;String\u0026gt;() { @Override public void subscribe(ObservableEmitter\u0026lt;String\u0026gt; e) throws Exception { e.onNext(\u0026quot;hello world\u0026quot;); e.onComplete(); } }); observable.subscribe(observer); } 常用操作符 filter 你早上去吃早餐，师傅是被观察者，说咱这有\u0026quot;包子\u0026quot;, \u0026ldquo;馒头\u0026rdquo;, \u0026ldquo;花生\u0026rdquo;, \u0026ldquo;牛奶\u0026rdquo;, \u0026ldquo;饺子\u0026rdquo;, \u0026ldquo;春卷\u0026rdquo;, \u0026ldquo;油条\u0026rdquo;，你仔细想了想，发现你是最喜欢饺子的，所以把其他的都排除掉， 于是你就吃到了饺子。\n\tprivate static void helloFilter() { Consumer\u0026lt;String\u0026gt; consumer = new Consumer\u0026lt;String\u0026gt;() { @Override public void accept(String s) throws Exception { System.out.println(\u0026quot;accept: \u0026quot; + s); } }; Observable.just(\u0026quot;包子\u0026quot;, \u0026quot;馒头\u0026quot;, \u0026quot;花生\u0026quot;, \u0026quot;牛奶\u0026quot;, \u0026quot;饺子\u0026quot;, \u0026quot;春卷\u0026quot;, \u0026quot;油条\u0026quot;) .filter(new Predicate\u0026lt;String\u0026gt;() { @Override public boolean test(String s) throws Exception { System.out.println(\u0026quot;test: \u0026quot; + s); return s.equals(\u0026quot;饺子\u0026quot;); } }) .subscribe(consumer); } Map map操作符能够完成数据类型的转换。\n将String类型转换为Integer类型。\n\tprivate static void helloMap() { // 观察者观察Integer Observer\u0026lt;Integer\u0026gt; observer = new Observer\u0026lt;Integer\u0026gt;() { @Override public void onSubscribe(Disposable d) { System.out.println(\u0026quot;onSubscribe: \u0026quot; + d); } @Override public void onNext(Integer s) { System.out.println(\u0026quot;onNext: \u0026quot; + s); } @Override public void onError(Throwable e) { System.out.println(\u0026quot;onError: \u0026quot; + e); } @Override public void onComplete() { System.out.println(\u0026quot;onComplete: \u0026quot;); } }; Observable.just(\u0026quot;100\u0026quot;) .map(new Function\u0026lt;String, Integer\u0026gt;() { @Override public Integer apply(String s) throws Exception { return Integer.valueOf(s); } }) .subscribe(observer); } FlatMap flatmap能够链式地完成数据类型的转换和加工。\n遍历一个学校所有班级所有组的所有学生\nprivate void flatmapClassToGroupToStudent() { Observable.fromIterable(new School().getClasses()) //输入是Class类型，输出是ObservableSource\u0026lt;Group\u0026gt;类型 .flatMap(new Function\u0026lt;Class, ObservableSource\u0026lt;Group\u0026gt;\u0026gt;() { @Override public ObservableSource\u0026lt;Group\u0026gt; apply(Class aClass) throws Exception { Log.d(TAG, \u0026quot;apply: \u0026quot; + aClass.toString()); return Observable.fromIterable(aClass.getGroups()); } }) //输入类型是Group，输出类型是ObservableSource\u0026lt;Student\u0026gt;类型 .flatMap(new Function\u0026lt;Group, ObservableSource\u0026lt;Student\u0026gt;\u0026gt;() { @Override public ObservableSource\u0026lt;Student\u0026gt; apply(Group group) throws Exception { Log.d(TAG, \u0026quot;apply: \u0026quot; + group.toString()); return Observable.fromIterable(group.getStudents()); } }) .subscribe( new Observer\u0026lt;Student\u0026gt;() { @Override public void onSubscribe(Disposable d) { Log.d(TAG, \u0026quot;onSubscribe: \u0026quot;); } @Override public void onNext(Student value) { Log.d(TAG, \u0026quot;onNext: \u0026quot; + value.toString()); } @Override public void onError(Throwable e) { } @Override public void onComplete() { } }); } 线程调度 关于RxJava的线程调度，初学者只需要掌握两个api就够够的啦。\nsubscribeOn 指定Observable在一个指定的线程调度器上创建。只能指定一次，如果指定多次则以第一次为准\nobserveOn 指定在事件传递，转换，加工和最终被观察者接受发生在哪一个线程调度器。可指定多次，每次指定完都在下一步生效。\n常用线程调度器类型  Schedulers.single() 单线程调度器，线程可复用 Schedulers.newThread() 为每个任务创建新的线程 Schedulers.io() 处理io密集型任务，内部是线程池实现，可自动根据需求增长 Schedulers.computation() 处理计算任务，如事件循环和回调任务 AndroidSchedulers.mainThread() Android主线程调度器  ","permalink":"https://zhenfeng-zhu.github.io/posts/rxjava2%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/","summary":"RxJava2快速入门 引入依赖 compile 'io.reactivex.rxjava2:rxjava:2.0.1' 写法 简单版本 \tprivate static void helloSimple() { Consumer\u0026lt;String\u0026gt; consumer = new Consumer\u0026lt;String\u0026gt;() { @Override public void accept(String s) throws Exception { System.out.println(\u0026quot;consumer accept is \u0026quot; + s); } }; Observable.just(\u0026quot;hello world\u0026quot;).subscribe(consumer); } 复杂版本 \tprivate static void helloComplex() { Observer\u0026lt;String\u0026gt; observer = new Observer\u0026lt;String\u0026gt;() { @Override public void onSubscribe(Disposable d) { System.out.println(\u0026quot;onSubscribe: \u0026quot; + d); } @Override public void onNext(String s) { System.out.println(\u0026quot;onNext: \u0026quot; + s); } @Override public void onError(Throwable e) { System.","title":"RxJava2快速入门"},{"content":"在看项目代码的时候，发现了entity包和dto包，里面都是只保存数据的类，仔细查了资料，才发现java对于只保存数据的类有好几个分类。\n pojo类：这是普通的java类，具有一部分的get和set方法。 dto类：data transfer object 数据传输对象类，泛指用于展示层与服务层之间传输的对象。 vo类：vo有两种说法，一种是view object，一种是value object。 po类：persisent object 持久对象。和pojo类一样，也是只有get set方法，但是这种类一般是用于持久层。 bo类：business object，业务对象，表示应用程序领域内事物的所有实体类。 do类：domain object，领域对象，就是从现实中抽象出来的有形或者无形的业务实体。  根据我的经验来看，大部分人都没有分那么清楚，一般是把数据类放在domain包，或者entity包里。再细分一下的话，可以把dto类单独提取到一个包里。\n","permalink":"https://zhenfeng-zhu.github.io/posts/%E9%A2%86%E5%9F%9F%E5%AE%9E%E4%BD%93%E7%B1%BB/","summary":"在看项目代码的时候，发现了entity包和dto包，里面都是只保存数据的类，仔细查了资料，才发现java对于只保存数据的类有好几个分类。\n pojo类：这是普通的java类，具有一部分的get和set方法。 dto类：data transfer object 数据传输对象类，泛指用于展示层与服务层之间传输的对象。 vo类：vo有两种说法，一种是view object，一种是value object。 po类：persisent object 持久对象。和pojo类一样，也是只有get set方法，但是这种类一般是用于持久层。 bo类：business object，业务对象，表示应用程序领域内事物的所有实体类。 do类：domain object，领域对象，就是从现实中抽象出来的有形或者无形的业务实体。  根据我的经验来看，大部分人都没有分那么清楚，一般是把数据类放在domain包，或者entity包里。再细分一下的话，可以把dto类单独提取到一个包里。","title":"领域实体类"},{"content":"docker常用命令 docker   获取镜像\ndocker pull\n  新建并启动\ndocker run\n  列出镜像\ndocker image ls\ndocker images\n  删除虚悬镜像\ndocker image prune\n  删除本地镜像\ndocker iamge rm\n  查看应用信息\ndocker logs\n  dockerfile 一般步骤：  在一个目录里，新建一个文件，命名为Dockerfile 在Dockerfile的目录内，执行docker build  常用指令   FROM 指定基础镜像，且是第一条命令\n  RUN 执行命令\nshell格式\nexec格式\n  COPY和ADD指令是复制文件\n  CMD指令和RUN类似，容器启动命令\nshell格式\nexec格式\n参数列表格式\n  ENV 设置环境变量\n  EXPOSE 声明对外暴露的端口\n  WORKDIR 指定工作目录\n  compose 两个重要的概念  service 服务：一个应用的容器，实际上可以包括若干运行相同镜像的实例。 project 项目：由一组关联的容器组成一个完整业务单元，在docker-compose.yml文件中定义。  一般步骤：   在一个项目目录里，新建一个Dockerfile\n  新建一个文件docker-compose.yml\n模板格式\nversion: 3.0 services: web: build: . ports: - \u0026quot;5000:5000\u0026quot; redis: images: \u0026quot;redis:alpine\u0026quot;   docker-compose up运行项目\n  常用命令：  docker-compose build 重新构建项目中的服务容器 config 验证compose文件格式是否正确 down 停止up命令所启动的容器 images 列出compose文件中包含的镜像 exec 进入指定的容器 kill 强制停止服务容器 ps 列出目前所有容器 rm 删除停止状态的容器 top 显示所有容器的进程  compose模板文件： 每个服务都必须通过image指令指定镜像或者build指令（需要dockerfile）来构建生成的镜像。\n  build\n指定dockerfile所在的文件夹路径，compose将会利用它来自动构建这个镜像，然后使用。\n  depends_on\n解决容器的依赖和先后启动问题。但是不会等待完成启动之后再启动，而是在他们启动之后就去启动。\n  environment\n设置环境变量，在这里指定程序或者容器启动时所依赖的环境参数。\n  expose\n指定暴露的端口，只被连接的服务访问。\n  image\n指定镜像名称，如果本地不存在则去拉取这个镜像。\n  labels\n为容器添加docker元数据信息，即一些辅助说明。\n  ports\n暴露端口信息，宿主端口:容器端口，或者只指定容器端口。\n  ","permalink":"https://zhenfeng-zhu.github.io/posts/docker/","summary":"docker常用命令 docker   获取镜像\ndocker pull\n  新建并启动\ndocker run\n  列出镜像\ndocker image ls\ndocker images\n  删除虚悬镜像\ndocker image prune\n  删除本地镜像\ndocker iamge rm\n  查看应用信息\ndocker logs\n  dockerfile 一般步骤：  在一个目录里，新建一个文件，命名为Dockerfile 在Dockerfile的目录内，执行docker build  常用指令   FROM 指定基础镜像，且是第一条命令\n  RUN 执行命令\nshell格式\nexec格式\n  COPY和ADD指令是复制文件\n  CMD指令和RUN类似，容器启动命令\nshell格式\nexec格式\n参数列表格式\n  ENV 设置环境变量","title":"docker"},{"content":"Express 快速入门 安装 npm init npm install --save express hello world var express = require('express'); var app = express(); app.get('/', function (req, res) { res.send('Hello World!'); }); app.listen(3000, function () { console.log('Example app listening on port 3000!'); }); 执行命令运行应用程序\nnode app.js 然后，在浏览器中输入 http://localhost:3000/ 以查看输出。\nexpress程序生成器 安装 npm install -g express-generator 示例 以下语句在当前工作目录中创建名为 myapp 的 Express 应用程序：\nexpress --view=pug myapp 在 MacOS 或 Linux 上，采用以下命令运行此应用程序：\nDEBUG=myapp:* npm start 然后在浏览器中输入 http://localhost:3000/ 以访问此应用程序。\n路由 基本路由 路由用于确定应用程序如何响应对特定端点的客户机请求，包含一个 URI（或路径）和一个特定的 HTTP 请求方法（GET、POST 等）。\n每个路由可以具有一个或多个处理程序函数，这些函数在路由匹配时执行。\n路由定义采用以下结构：\napp.METHOD(PATH, HANDLER) 其中：\n app 是 express 的实例。 METHOD 是 HTTP 请求方法。 PATH 是服务器上的路径。 HANDLER 是在路由匹配时执行的函数。  比如简单的Hello world：\napp.get('/', function (req, res) { res.send('Hello World!'); }); 响应方法 下表中响应对象 (res) 的方法可以向客户机发送响应，并终止请求/响应循环。如果没有从路由处理程序调用其中任何方法，客户机请求将保持挂起状态。\n   方法 描述     res.download() 提示将要下载文件。   res.end() 结束响应进程。   res.json() 发送 JSON 响应。   res.jsonp() 在 JSONP 的支持下发送 JSON 响应。   res.redirect() 重定向请求。   res.render() 呈现视图模板。   res.send() 发送各种类型的响应。   res.sendFile 以八位元流形式发送文件。   res.sendStatus() 设置响应状态码并以响应主体形式发送其字符串表示。    app.route() 可以使用 app.route() 为路由路径创建可链接的路由处理程序。 因为在单一位置指定路径，所以可以减少冗余和输入错误。\napp.route('/book') .get(function(req, res) { res.send('Get a random book'); }) .post(function(req, res) { res.send('Add a book'); }) .put(function(req, res) { res.send('Update the book'); }); express.Router 使用 express.Router 类来创建可安装的模块化路由处理程序。Router 实例是完整的中间件和路由系统；因此，常常将其称为“微型应用程序”。\n以下示例将路由器创建为模块，在其中装入中间件，定义一些路由，然后安装在主应用程序的路径中。\n在应用程序目录中创建名为 birds.js 的路由器文件，其中包含以下内容：\nvar express = require('express'); var router = express.Router(); // middleware that is specific to this router router.use(function timeLog(req, res, next) { console.log('Time: ', Date.now()); next(); }); // define the home page route router.get('/', function(req, res) { res.send('Birds home page'); }); // define the about route router.get('/about', function(req, res) { res.send('About birds'); }); module.exports = router; 接着，在应用程序中装入路由器模块：\nvar birds = require('./birds'); ... app.use('/birds', birds); 此应用程序现在可处理针对 /birds 和 /birds/about 的请求，调用特定于此路由的 timeLog 中间件函数。\n中间件 中间件函数能够访问请求对象 (req)、响应对象 (res) 以及应用程序的请求/响应循环中的下一个中间件函数。下一个中间件函数通常由名为 next 的变量来表示。\n next() 函数不是 Node.js 或 Express API 的一部分，而是传递给中间件函数的第三自变量。next() 函数可以命名为任何名称，但是按约定，始终命名为“next”。\n 中间件函数可以执行以下任务：\n 执行任何代码。 对请求和响应对象进行更改。 结束请求/响应循环。 调用堆栈中的下一个中间件。  如果当前中间件函数没有结束请求/响应循环，那么它必须调用 next()，以将控制权传递给下一个中间件函数。否则，请求将保持挂起状态。\nExpress 应用程序可以使用以下类型的中间件：\n 应用层中间件 路由器层中间件 错误处理中间件 内置中间件 第三方中间件  模板引擎 在 Express 可以呈现模板文件之前，必须设置以下应用程序设置：\n views：模板文件所在目录。例如：app.set('views', './views') view engine：要使用的模板引擎。例如：app.set('view engine', 'pug')  然后安装对应的模板引擎 npm 包：\nnpm install pug --save 在 views 目录中创建名为 index.pug 的 Pug 模板文件，其中包含以下内容：\nhtml head title!= title body h1!= message 随后创建路由以呈现 index.pug 文件。如果未设置 view engine 属性，必须指定 view 文件的扩展名。否则，可以将其忽略。\napp.get('/', function (req, res) { res.render('index', { title: 'Hey', message: 'Hello there!'}); }); 向主页发出请求时，index.pug 文件将呈现为 HTML。\n","permalink":"https://zhenfeng-zhu.github.io/posts/express/","summary":"Express 快速入门 安装 npm init npm install --save express hello world var express = require('express'); var app = express(); app.get('/', function (req, res) { res.send('Hello World!'); }); app.listen(3000, function () { console.log('Example app listening on port 3000!'); }); 执行命令运行应用程序\nnode app.js 然后，在浏览器中输入 http://localhost:3000/ 以查看输出。\nexpress程序生成器 安装 npm install -g express-generator 示例 以下语句在当前工作目录中创建名为 myapp 的 Express 应用程序：\nexpress --view=pug myapp 在 MacOS 或 Linux 上，采用以下命令运行此应用程序：\nDEBUG=myapp:* npm start 然后在浏览器中输入 http://localhost:3000/ 以访问此应用程序。","title":"express"},{"content":"一直没有机会写2017的年终总结，想到去年写的新的一年的计划，好像自己都没有按照计划来做，而且写的计划也不知道写到哪里去了。\n站在现在的时间点去审视过去的一年，这个本命年还是发生了很多对自己的未来有着比较大影响的的事情。房子+女朋友+新工作，这些事情突然的涌现出来，搞得自己有些手忙脚乱。\n梳理一下自己的收获吧：\n首先当然是结识了一帮小伙伴，我们一起打农药，一起调bug，一起奋战双十一。\n在技术上也有了一定的提升，关键是自己的视野上有了很大的变化，不再是像当初大学的时候，不知道自己在做什么。这里对我影响比较大的一个是phodal，看了他写的博客和书之后，对自己的触动很大，感觉他懂得很多东西，知识面很广，而且能够写出来，扩大了自己的影响力，所以我就想着自己能不能模仿他。另外一个就是田哥了，我觉得他是我认识的同龄人中，比较有自己想法的一个人，打进acm world final的人就是不一样，他看问题的角度比较新颖，而且归纳总结能力很强，给了我一些在编程上的指点，让我少走了很多弯路。\n也很感谢自己的几个室友，让我不再感到孤单。自从刘巍来了深圳之后，11I更欢乐了，也更污了。我们经常在家里煮火锅吃，吃的特别爽，以至于现在我都不想去火锅店里吃，总觉得在家吃的比较爽。\n女朋友，出乎我的意料，现在想想还是感觉活在梦里，略过略过。\n至于买房，没买之前的想法盲目乐观，后来算了一下，要是在深圳买房的话，我每个月的房贷是2万多，关键是首付还不一定能搞得出来。还是建议在北上广深工作的人，有机会现在老家的省会一类的城市，先搞一套，以后可以置换，能上车的时候早点上车，也是相当于变相攒钱了。对于不会投资或者创业的朋友，有时候辛辛苦苦干一年之后算一下，不知不觉中，自己的钱都不知道花在了哪里，如果有房贷的话，等用的时候说不定卖掉还能赚一些钱。\n感觉自己换公司还是挺戏剧化的，当时也没想着真的就换工作吧，只是想投投简历，然后去面试一波看看自己的水平怎么样。总觉得自己在招银的舒适区待的太久了，没有什么激情了。同时自己也想出来试试，万一公司上市，摇身一变成为富翁了。当然这是白日梦了，路还是要一步一步的走的。\n总感觉自己想了很多东西，但是就是写不出来，自己讲故事的能力还是要提升一些。\n","permalink":"https://zhenfeng-zhu.github.io/posts/a-month-in-finogeeks/","summary":"一直没有机会写2017的年终总结，想到去年写的新的一年的计划，好像自己都没有按照计划来做，而且写的计划也不知道写到哪里去了。\n站在现在的时间点去审视过去的一年，这个本命年还是发生了很多对自己的未来有着比较大影响的的事情。房子+女朋友+新工作，这些事情突然的涌现出来，搞得自己有些手忙脚乱。\n梳理一下自己的收获吧：\n首先当然是结识了一帮小伙伴，我们一起打农药，一起调bug，一起奋战双十一。\n在技术上也有了一定的提升，关键是自己的视野上有了很大的变化，不再是像当初大学的时候，不知道自己在做什么。这里对我影响比较大的一个是phodal，看了他写的博客和书之后，对自己的触动很大，感觉他懂得很多东西，知识面很广，而且能够写出来，扩大了自己的影响力，所以我就想着自己能不能模仿他。另外一个就是田哥了，我觉得他是我认识的同龄人中，比较有自己想法的一个人，打进acm world final的人就是不一样，他看问题的角度比较新颖，而且归纳总结能力很强，给了我一些在编程上的指点，让我少走了很多弯路。\n也很感谢自己的几个室友，让我不再感到孤单。自从刘巍来了深圳之后，11I更欢乐了，也更污了。我们经常在家里煮火锅吃，吃的特别爽，以至于现在我都不想去火锅店里吃，总觉得在家吃的比较爽。\n女朋友，出乎我的意料，现在想想还是感觉活在梦里，略过略过。\n至于买房，没买之前的想法盲目乐观，后来算了一下，要是在深圳买房的话，我每个月的房贷是2万多，关键是首付还不一定能搞得出来。还是建议在北上广深工作的人，有机会现在老家的省会一类的城市，先搞一套，以后可以置换，能上车的时候早点上车，也是相当于变相攒钱了。对于不会投资或者创业的朋友，有时候辛辛苦苦干一年之后算一下，不知不觉中，自己的钱都不知道花在了哪里，如果有房贷的话，等用的时候说不定卖掉还能赚一些钱。\n感觉自己换公司还是挺戏剧化的，当时也没想着真的就换工作吧，只是想投投简历，然后去面试一波看看自己的水平怎么样。总觉得自己在招银的舒适区待的太久了，没有什么激情了。同时自己也想出来试试，万一公司上市，摇身一变成为富翁了。当然这是白日梦了，路还是要一步一步的走的。\n总感觉自己想了很多东西，但是就是写不出来，自己讲故事的能力还是要提升一些。","title":"碎碎念"},{"content":"java内存模型和线程  并发不一定依赖多线程，但是在java里面谈论并发，大多与线程脱不开关系。\n 线程是大多是面试都会问到的问题。我们都知道，线程是比进程更轻量级的调度单位，线程之间可以共享内存。之前面试的时候，也是这样回答，迷迷糊糊，没有一个清晰的概念。\n大学的学习的时候，写C和C++，自己都没有用过多线程，看过一个Windows编程的书，里面讲多线程的时候，一大堆大写的字母，看着一点都不爽，也是惭愧。后来的实习，写unity，unity的C#使用的是协程。只有在做了java后端之后，才知道线程到底是怎么用的。了解了java内存模型之后，仔细看了一些资料，对java线程有了更深入的认识，整理写成这篇文章，用来以后参考。\n1 Java内存模型 Java虚拟机规范试图定义一种java内存模型来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能达到一致性内存访问的效果。\njava内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量的底层细节。（这里所说的变量包括了实例字段、静态字段和数组等，但不包括局部变量与方法参数，因为这些是线程私有的，不被共享。）\n1.1 主内存和工作内存 java规定所有的变量都存储在主内存。每条线程有自己的工作内存。\n线程的工作内存中的变量是主内存中该变量的副本，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。不同线程间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。\n1.2 内存之间的交互 关于主内存和工作内存之间的具体交互协议，java内存模型定义了8中操作来完成，虚拟机实现的时候必须保证每个操作都是原子的，不可分割的（对于long和double有例外）\n lock锁定：作用于主内存变量，代表一个变量是一条线程独占。 unlock解锁：作用于主内存变量，把锁定的变量解锁。 read读取：作用于主内存变量，把变量值从主内存传到线程的工作内存中，供load使用。 load载入：作用工作内存变量，把上一个read到的值放入到工作内存中的变量中。 use使用：作用于工作内存变量，把工作内存中的一个变量的值传递给执行引擎。 assign：作用于工作内存变量，把执行引擎执行过的值赋给工作内存中的变量。 store存储：作用于工作内存变量，把工作内存中的变量值传给主内存，供write使用。  这些操作要满足一定的规则。\n1.3 volatile volatile可以说是java的最轻量级的同步机制。\n当一个变量被定义为volatile之后，他就具备两种特性：\n  保证此变量对所有线程都是可见的\n这里的可见性是指当一个线程修改了某变量的值，新值对于其他线程来讲是立即得知的。而普通变量做不到，因为普通变量需要传递到主内存中才可以做到这点。\n  禁止指令重排\n对于普通变量来说，仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执性顺序一致。\n若用volatile修饰变量，在编译时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。\n  volatile对于单个的共享变量的读/写具有原子性，但是像num++这种复合操作，volatile无法保证其原子性。\n1.4 long和double long和double是一个64位的数据类型。\n虚拟机允许将没有被volatile修饰的64位变量的读写操作分为两次32位的操作来进行。因此当多个线程操作一个没有声明为volatile的long或者double变量，可能出现操作半个变量的情况。\n但是这种情况是罕见的，一般商用的虚拟机都是讲long和double的读写当成原子操作进行的，所以在写代码时不需要将long和double专门声明为volatile。\n1.5 原子性、可见性和有序性 java的内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性。\n原子性\n基本数据类型的访问读写是剧本原子性的。\n如果需要一个更大范围的原子性保证，java提供了lock和unlock操作，对应于写代码时就是synchronized关键字，因此在synchronized块之间的操作也是具备原子性的。\n可见性\n可见性是指当一个线程修改到了一个共享变量的值，其他的线程能够立即得知这个修改。共享变量的读写都是通过主内存作为媒介来处理可见性的。\nvolatile的特殊规则保证了新值可以立即同步到主内存，每次使用前立即从主内存刷新。\nsynchronized同步块的可见性是由”对于一个变量unlock操作之前，必须先把此变量同步回内存中“来实现的。\nfinal的可见性是指被final修饰的字段在构造器中一旦初始化完成，并且构造器没有把this的引用传递出去，那么在其他线程中就能看见final字段的值。\n有序性\n如果在本线程内观察，所有的操作都是有序的；如果在一个线程内观察另一个线程，所有的操作都是无序的。 volatile关键字本身就包含了禁止指令重排的语义，而synchronized则是由“一个变量在同一时刻只允许一条线程对其进行lock操作”这条规则来实现有序性的。\n1.6 先行发生原则 如果java内存模型中的所有有序性都是靠着volatile和synchronized来完成，那有些操作将会变得很繁琐，但是我们在写java并发代码的时候没有感受到这一点，都是因为java有一个“先行发生”原则。\n先行发生是java内存模型中定义的两项操作之间的偏序关系，如果说操作A先发生于操作B，其实就是说在发生B之前，A产生的影响都能被B观察到，这里的影响包括修改了内存中共享变量的值、发送了消息、调用了方法等等。\n  程序次序规则\n在一个线程内，按程序代码控制流顺序执行。\n  管程锁定规则\nunlock发生在后面时间同一个锁的lock操作。\n  volatile变量规则\nvolatile变量的写操作发生在后面时间的读操作。\n  线程启动规则\n  线程终止规则\n  线程中断规则\n  对象终结规则\n一个对象的初始化完成在finalize方法之前。\n  传递性\n如果A先行发生B，B先行发生C，那么A先行发生C。\n  由于指令重排的原因，所以一个操作的时间上的先发生，不代表这个操作就是先行发生；同样一个操作的先行发生，也不代表这个操作必定在时间上先发生。\n2 Java线程 2.1 线程的实现 主流的操作系统都提供了线程的实现，java则是在不同的硬件和操作系统的平台下，对线程的操作提供了统一的处理，一个Thread类的实例就代表了一个线程。Thread类的关键方法都是native的，所以java的线程实现也都是依赖于平台相关的技术手段来实现的。\n实现线程主要有3种方式：使用内核线程实现，使用用户线程实现和使用用户线程加轻量级进程实现。\n2.1.1 使用内核线程实现 内核线程就是直接由操作系统内核支持的线程，这种线程由内核来完成线程的切换，内核通过操纵调度器对线程进行调度，并负责将线程的任务映射到各个处理器上。\n程序一般不会直接去调用内核线程，而是使用内核线程的一个高级接口——轻量级进程（Light Weigh Process），LWP就是我们通常意义上所说的线程。\n由于每个轻量级进程都由一个内核线程支持，这种轻量级进程与内核线程之间1:1的关系成为一对一线程模型。\n局限性\n虽然由于内核线程的支持，每个轻量级进程都成为了一个独立的调度单元，即使有一个阻塞，也不影响整个进程的工作，但是还是有一定的局限性：\n  系统调用代价较高\n由于基于内核线程实现，所以各种线程的操作都要进行系统调用。而系统调用的代价比较高，需要在用户态和内核态来回切换。\n  系统支持数量有限\n每个轻量级进程都需要一个内核线程支持，需要消耗一定的内核资源，所以支持的线程数量是有限的。\n  2.1.2 使用用户线程实现 指的是完全建立在用户空间的线程库上，系统内核不能感知线程存在的实现。用户线程的建立、同布、销毁和调度完全在用户态中完成，不需要内核帮助。\n如果程序实现得当，则这些线程都不需要切换到内核态，操作非常快速消耗低，可以支持大规模线程数量。这种进程和用户线程之间1:N的关系成为一对多线程模型。\n局限性\n不需要系统内核的，既是优势也是劣势。由于没有系统内核支援，所有的操作都需要程序去处理，由于操作系统只是把处理器资源分给进程，那“阻塞如何处理”、“多处理器系统如何将线程映射到其他处理器上”这类问题的解决十分困难，所以现在使用用户线程的越来越少了。\n2.1.3 使用用户线程加轻量级进程混合实现 在这种混合模式下，既存在用户线程，也存在轻量级进程。\n用户线程还是完全建立在用户空间中，因此用户线程的创建、切换、析构等操作依然廉价，而且支持大规模用户线程并发、而操作系统提供支持的轻量级进程则作为用户线程和内核线程之间的桥梁，这样可以使用内核提供的线程调度和处理器映射，并且用户线程的系统调用要通过轻量级进程来完成，大大降低了整个进程被完全阻塞的风险。\n在这种模式下，用户线程和轻量级进程数量比不固定N:M，这种模式就是多对多线程模型。\n2.1.4 java线程的实现 目前的jdk版本中，操作系统支持怎样的线程模型，很大程度上就决定了jvm的线程是怎么映射的，这点在不同的平台没办法打成一致。线程模型只对线程的并发规模和操作成本产生影响，对编码和运行都没什么差异。\nwindows和linux都是一对一的线程模型。\n2.2 线程调度 线程的调度是指系统为线程分配处理器使用权的过程，主要的调度方式有两种：协同式线程调度和抢占式线程调度。\n2.2.1 协同式线程调度 线程的执性时间由线程本身来控制，线程把自己的工作执性完了之后，要主动通知系统切换到另外一个线程上。Lua的协程就是这样。\n好处\n协同式多线程最大的好处就是实现简单。\n由于线程要把自己的事情干完之后才进行线程切换，切换操作对线程是克制的，所以没有什么线程同步的问题。\n坏处\n坏处也很明显，线程执行时间不可控。甚至如果一个线程写的问题，一直不告诉系统切换，那程序就会一直阻塞。\n2.2.2 抢占式线程调度 每个线程由系统分配执行时间，线程的切换不是又线程本身来决定。\n使用yield方法是可以让出执行时间，但是要获取执行时间，线程本身是没有什么办法的。\n在这种调度模式下，线程的执行时间是系统可控的，也就不会出现一个线程导致整个进程阻塞。\n2.2.3 java线程调度 java使用的是抢占式线程调度。\n虽然java的线程调度是系统来控制的，但是可以通过设置线程优先级的方式，让某些线程多分配一些时间，某些线程少分配一些时间。\n不过线程优先级还是不太靠谱，原因就是java的线程是通过映射到系统的原生线程来实现的，所以线程的调度还是取决于操作系统，操作系统的线程优先级不一定和java的线程优先级一一对应。而且优先级还可能被系统自行改变。所以我们不能在程序中通过优先级来准确的判断先执行哪一个线程。\n2.3 线程的状态转换 看到网上有好多种说法，不过大致也都是说5种状态：新建（new）、可运行（runnable）、运行（running）、阻塞（blocked）和死亡（dead）。\n而深入理解jvm虚拟机中说java定义了5种线程状态，在任一时间点，一个线程只能有其中的一种状态：\n  新建new\n  运行runnable\n包括了操作系统线程状态的running和ready，也就是说处于此状态的线程可能正在执行，也可能正在等待cpu给分配执行时间。\n  无限期等待waiting\n处于这种状态的线程不会被cpu分配执行时间，需要被其他线程显示唤醒，能够导致线程陷入无限期等待的方法有：\n 没有设置timeout参数的wait方法。 没有设置timeout参数的join方法。 LockSupport.park方法。    限期等待timed waiting\n处于这种状态的线程也不会被cpu分配执行时间，不过不需要被其他线程显示唤醒，是经过一段时间之后，被操作系统自动唤醒。能够导致线程陷入限期等待的方法有：\n sleep方法。 设置timeout参数的wait方法。 设置参数的join方法。 LockSupport.parkNanos方法。 LockSupport.parkUntil方法。    阻塞blocked\n线程被阻塞了。在线程等待进入同步区域的时候是这个状态。\n阻塞和等待的区别是：阻塞是排队等待获取一个排他锁，而等待是指等一段时间或者一个唤醒动作。\n  结束terminated\n已经终止的线程。\n  3 写在最后 并发处理的广泛应用是使得Amdahl定律代替摩尔定律成为计算机性能发展源动力的根本原因，也是人类压榨计算机运算能力的最有力武器。有些问题使用越多的资源就能越快地解决——越多的工人参与收割庄稼，那么就能越快地完成收获。但是另一些任务根本就是串行化的——增加更多的工人根本不可能提高收割速度。\n我们使用线程的重要原因之一是为了支配多处理器的能力，我们必须保证问题被恰当地进行了并行化的分解，并且我们的程序有效地使用了这种并行的潜能。有时候良好的设计原则不得不向现实做出一些让步，我们必须让计算机正确无误的运行，首先保证并发的正确性，才能够在此基础上谈高效，所以线程的安全问题是一个很值得考虑的问题。\n虽然一直说java不好，但是java带给我的影响确实最大的，从java这个平台里学到了很多有用的东西。现在golang，nodejs，python等语言，每个都是在一方面能秒java，可是java生态和java对软件行业的影响，是无法被超越的，java这种语言，从出生到现在几十年了，基本上每次软件技术的革命都没有落下，每次都觉得要死的时候，忽然间柳暗花明，枯木逢春。咳咳，扯远了。\n","permalink":"https://zhenfeng-zhu.github.io/posts/java-memory-thread/","summary":"java内存模型和线程  并发不一定依赖多线程，但是在java里面谈论并发，大多与线程脱不开关系。\n 线程是大多是面试都会问到的问题。我们都知道，线程是比进程更轻量级的调度单位，线程之间可以共享内存。之前面试的时候，也是这样回答，迷迷糊糊，没有一个清晰的概念。\n大学的学习的时候，写C和C++，自己都没有用过多线程，看过一个Windows编程的书，里面讲多线程的时候，一大堆大写的字母，看着一点都不爽，也是惭愧。后来的实习，写unity，unity的C#使用的是协程。只有在做了java后端之后，才知道线程到底是怎么用的。了解了java内存模型之后，仔细看了一些资料，对java线程有了更深入的认识，整理写成这篇文章，用来以后参考。\n1 Java内存模型 Java虚拟机规范试图定义一种java内存模型来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让java程序在各种平台下都能达到一致性内存访问的效果。\njava内存模型的主要目标是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量的底层细节。（这里所说的变量包括了实例字段、静态字段和数组等，但不包括局部变量与方法参数，因为这些是线程私有的，不被共享。）\n1.1 主内存和工作内存 java规定所有的变量都存储在主内存。每条线程有自己的工作内存。\n线程的工作内存中的变量是主内存中该变量的副本，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存中的变量。不同线程间也无法直接访问对方工作内存中的变量，线程间变量值的传递需要通过主内存来完成。\n1.2 内存之间的交互 关于主内存和工作内存之间的具体交互协议，java内存模型定义了8中操作来完成，虚拟机实现的时候必须保证每个操作都是原子的，不可分割的（对于long和double有例外）\n lock锁定：作用于主内存变量，代表一个变量是一条线程独占。 unlock解锁：作用于主内存变量，把锁定的变量解锁。 read读取：作用于主内存变量，把变量值从主内存传到线程的工作内存中，供load使用。 load载入：作用工作内存变量，把上一个read到的值放入到工作内存中的变量中。 use使用：作用于工作内存变量，把工作内存中的一个变量的值传递给执行引擎。 assign：作用于工作内存变量，把执行引擎执行过的值赋给工作内存中的变量。 store存储：作用于工作内存变量，把工作内存中的变量值传给主内存，供write使用。  这些操作要满足一定的规则。\n1.3 volatile volatile可以说是java的最轻量级的同步机制。\n当一个变量被定义为volatile之后，他就具备两种特性：\n  保证此变量对所有线程都是可见的\n这里的可见性是指当一个线程修改了某变量的值，新值对于其他线程来讲是立即得知的。而普通变量做不到，因为普通变量需要传递到主内存中才可以做到这点。\n  禁止指令重排\n对于普通变量来说，仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中的执性顺序一致。\n若用volatile修饰变量，在编译时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。\n  volatile对于单个的共享变量的读/写具有原子性，但是像num++这种复合操作，volatile无法保证其原子性。\n1.4 long和double long和double是一个64位的数据类型。\n虚拟机允许将没有被volatile修饰的64位变量的读写操作分为两次32位的操作来进行。因此当多个线程操作一个没有声明为volatile的long或者double变量，可能出现操作半个变量的情况。\n但是这种情况是罕见的，一般商用的虚拟机都是讲long和double的读写当成原子操作进行的，所以在写代码时不需要将long和double专门声明为volatile。\n1.5 原子性、可见性和有序性 java的内存模型是围绕着在并发过程中如何处理原子性、可见性和有序性。\n原子性\n基本数据类型的访问读写是剧本原子性的。\n如果需要一个更大范围的原子性保证，java提供了lock和unlock操作，对应于写代码时就是synchronized关键字，因此在synchronized块之间的操作也是具备原子性的。\n可见性\n可见性是指当一个线程修改到了一个共享变量的值，其他的线程能够立即得知这个修改。共享变量的读写都是通过主内存作为媒介来处理可见性的。\nvolatile的特殊规则保证了新值可以立即同步到主内存，每次使用前立即从主内存刷新。\nsynchronized同步块的可见性是由”对于一个变量unlock操作之前，必须先把此变量同步回内存中“来实现的。\nfinal的可见性是指被final修饰的字段在构造器中一旦初始化完成，并且构造器没有把this的引用传递出去，那么在其他线程中就能看见final字段的值。\n有序性\n如果在本线程内观察，所有的操作都是有序的；如果在一个线程内观察另一个线程，所有的操作都是无序的。 volatile关键字本身就包含了禁止指令重排的语义，而synchronized则是由“一个变量在同一时刻只允许一条线程对其进行lock操作”这条规则来实现有序性的。\n1.6 先行发生原则 如果java内存模型中的所有有序性都是靠着volatile和synchronized来完成，那有些操作将会变得很繁琐，但是我们在写java并发代码的时候没有感受到这一点，都是因为java有一个“先行发生”原则。\n先行发生是java内存模型中定义的两项操作之间的偏序关系，如果说操作A先发生于操作B，其实就是说在发生B之前，A产生的影响都能被B观察到，这里的影响包括修改了内存中共享变量的值、发送了消息、调用了方法等等。\n  程序次序规则\n在一个线程内，按程序代码控制流顺序执行。","title":"Java内存模型和线程"},{"content":"一些基础概念\nEC2 云服务器，可以理解成虚拟机，新建一个实例，就是新建一个虚拟机并安装操作系统（Linux或者windows）。\nVPC Virtual Private Cloud。可以理解成数据中心，机房。对于灾备或者双活需要的，可以创建两个VPC。\n子网 一个VPC里可以有多个子网。比如某机构的一个VPC可以办公网和生产网段，或者内网和外网。一般外网可以被访问，内网的话可以是数据库的服务器之类的。\nIAM角色 类似于用户，可以被分配权限。\n安全组 控制连接到此EC2实例的流量，或者是控制对外暴露的端口。\nCIDR CIDR主要是一个按位的、基于前缀的，用于解释IP地址的标准。当用二进制表示这些地址时，它们有着在开头部分的一系列相同的位。\nIPv4的CIDR地址块的表示方法和IPv4地址的表示方法是相似的：由四部分组成的点分十进制地址，后跟一个斜线，最后是范围在0到32之间的一个数字：A.B.C.D/N。点分十进制的部分和IPv4地址一样是一个被分成四个八位位组的32位二进制数。斜线后面的数字就是前缀长度，也就是从左到右，被地址块里的地址所共享的位的数目。\n十进制部分有时会被省略，因此，/20就表示一个前缀长度是20的CIDR地址块。如果一个IP地址的前N位与一个CIDR地址块的前缀是相同的话，那么就说这个地址属于这个CIDR地址块，也可以说是与CIDR地址块的前缀匹配。所以，要理解CIDR，就要把地址写成二进制的形式。\n","permalink":"https://zhenfeng-zhu.github.io/posts/aws-md/","summary":"一些基础概念\nEC2 云服务器，可以理解成虚拟机，新建一个实例，就是新建一个虚拟机并安装操作系统（Linux或者windows）。\nVPC Virtual Private Cloud。可以理解成数据中心，机房。对于灾备或者双活需要的，可以创建两个VPC。\n子网 一个VPC里可以有多个子网。比如某机构的一个VPC可以办公网和生产网段，或者内网和外网。一般外网可以被访问，内网的话可以是数据库的服务器之类的。\nIAM角色 类似于用户，可以被分配权限。\n安全组 控制连接到此EC2实例的流量，或者是控制对外暴露的端口。\nCIDR CIDR主要是一个按位的、基于前缀的，用于解释IP地址的标准。当用二进制表示这些地址时，它们有着在开头部分的一系列相同的位。\nIPv4的CIDR地址块的表示方法和IPv4地址的表示方法是相似的：由四部分组成的点分十进制地址，后跟一个斜线，最后是范围在0到32之间的一个数字：A.B.C.D/N。点分十进制的部分和IPv4地址一样是一个被分成四个八位位组的32位二进制数。斜线后面的数字就是前缀长度，也就是从左到右，被地址块里的地址所共享的位的数目。\n十进制部分有时会被省略，因此，/20就表示一个前缀长度是20的CIDR地址块。如果一个IP地址的前N位与一个CIDR地址块的前缀是相同的话，那么就说这个地址属于这个CIDR地址块，也可以说是与CIDR地址块的前缀匹配。所以，要理解CIDR，就要把地址写成二进制的形式。","title":"aws.md"},{"content":" 软件工程师\n 编程小语种爱好者，精通各种语言的hello world，目前沉迷Clojure、elixir Get Started工程师，止步于大量框架和包的readme 后端开发工程师，偶尔写一些前端，伪全栈 也是一名数据工程师，朝着SQL Boy进化   Skills Languages, software and services used:\n Python | Django Go | Gin Node.js | Vue.js MongoDB | Mysql Docker | K8S Git Hugo | Markdown Hive | Flink  Tools  VSCode Postman  Education 北京邮电大学\nYou can also find me on:  知乎 微博 twitter  ","permalink":"https://zhenfeng-zhu.github.io/about/","summary":"about","title":"About Me"},{"content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start Create a new post $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server $ hexo server More info: Server\nGenerate static files $ hexo generate More info: Generating\nDeploy to remote sites $ hexo deploy More info: Deployment\n","permalink":"https://zhenfeng-zhu.github.io/posts/hello-world/","summary":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick Start Create a new post $ hexo new \u0026#34;My New Post\u0026#34; More info: Writing\nRun server $ hexo server More info: Server\nGenerate static files $ hexo generate More info: Generating\nDeploy to remote sites $ hexo deploy More info: Deployment","title":"Hello World"}]